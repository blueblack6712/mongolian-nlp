{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5727617c-cbaa-4d8e-933c-cb5312b599a1",
   "metadata": {},
   "source": [
    "Fine-tune Mongolian Pos-tagged dataset using Llama-2-3B-hf from hugging face \n",
    "https://huggingface.co/winglian/Llama-2-3b-hf\n",
    "-we download all files needed because this is run in Autodl and there's no VPN to access Hugging face's API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1e1fd-d5bc-449e-93d2-2d80d79c2058",
   "metadata": {},
   "source": [
    "#1 Download and upgrade required Libraries(make sure that they are compatible with your environment) before running all lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a3b740-ac16-46e1-b83f-fda5fc3cce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.50.1)\n",
      "Requirement already satisfied: torch in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.5.1+cu124)\n",
      "Requirement already satisfied: sentencepiece in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
      "Requirement already satisfied: nltk in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: peft in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.15.0)\n",
      "Requirement already satisfied: bitsandbytes in ./miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.45.4)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./miniconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: click in ./miniconda3/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./miniconda3/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: psutil in ./miniconda3/lib/python3.12/site-packages (from peft->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./miniconda3/lib/python3.12/site-packages (from peft->-r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33bc87a-8e0a-42bd-ac47-49cf12dac2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb96b08-ec28-4512-8e5a-49661c55cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: bitsandbytes in ./miniconda3/lib/python3.12/site-packages (0.45.4)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./miniconda3/lib/python3.12/site-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./miniconda3/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.12/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0532249-0825-4d13-9aa9-1acbff599922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: protobuf in ./miniconda3/lib/python3.12/site-packages (6.30.1)\n",
      "Requirement already satisfied: sentencepiece in ./miniconda3/lib/python3.12/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6871481d-8b3a-4278-b4ab-eb55eba18f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: protobuf in ./miniconda3/lib/python3.12/site-packages (6.30.1)\n",
      "Requirement already satisfied: sentencepiece in ./miniconda3/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in ./miniconda3/lib/python3.12/site-packages (4.50.1)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./miniconda3/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf sentencepiece transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d9eb7-a56d-4329-8a33-45a57bb7b21b",
   "metadata": {},
   "source": [
    "#2 (skip this if you use API from hugging face) Try and load downloaded files for Llama-2-3B and test run to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6250ff-9e8b-4167-92b2-3b1b84e3b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁С', 'ү', 'ү', 'лий', 'н', '▁та', 'ван', '▁жи', 'л', '▁да', 'ра', 'а', 'лан']\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "import torch\n",
    "\n",
    "# 确保 tokenizer_class 明确指定为 LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"/dev/shm/Llama-2-3b-hf/\",\n",
    "    tokenizer_file=\"/dev/shm/Llama-2-3b-hf/tokenizer.model\",\n",
    "    legacy=False  # 强制使用新版本处理方式\n",
    ")\n",
    "\n",
    "# 检查 tokenizer 是否加载成功\n",
    "print(tokenizer.tokenize(\"Сүүлийн таван жил дараалан\"))  # 应输出 ['▁Hello', '▁world']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6e59-1e9a-45f9-8b3d-f7501448de55",
   "metadata": {},
   "source": [
    "#3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cb7c56-bfa7-43c4-8c3f-6cd56d226d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb  # For 4-bit quantization\n",
    "\n",
    "LLAMA_MODEL_PATH = \"/dev/shm/Llama-2-3b-hf/\"\n",
    "DATASET_PATH_LLAMA = \"/dev/shm/train.jsonl\"\n",
    "USE_LORA = True  # Enable LoRA to reduce memory usage\n",
    "USE_4BIT = True  # Use 4-bit quantization for efficient training\n",
    "DOWNSAMPLE_RATIO = 0.3  # Adjust dataset size (e.g., 0.3 = 30% of full data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69968f9-2ab7-48a4-a9e8-9d1e9d1a2c66",
   "metadata": {},
   "source": [
    "#4 Memory Management Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91199812-0ea6-4c1a-9e16-b8de28c9d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    \"\"\"Clears unused GPU memory to prevent OutOfMemory errors.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a68537-ead5-4597-90c8-82d2e014941b",
   "metadata": {},
   "source": [
    "#5 POS Tagging Dataset Loader Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947f6ae2-bf1e-49c6-8e8a-85b88d2cb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaPOSDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.samples = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                prompt = data[\"original_text\"]\n",
    "                completion = \" \".join(data[\"pos_tags\"])\n",
    "                self.samples.append({\"prompt\": prompt, \"completion\": completion})\n",
    "        \n",
    "        # Downsample dataset to reduce training time\n",
    "        if 0 < DOWNSAMPLE_RATIO < 1:\n",
    "            self.samples = random.sample(self.samples, int(len(self.samples) * DOWNSAMPLE_RATIO))\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_text = sample[\"prompt\"] + \" \" + sample[\"completion\"]\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "        return encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6d115-2a7b-4e43-8d44-62a87f77d5db",
   "metadata": {},
   "source": [
    "#6 Load Tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a305ff2e-b71b-48ba-99d8-b2aefd0a8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_llama = LlamaTokenizer.from_pretrained(\n",
    "        LLAMA_MODEL_PATH, \n",
    "        tokenizer_file=os.path.join(LLAMA_MODEL_PATH, \"tokenizer.model\"),\n",
    "        legacy=False,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    # **Fix**: Set padding token to eos token\n",
    "    tokenizer_llama.pad_token = tokenizer_llama.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "    model_llama = LlamaForCausalLM.from_pretrained(LLAMA_MODEL_PATH, local_files_only=True).cuda()\n",
    "\n",
    "    print(\"Model and Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise RuntimeError(\"Model or Tokenizer could not be loaded.\")\n",
    "\n",
    "# **Fix**: Prepare 4-bit model for LoRA\n",
    "if USE_4BIT:\n",
    "    model_llama = prepare_model_for_kbit_training(model_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57df13-9c8b-4530-b84a-e4794a99de5c",
   "metadata": {},
   "source": [
    "#7 LoRA Configuration for Efficient Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d686639e-15e6-41e3-863c-8f12fae4903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for low-rank adaptation\n",
    "if USE_LORA:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False,\n",
    "        r=8,  # Low-rank dimension (adjust as needed)\n",
    "        lora_alpha=16,  \n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    model_llama = get_peft_model(model_llama, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57b73c-85f8-4550-a224-9cbe9a245836",
   "metadata": {},
   "source": [
    "#8 Train-Validation Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffd7ec-f53d-4867-a7be-76050c2b209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_llama = LlamaPOSDataset(DATASET_PATH_LLAMA, tokenizer_llama)\n",
    "dataset_size = len(dataset_llama)\n",
    "split_point = int(0.8 * dataset_size)\n",
    "train_dataset_llama = Subset(dataset_llama, list(range(split_point)))\n",
    "val_dataset_llama = Subset(dataset_llama, list(range(split_point, dataset_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adda98-1172-4486-b522-3e5fc183abb4",
   "metadata": {},
   "source": [
    "#9 Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b8415b-3fab-464e-909e-557d4c7706cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== TRAINING ARGUMENTS ======\n",
    "training_args_llama = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Increase if memory allows\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Adjust to fit memory\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs_llama\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    optim=\"adamw_bnb_8bit\" if USE_4BIT else \"adamw_torch_fused\",  # Use optimized optimizer\n",
    "    torch_compile=True,\n",
    "    save_steps=500,  # Save the model every 500 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226ee90-e524-43b4-bcbc-1f2ee050927c",
   "metadata": {},
   "source": [
    "#9 LLaMA Training Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78bbbd3a-04ec-4775-837d-46c62ca782c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14238/1351393481.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_llama = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Fine-tuning started at: 2025-03-26 20:33:18\n"
     ]
    }
   ],
   "source": [
    "# ====== TRAINER SETUP ======\n",
    "trainer_llama = Trainer(\n",
    "    model=model_llama,\n",
    "    args=training_args_llama,\n",
    "    train_dataset=train_dataset_llama,\n",
    "    eval_dataset=val_dataset_llama,\n",
    "    tokenizer=tokenizer_llama\n",
    ")\n",
    "\n",
    "# ====== TRAINING LOOP WITH MEMORY MANAGEMENT ======\n",
    "print(\"LLaMA Fine-tuning started at:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693151d6-a8c0-4d05-b751-1ba5579697b4",
   "metadata": {},
   "source": [
    "#10 OOM Handling & Adaptive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b162bf1-7882-450b-b9aa-9f12e47439cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49490' max='57483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49490/57483 7:10:57 < 1:09:36, 1.91 it/s, Epoch 2.58/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.942197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.869438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer_llama.train()\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"CUDA Out of Memory! Reducing batch size...\")\n",
    "        clear_cuda_memory()\n",
    "\n",
    "        # Retry with lower batch size\n",
    "        training_args_llama.per_device_train_batch_size = max(1, training_args_llama.per_device_train_batch_size // 2)\n",
    "        training_args_llama.gradient_accumulation_steps *= 2  # Compensate for smaller batches\n",
    "\n",
    "        trainer_llama = Trainer(\n",
    "            model=model_llama,\n",
    "            args=training_args_llama,\n",
    "            train_dataset=train_dataset_llama,\n",
    "            eval_dataset=val_dataset_llama,\n",
    "            tokenizer=tokenizer_llama\n",
    "        )\n",
    "\n",
    "        trainer_llama.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2434c3-9e60-4834-8033-191b3ba3b8cf",
   "metadata": {},
   "source": [
    "#11 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8372d4-9ec9-4dbe-abe0-f759384e30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLaMA Fine-tuning finished at:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# ====== SAVE FINAL MODEL ======\n",
    "trainer_llama.save_model(\"./llama_finetuned\")\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f711b718-7302-46bc-8113-10d3e7dd9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LoRA Configuration ===\n",
      "LoRA Rank (r): 8\n",
      "LoRA Alpha (α): 16\n",
      "LoRA Dropout: 0.05\n",
      "Graphs saved as training_loss.png, evaluation_loss.png, and learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "%run graph.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
