{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.20874920088196539,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010437460044098268,
      "grad_norm": 8.523614883422852,
      "learning_rate": 4.998260264439805e-05,
      "loss": 7.8659,
      "step": 10
    },
    {
      "epoch": 0.0020874920088196536,
      "grad_norm": 5.895302772521973,
      "learning_rate": 4.996520528879611e-05,
      "loss": 6.6604,
      "step": 20
    },
    {
      "epoch": 0.0031312380132294806,
      "grad_norm": 6.786211013793945,
      "learning_rate": 4.994780793319416e-05,
      "loss": 5.1571,
      "step": 30
    },
    {
      "epoch": 0.004174984017639307,
      "grad_norm": 2.0219368934631348,
      "learning_rate": 4.993041057759221e-05,
      "loss": 4.1362,
      "step": 40
    },
    {
      "epoch": 0.005218730022049135,
      "grad_norm": 3.1796302795410156,
      "learning_rate": 4.991301322199026e-05,
      "loss": 3.6123,
      "step": 50
    },
    {
      "epoch": 0.006262476026458961,
      "grad_norm": 1.9410531520843506,
      "learning_rate": 4.989561586638831e-05,
      "loss": 3.1131,
      "step": 60
    },
    {
      "epoch": 0.007306222030868788,
      "grad_norm": 1.1769378185272217,
      "learning_rate": 4.987821851078636e-05,
      "loss": 2.7265,
      "step": 70
    },
    {
      "epoch": 0.008349968035278614,
      "grad_norm": 1.5713412761688232,
      "learning_rate": 4.986082115518441e-05,
      "loss": 2.5289,
      "step": 80
    },
    {
      "epoch": 0.009393714039688441,
      "grad_norm": 1.5457574129104614,
      "learning_rate": 4.984342379958247e-05,
      "loss": 2.3842,
      "step": 90
    },
    {
      "epoch": 0.01043746004409827,
      "grad_norm": 1.4703526496887207,
      "learning_rate": 4.982602644398052e-05,
      "loss": 2.2712,
      "step": 100
    },
    {
      "epoch": 0.011481206048508096,
      "grad_norm": 1.5064976215362549,
      "learning_rate": 4.9808629088378566e-05,
      "loss": 2.1134,
      "step": 110
    },
    {
      "epoch": 0.012524952052917922,
      "grad_norm": 1.2361057996749878,
      "learning_rate": 4.979123173277662e-05,
      "loss": 2.0317,
      "step": 120
    },
    {
      "epoch": 0.013568698057327749,
      "grad_norm": 1.3537870645523071,
      "learning_rate": 4.977383437717467e-05,
      "loss": 1.9666,
      "step": 130
    },
    {
      "epoch": 0.014612444061737576,
      "grad_norm": 1.2259069681167603,
      "learning_rate": 4.975643702157272e-05,
      "loss": 1.7377,
      "step": 140
    },
    {
      "epoch": 0.015656190066147404,
      "grad_norm": 1.5078082084655762,
      "learning_rate": 4.973903966597078e-05,
      "loss": 1.8957,
      "step": 150
    },
    {
      "epoch": 0.01669993607055723,
      "grad_norm": 1.9150372743606567,
      "learning_rate": 4.9721642310368827e-05,
      "loss": 1.8171,
      "step": 160
    },
    {
      "epoch": 0.017743682074967057,
      "grad_norm": 2.066640615463257,
      "learning_rate": 4.9704244954766876e-05,
      "loss": 1.8468,
      "step": 170
    },
    {
      "epoch": 0.018787428079376882,
      "grad_norm": 1.7159478664398193,
      "learning_rate": 4.968684759916493e-05,
      "loss": 1.7096,
      "step": 180
    },
    {
      "epoch": 0.01983117408378671,
      "grad_norm": 1.4332084655761719,
      "learning_rate": 4.966945024356298e-05,
      "loss": 1.7738,
      "step": 190
    },
    {
      "epoch": 0.02087492008819654,
      "grad_norm": 1.5466413497924805,
      "learning_rate": 4.965205288796103e-05,
      "loss": 1.6805,
      "step": 200
    },
    {
      "epoch": 0.021918666092606363,
      "grad_norm": 1.74297034740448,
      "learning_rate": 4.963465553235908e-05,
      "loss": 1.8297,
      "step": 210
    },
    {
      "epoch": 0.02296241209701619,
      "grad_norm": 2.5368425846099854,
      "learning_rate": 4.9617258176757137e-05,
      "loss": 1.7077,
      "step": 220
    },
    {
      "epoch": 0.024006158101426017,
      "grad_norm": 1.8184462785720825,
      "learning_rate": 4.9599860821155186e-05,
      "loss": 1.7526,
      "step": 230
    },
    {
      "epoch": 0.025049904105835845,
      "grad_norm": 3.0918869972229004,
      "learning_rate": 4.9582463465553235e-05,
      "loss": 1.6741,
      "step": 240
    },
    {
      "epoch": 0.026093650110245673,
      "grad_norm": 2.0034022331237793,
      "learning_rate": 4.956506610995129e-05,
      "loss": 1.5974,
      "step": 250
    },
    {
      "epoch": 0.027137396114655498,
      "grad_norm": 1.6114243268966675,
      "learning_rate": 4.954766875434934e-05,
      "loss": 1.6215,
      "step": 260
    },
    {
      "epoch": 0.028181142119065326,
      "grad_norm": 1.6844960451126099,
      "learning_rate": 4.953027139874739e-05,
      "loss": 1.6283,
      "step": 270
    },
    {
      "epoch": 0.02922488812347515,
      "grad_norm": 1.5594226121902466,
      "learning_rate": 4.9512874043145446e-05,
      "loss": 1.5574,
      "step": 280
    },
    {
      "epoch": 0.03026863412788498,
      "grad_norm": 2.2042055130004883,
      "learning_rate": 4.9495476687543496e-05,
      "loss": 1.6782,
      "step": 290
    },
    {
      "epoch": 0.03131238013229481,
      "grad_norm": 1.7636032104492188,
      "learning_rate": 4.9478079331941545e-05,
      "loss": 1.6046,
      "step": 300
    },
    {
      "epoch": 0.03235612613670463,
      "grad_norm": 2.121058702468872,
      "learning_rate": 4.94606819763396e-05,
      "loss": 1.6601,
      "step": 310
    },
    {
      "epoch": 0.03339987214111446,
      "grad_norm": 2.0677473545074463,
      "learning_rate": 4.944328462073765e-05,
      "loss": 1.5335,
      "step": 320
    },
    {
      "epoch": 0.03444361814552429,
      "grad_norm": 2.3938567638397217,
      "learning_rate": 4.94258872651357e-05,
      "loss": 1.5547,
      "step": 330
    },
    {
      "epoch": 0.035487364149934114,
      "grad_norm": 1.8895697593688965,
      "learning_rate": 4.940848990953375e-05,
      "loss": 1.6234,
      "step": 340
    },
    {
      "epoch": 0.03653111015434394,
      "grad_norm": 2.617790937423706,
      "learning_rate": 4.9391092553931806e-05,
      "loss": 1.529,
      "step": 350
    },
    {
      "epoch": 0.037574856158753764,
      "grad_norm": 3.0380053520202637,
      "learning_rate": 4.9373695198329855e-05,
      "loss": 1.5759,
      "step": 360
    },
    {
      "epoch": 0.038618602163163596,
      "grad_norm": 2.6452436447143555,
      "learning_rate": 4.9356297842727905e-05,
      "loss": 1.5486,
      "step": 370
    },
    {
      "epoch": 0.03966234816757342,
      "grad_norm": 2.3342792987823486,
      "learning_rate": 4.933890048712596e-05,
      "loss": 1.4438,
      "step": 380
    },
    {
      "epoch": 0.040706094171983245,
      "grad_norm": 3.2901759147644043,
      "learning_rate": 4.932150313152401e-05,
      "loss": 1.5455,
      "step": 390
    },
    {
      "epoch": 0.04174984017639308,
      "grad_norm": 2.452566385269165,
      "learning_rate": 4.930410577592206e-05,
      "loss": 1.4239,
      "step": 400
    },
    {
      "epoch": 0.0427935861808029,
      "grad_norm": 2.4525604248046875,
      "learning_rate": 4.9286708420320116e-05,
      "loss": 1.5279,
      "step": 410
    },
    {
      "epoch": 0.04383733218521273,
      "grad_norm": 2.5176737308502197,
      "learning_rate": 4.9269311064718165e-05,
      "loss": 1.517,
      "step": 420
    },
    {
      "epoch": 0.04488107818962256,
      "grad_norm": 2.1970272064208984,
      "learning_rate": 4.9251913709116214e-05,
      "loss": 1.5686,
      "step": 430
    },
    {
      "epoch": 0.04592482419403238,
      "grad_norm": 2.0587196350097656,
      "learning_rate": 4.923451635351427e-05,
      "loss": 1.4957,
      "step": 440
    },
    {
      "epoch": 0.04696857019844221,
      "grad_norm": 2.885244846343994,
      "learning_rate": 4.921711899791232e-05,
      "loss": 1.5005,
      "step": 450
    },
    {
      "epoch": 0.04801231620285203,
      "grad_norm": 2.489414930343628,
      "learning_rate": 4.919972164231037e-05,
      "loss": 1.4652,
      "step": 460
    },
    {
      "epoch": 0.049056062207261865,
      "grad_norm": 2.7048799991607666,
      "learning_rate": 4.918232428670842e-05,
      "loss": 1.4756,
      "step": 470
    },
    {
      "epoch": 0.05009980821167169,
      "grad_norm": 2.6549763679504395,
      "learning_rate": 4.9164926931106475e-05,
      "loss": 1.4172,
      "step": 480
    },
    {
      "epoch": 0.051143554216081515,
      "grad_norm": 3.3704168796539307,
      "learning_rate": 4.9147529575504524e-05,
      "loss": 1.4944,
      "step": 490
    },
    {
      "epoch": 0.052187300220491346,
      "grad_norm": 3.0778634548187256,
      "learning_rate": 4.9130132219902574e-05,
      "loss": 1.4459,
      "step": 500
    },
    {
      "epoch": 0.05323104622490117,
      "grad_norm": 2.523585796356201,
      "learning_rate": 4.911273486430063e-05,
      "loss": 1.4274,
      "step": 510
    },
    {
      "epoch": 0.054274792229310996,
      "grad_norm": 2.8418996334075928,
      "learning_rate": 4.909533750869868e-05,
      "loss": 1.4335,
      "step": 520
    },
    {
      "epoch": 0.05531853823372082,
      "grad_norm": 2.5925774574279785,
      "learning_rate": 4.907794015309673e-05,
      "loss": 1.4419,
      "step": 530
    },
    {
      "epoch": 0.05636228423813065,
      "grad_norm": 3.0863401889801025,
      "learning_rate": 4.9060542797494785e-05,
      "loss": 1.3181,
      "step": 540
    },
    {
      "epoch": 0.05740603024254048,
      "grad_norm": 2.443354606628418,
      "learning_rate": 4.9043145441892834e-05,
      "loss": 1.4843,
      "step": 550
    },
    {
      "epoch": 0.0584497762469503,
      "grad_norm": 2.775830030441284,
      "learning_rate": 4.9025748086290884e-05,
      "loss": 1.4027,
      "step": 560
    },
    {
      "epoch": 0.059493522251360134,
      "grad_norm": 2.36427640914917,
      "learning_rate": 4.900835073068894e-05,
      "loss": 1.4293,
      "step": 570
    },
    {
      "epoch": 0.06053726825576996,
      "grad_norm": 2.5213277339935303,
      "learning_rate": 4.899095337508699e-05,
      "loss": 1.3836,
      "step": 580
    },
    {
      "epoch": 0.061581014260179784,
      "grad_norm": 3.0936291217803955,
      "learning_rate": 4.897355601948504e-05,
      "loss": 1.3928,
      "step": 590
    },
    {
      "epoch": 0.06262476026458962,
      "grad_norm": 2.561060905456543,
      "learning_rate": 4.895615866388309e-05,
      "loss": 1.3436,
      "step": 600
    },
    {
      "epoch": 0.06366850626899943,
      "grad_norm": 3.146256685256958,
      "learning_rate": 4.8938761308281144e-05,
      "loss": 1.3676,
      "step": 610
    },
    {
      "epoch": 0.06471225227340927,
      "grad_norm": 2.801110029220581,
      "learning_rate": 4.8921363952679194e-05,
      "loss": 1.28,
      "step": 620
    },
    {
      "epoch": 0.0657559982778191,
      "grad_norm": 2.8328936100006104,
      "learning_rate": 4.890396659707724e-05,
      "loss": 1.4353,
      "step": 630
    },
    {
      "epoch": 0.06679974428222892,
      "grad_norm": 3.2667737007141113,
      "learning_rate": 4.88865692414753e-05,
      "loss": 1.3552,
      "step": 640
    },
    {
      "epoch": 0.06784349028663875,
      "grad_norm": 3.4283764362335205,
      "learning_rate": 4.886917188587335e-05,
      "loss": 1.3115,
      "step": 650
    },
    {
      "epoch": 0.06888723629104858,
      "grad_norm": 3.6147899627685547,
      "learning_rate": 4.88517745302714e-05,
      "loss": 1.3064,
      "step": 660
    },
    {
      "epoch": 0.0699309822954584,
      "grad_norm": 3.4828693866729736,
      "learning_rate": 4.8834377174669454e-05,
      "loss": 1.3891,
      "step": 670
    },
    {
      "epoch": 0.07097472829986823,
      "grad_norm": 3.4031224250793457,
      "learning_rate": 4.8816979819067504e-05,
      "loss": 1.399,
      "step": 680
    },
    {
      "epoch": 0.07201847430427806,
      "grad_norm": 3.044623613357544,
      "learning_rate": 4.879958246346555e-05,
      "loss": 1.2873,
      "step": 690
    },
    {
      "epoch": 0.07306222030868788,
      "grad_norm": 3.060896396636963,
      "learning_rate": 4.878218510786361e-05,
      "loss": 1.321,
      "step": 700
    },
    {
      "epoch": 0.07410596631309771,
      "grad_norm": 3.1356711387634277,
      "learning_rate": 4.876478775226166e-05,
      "loss": 1.372,
      "step": 710
    },
    {
      "epoch": 0.07514971231750753,
      "grad_norm": 3.2255043983459473,
      "learning_rate": 4.874739039665971e-05,
      "loss": 1.3372,
      "step": 720
    },
    {
      "epoch": 0.07619345832191736,
      "grad_norm": 3.638859510421753,
      "learning_rate": 4.8729993041057764e-05,
      "loss": 1.4511,
      "step": 730
    },
    {
      "epoch": 0.07723720432632719,
      "grad_norm": 3.6662185192108154,
      "learning_rate": 4.8712595685455813e-05,
      "loss": 1.3563,
      "step": 740
    },
    {
      "epoch": 0.07828095033073701,
      "grad_norm": 2.736846446990967,
      "learning_rate": 4.869519832985386e-05,
      "loss": 1.3291,
      "step": 750
    },
    {
      "epoch": 0.07932469633514684,
      "grad_norm": 3.405195951461792,
      "learning_rate": 4.867780097425191e-05,
      "loss": 1.3793,
      "step": 760
    },
    {
      "epoch": 0.08036844233955667,
      "grad_norm": 2.9752981662750244,
      "learning_rate": 4.866040361864997e-05,
      "loss": 1.3786,
      "step": 770
    },
    {
      "epoch": 0.08141218834396649,
      "grad_norm": 3.4894630908966064,
      "learning_rate": 4.864300626304802e-05,
      "loss": 1.2159,
      "step": 780
    },
    {
      "epoch": 0.08245593434837632,
      "grad_norm": 3.33750057220459,
      "learning_rate": 4.862560890744607e-05,
      "loss": 1.264,
      "step": 790
    },
    {
      "epoch": 0.08349968035278615,
      "grad_norm": 3.7672841548919678,
      "learning_rate": 4.860821155184412e-05,
      "loss": 1.2789,
      "step": 800
    },
    {
      "epoch": 0.08454342635719597,
      "grad_norm": 2.7405295372009277,
      "learning_rate": 4.859081419624217e-05,
      "loss": 1.3334,
      "step": 810
    },
    {
      "epoch": 0.0855871723616058,
      "grad_norm": 4.042642593383789,
      "learning_rate": 4.857341684064022e-05,
      "loss": 1.3728,
      "step": 820
    },
    {
      "epoch": 0.08663091836601564,
      "grad_norm": 3.692669153213501,
      "learning_rate": 4.855601948503828e-05,
      "loss": 1.2319,
      "step": 830
    },
    {
      "epoch": 0.08767466437042545,
      "grad_norm": 3.4934074878692627,
      "learning_rate": 4.853862212943633e-05,
      "loss": 1.2999,
      "step": 840
    },
    {
      "epoch": 0.08871841037483529,
      "grad_norm": 2.9408931732177734,
      "learning_rate": 4.852122477383438e-05,
      "loss": 1.3514,
      "step": 850
    },
    {
      "epoch": 0.08976215637924512,
      "grad_norm": 4.195568084716797,
      "learning_rate": 4.850382741823243e-05,
      "loss": 1.3645,
      "step": 860
    },
    {
      "epoch": 0.09080590238365494,
      "grad_norm": 3.6301825046539307,
      "learning_rate": 4.848643006263048e-05,
      "loss": 1.3674,
      "step": 870
    },
    {
      "epoch": 0.09184964838806477,
      "grad_norm": 3.7087204456329346,
      "learning_rate": 4.846903270702853e-05,
      "loss": 1.285,
      "step": 880
    },
    {
      "epoch": 0.09289339439247458,
      "grad_norm": 3.0189831256866455,
      "learning_rate": 4.845163535142658e-05,
      "loss": 1.2808,
      "step": 890
    },
    {
      "epoch": 0.09393714039688442,
      "grad_norm": 2.952362060546875,
      "learning_rate": 4.843423799582464e-05,
      "loss": 1.2494,
      "step": 900
    },
    {
      "epoch": 0.09498088640129425,
      "grad_norm": 3.8399035930633545,
      "learning_rate": 4.841684064022269e-05,
      "loss": 1.2888,
      "step": 910
    },
    {
      "epoch": 0.09602463240570407,
      "grad_norm": 3.2061569690704346,
      "learning_rate": 4.8399443284620736e-05,
      "loss": 1.3672,
      "step": 920
    },
    {
      "epoch": 0.0970683784101139,
      "grad_norm": 3.3855128288269043,
      "learning_rate": 4.838204592901879e-05,
      "loss": 1.2718,
      "step": 930
    },
    {
      "epoch": 0.09811212441452373,
      "grad_norm": 3.4758858680725098,
      "learning_rate": 4.836464857341684e-05,
      "loss": 1.2141,
      "step": 940
    },
    {
      "epoch": 0.09915587041893355,
      "grad_norm": 3.1619112491607666,
      "learning_rate": 4.834725121781489e-05,
      "loss": 1.281,
      "step": 950
    },
    {
      "epoch": 0.10019961642334338,
      "grad_norm": 3.3238589763641357,
      "learning_rate": 4.832985386221295e-05,
      "loss": 1.3246,
      "step": 960
    },
    {
      "epoch": 0.10124336242775321,
      "grad_norm": 4.692479610443115,
      "learning_rate": 4.8312456506611e-05,
      "loss": 1.3826,
      "step": 970
    },
    {
      "epoch": 0.10228710843216303,
      "grad_norm": 4.413359642028809,
      "learning_rate": 4.8295059151009046e-05,
      "loss": 1.1526,
      "step": 980
    },
    {
      "epoch": 0.10333085443657286,
      "grad_norm": 3.593838691711426,
      "learning_rate": 4.82776617954071e-05,
      "loss": 1.2516,
      "step": 990
    },
    {
      "epoch": 0.10437460044098269,
      "grad_norm": 3.4141323566436768,
      "learning_rate": 4.826026443980515e-05,
      "loss": 1.2831,
      "step": 1000
    },
    {
      "epoch": 0.10541834644539251,
      "grad_norm": 3.442960262298584,
      "learning_rate": 4.82428670842032e-05,
      "loss": 1.2225,
      "step": 1010
    },
    {
      "epoch": 0.10646209244980234,
      "grad_norm": 3.3055434226989746,
      "learning_rate": 4.822546972860125e-05,
      "loss": 1.2395,
      "step": 1020
    },
    {
      "epoch": 0.10750583845421217,
      "grad_norm": 4.852781295776367,
      "learning_rate": 4.820807237299931e-05,
      "loss": 1.3361,
      "step": 1030
    },
    {
      "epoch": 0.10854958445862199,
      "grad_norm": 4.456081867218018,
      "learning_rate": 4.8190675017397356e-05,
      "loss": 1.311,
      "step": 1040
    },
    {
      "epoch": 0.10959333046303182,
      "grad_norm": 4.248234748840332,
      "learning_rate": 4.8173277661795406e-05,
      "loss": 1.2439,
      "step": 1050
    },
    {
      "epoch": 0.11063707646744164,
      "grad_norm": 4.108493328094482,
      "learning_rate": 4.815588030619346e-05,
      "loss": 1.3574,
      "step": 1060
    },
    {
      "epoch": 0.11168082247185147,
      "grad_norm": 3.8025126457214355,
      "learning_rate": 4.813848295059151e-05,
      "loss": 1.3107,
      "step": 1070
    },
    {
      "epoch": 0.1127245684762613,
      "grad_norm": 3.683912515640259,
      "learning_rate": 4.812108559498956e-05,
      "loss": 1.3084,
      "step": 1080
    },
    {
      "epoch": 0.11376831448067112,
      "grad_norm": 3.837519884109497,
      "learning_rate": 4.810368823938762e-05,
      "loss": 1.2083,
      "step": 1090
    },
    {
      "epoch": 0.11481206048508096,
      "grad_norm": 3.1101255416870117,
      "learning_rate": 4.8086290883785666e-05,
      "loss": 1.2376,
      "step": 1100
    },
    {
      "epoch": 0.11585580648949079,
      "grad_norm": 4.115686893463135,
      "learning_rate": 4.8068893528183716e-05,
      "loss": 1.2246,
      "step": 1110
    },
    {
      "epoch": 0.1168995524939006,
      "grad_norm": 3.758197069168091,
      "learning_rate": 4.805149617258177e-05,
      "loss": 1.2602,
      "step": 1120
    },
    {
      "epoch": 0.11794329849831044,
      "grad_norm": 3.271947145462036,
      "learning_rate": 4.803409881697982e-05,
      "loss": 1.2147,
      "step": 1130
    },
    {
      "epoch": 0.11898704450272027,
      "grad_norm": 4.176307678222656,
      "learning_rate": 4.801670146137787e-05,
      "loss": 1.2204,
      "step": 1140
    },
    {
      "epoch": 0.12003079050713009,
      "grad_norm": 4.171455383300781,
      "learning_rate": 4.799930410577592e-05,
      "loss": 1.2705,
      "step": 1150
    },
    {
      "epoch": 0.12107453651153992,
      "grad_norm": 3.5750319957733154,
      "learning_rate": 4.7981906750173976e-05,
      "loss": 1.3073,
      "step": 1160
    },
    {
      "epoch": 0.12211828251594975,
      "grad_norm": 3.3760180473327637,
      "learning_rate": 4.7964509394572026e-05,
      "loss": 1.1335,
      "step": 1170
    },
    {
      "epoch": 0.12316202852035957,
      "grad_norm": 3.795766830444336,
      "learning_rate": 4.7947112038970075e-05,
      "loss": 1.3329,
      "step": 1180
    },
    {
      "epoch": 0.1242057745247694,
      "grad_norm": 3.76965594291687,
      "learning_rate": 4.792971468336813e-05,
      "loss": 1.2481,
      "step": 1190
    },
    {
      "epoch": 0.12524952052917923,
      "grad_norm": 2.9602913856506348,
      "learning_rate": 4.791231732776618e-05,
      "loss": 1.2662,
      "step": 1200
    },
    {
      "epoch": 0.12629326653358905,
      "grad_norm": 3.9874682426452637,
      "learning_rate": 4.789491997216423e-05,
      "loss": 1.2595,
      "step": 1210
    },
    {
      "epoch": 0.12733701253799887,
      "grad_norm": 3.565216064453125,
      "learning_rate": 4.7877522616562286e-05,
      "loss": 1.1998,
      "step": 1220
    },
    {
      "epoch": 0.1283807585424087,
      "grad_norm": 3.3291354179382324,
      "learning_rate": 4.7860125260960335e-05,
      "loss": 1.2023,
      "step": 1230
    },
    {
      "epoch": 0.12942450454681853,
      "grad_norm": 3.5188546180725098,
      "learning_rate": 4.7842727905358385e-05,
      "loss": 1.2768,
      "step": 1240
    },
    {
      "epoch": 0.13046825055122835,
      "grad_norm": 3.8903634548187256,
      "learning_rate": 4.782533054975644e-05,
      "loss": 1.2093,
      "step": 1250
    },
    {
      "epoch": 0.1315119965556382,
      "grad_norm": 2.872838258743286,
      "learning_rate": 4.780793319415449e-05,
      "loss": 1.2205,
      "step": 1260
    },
    {
      "epoch": 0.132555742560048,
      "grad_norm": 3.1458616256713867,
      "learning_rate": 4.779053583855254e-05,
      "loss": 1.2665,
      "step": 1270
    },
    {
      "epoch": 0.13359948856445783,
      "grad_norm": 3.6049320697784424,
      "learning_rate": 4.777313848295059e-05,
      "loss": 1.2154,
      "step": 1280
    },
    {
      "epoch": 0.13464323456886768,
      "grad_norm": 2.709524631500244,
      "learning_rate": 4.7755741127348645e-05,
      "loss": 1.2917,
      "step": 1290
    },
    {
      "epoch": 0.1356869805732775,
      "grad_norm": 3.5064685344696045,
      "learning_rate": 4.7738343771746695e-05,
      "loss": 1.2574,
      "step": 1300
    },
    {
      "epoch": 0.1367307265776873,
      "grad_norm": 3.6428675651550293,
      "learning_rate": 4.7720946416144744e-05,
      "loss": 1.1561,
      "step": 1310
    },
    {
      "epoch": 0.13777447258209716,
      "grad_norm": 3.1255738735198975,
      "learning_rate": 4.77035490605428e-05,
      "loss": 1.1436,
      "step": 1320
    },
    {
      "epoch": 0.13881821858650698,
      "grad_norm": 4.475252628326416,
      "learning_rate": 4.768615170494085e-05,
      "loss": 1.287,
      "step": 1330
    },
    {
      "epoch": 0.1398619645909168,
      "grad_norm": 4.207267761230469,
      "learning_rate": 4.76687543493389e-05,
      "loss": 1.2275,
      "step": 1340
    },
    {
      "epoch": 0.14090571059532664,
      "grad_norm": 4.140781879425049,
      "learning_rate": 4.7651356993736955e-05,
      "loss": 1.2702,
      "step": 1350
    },
    {
      "epoch": 0.14194945659973646,
      "grad_norm": 3.8638670444488525,
      "learning_rate": 4.7633959638135005e-05,
      "loss": 1.2656,
      "step": 1360
    },
    {
      "epoch": 0.14299320260414627,
      "grad_norm": 3.188582420349121,
      "learning_rate": 4.7616562282533054e-05,
      "loss": 1.1499,
      "step": 1370
    },
    {
      "epoch": 0.14403694860855612,
      "grad_norm": 4.511153697967529,
      "learning_rate": 4.759916492693111e-05,
      "loss": 1.2129,
      "step": 1380
    },
    {
      "epoch": 0.14508069461296594,
      "grad_norm": 4.0871195793151855,
      "learning_rate": 4.758176757132916e-05,
      "loss": 1.2373,
      "step": 1390
    },
    {
      "epoch": 0.14612444061737576,
      "grad_norm": 4.063108921051025,
      "learning_rate": 4.756437021572721e-05,
      "loss": 1.2298,
      "step": 1400
    },
    {
      "epoch": 0.1471681866217856,
      "grad_norm": 3.881640672683716,
      "learning_rate": 4.754697286012526e-05,
      "loss": 1.2397,
      "step": 1410
    },
    {
      "epoch": 0.14821193262619542,
      "grad_norm": 4.728618144989014,
      "learning_rate": 4.7529575504523315e-05,
      "loss": 1.2505,
      "step": 1420
    },
    {
      "epoch": 0.14925567863060524,
      "grad_norm": 3.7888247966766357,
      "learning_rate": 4.7512178148921364e-05,
      "loss": 1.1388,
      "step": 1430
    },
    {
      "epoch": 0.15029942463501506,
      "grad_norm": 3.303753614425659,
      "learning_rate": 4.749478079331941e-05,
      "loss": 1.247,
      "step": 1440
    },
    {
      "epoch": 0.1513431706394249,
      "grad_norm": 3.1645898818969727,
      "learning_rate": 4.747738343771747e-05,
      "loss": 1.1236,
      "step": 1450
    },
    {
      "epoch": 0.15238691664383472,
      "grad_norm": 3.6226603984832764,
      "learning_rate": 4.745998608211552e-05,
      "loss": 1.1806,
      "step": 1460
    },
    {
      "epoch": 0.15343066264824454,
      "grad_norm": 3.460843086242676,
      "learning_rate": 4.744258872651357e-05,
      "loss": 1.1572,
      "step": 1470
    },
    {
      "epoch": 0.15447440865265438,
      "grad_norm": 3.807436227798462,
      "learning_rate": 4.7425191370911624e-05,
      "loss": 1.1667,
      "step": 1480
    },
    {
      "epoch": 0.1555181546570642,
      "grad_norm": 4.311255931854248,
      "learning_rate": 4.7407794015309674e-05,
      "loss": 1.2294,
      "step": 1490
    },
    {
      "epoch": 0.15656190066147402,
      "grad_norm": 3.8890631198883057,
      "learning_rate": 4.739039665970772e-05,
      "loss": 1.1622,
      "step": 1500
    },
    {
      "epoch": 0.15760564666588386,
      "grad_norm": 3.9495062828063965,
      "learning_rate": 4.737299930410578e-05,
      "loss": 1.2482,
      "step": 1510
    },
    {
      "epoch": 0.15864939267029368,
      "grad_norm": 3.453996419906616,
      "learning_rate": 4.735560194850383e-05,
      "loss": 1.2466,
      "step": 1520
    },
    {
      "epoch": 0.1596931386747035,
      "grad_norm": 4.119969844818115,
      "learning_rate": 4.733820459290188e-05,
      "loss": 1.1876,
      "step": 1530
    },
    {
      "epoch": 0.16073688467911335,
      "grad_norm": 3.5546047687530518,
      "learning_rate": 4.732080723729993e-05,
      "loss": 1.294,
      "step": 1540
    },
    {
      "epoch": 0.16178063068352316,
      "grad_norm": 4.256692886352539,
      "learning_rate": 4.7303409881697984e-05,
      "loss": 1.1676,
      "step": 1550
    },
    {
      "epoch": 0.16282437668793298,
      "grad_norm": 3.869170665740967,
      "learning_rate": 4.728601252609603e-05,
      "loss": 1.1199,
      "step": 1560
    },
    {
      "epoch": 0.16386812269234283,
      "grad_norm": 4.414412498474121,
      "learning_rate": 4.726861517049408e-05,
      "loss": 1.2718,
      "step": 1570
    },
    {
      "epoch": 0.16491186869675264,
      "grad_norm": 4.220846176147461,
      "learning_rate": 4.725121781489214e-05,
      "loss": 1.2436,
      "step": 1580
    },
    {
      "epoch": 0.16595561470116246,
      "grad_norm": 3.3340442180633545,
      "learning_rate": 4.723382045929019e-05,
      "loss": 1.1609,
      "step": 1590
    },
    {
      "epoch": 0.1669993607055723,
      "grad_norm": 4.385956764221191,
      "learning_rate": 4.721642310368824e-05,
      "loss": 1.2409,
      "step": 1600
    },
    {
      "epoch": 0.16804310670998213,
      "grad_norm": 3.6634840965270996,
      "learning_rate": 4.7199025748086294e-05,
      "loss": 1.2354,
      "step": 1610
    },
    {
      "epoch": 0.16908685271439194,
      "grad_norm": 4.207584381103516,
      "learning_rate": 4.718162839248434e-05,
      "loss": 1.17,
      "step": 1620
    },
    {
      "epoch": 0.1701305987188018,
      "grad_norm": 4.506627559661865,
      "learning_rate": 4.716423103688239e-05,
      "loss": 1.1605,
      "step": 1630
    },
    {
      "epoch": 0.1711743447232116,
      "grad_norm": 4.409481048583984,
      "learning_rate": 4.714683368128045e-05,
      "loss": 1.2007,
      "step": 1640
    },
    {
      "epoch": 0.17221809072762143,
      "grad_norm": 5.252344608306885,
      "learning_rate": 4.71294363256785e-05,
      "loss": 1.2026,
      "step": 1650
    },
    {
      "epoch": 0.17326183673203127,
      "grad_norm": 4.724670886993408,
      "learning_rate": 4.711203897007655e-05,
      "loss": 1.2427,
      "step": 1660
    },
    {
      "epoch": 0.1743055827364411,
      "grad_norm": 4.02918815612793,
      "learning_rate": 4.70946416144746e-05,
      "loss": 1.271,
      "step": 1670
    },
    {
      "epoch": 0.1753493287408509,
      "grad_norm": 3.124152660369873,
      "learning_rate": 4.707724425887265e-05,
      "loss": 1.1982,
      "step": 1680
    },
    {
      "epoch": 0.17639307474526075,
      "grad_norm": 4.242745876312256,
      "learning_rate": 4.70598469032707e-05,
      "loss": 1.1426,
      "step": 1690
    },
    {
      "epoch": 0.17743682074967057,
      "grad_norm": 4.757702350616455,
      "learning_rate": 4.704244954766875e-05,
      "loss": 1.2839,
      "step": 1700
    },
    {
      "epoch": 0.1784805667540804,
      "grad_norm": 4.548902988433838,
      "learning_rate": 4.702505219206681e-05,
      "loss": 1.1818,
      "step": 1710
    },
    {
      "epoch": 0.17952431275849023,
      "grad_norm": 4.518340110778809,
      "learning_rate": 4.700765483646486e-05,
      "loss": 1.1778,
      "step": 1720
    },
    {
      "epoch": 0.18056805876290005,
      "grad_norm": 3.8443961143493652,
      "learning_rate": 4.699025748086291e-05,
      "loss": 1.1613,
      "step": 1730
    },
    {
      "epoch": 0.18161180476730987,
      "grad_norm": 3.639802932739258,
      "learning_rate": 4.697286012526096e-05,
      "loss": 1.1712,
      "step": 1740
    },
    {
      "epoch": 0.18265555077171972,
      "grad_norm": 3.7906911373138428,
      "learning_rate": 4.695546276965901e-05,
      "loss": 1.2422,
      "step": 1750
    },
    {
      "epoch": 0.18369929677612953,
      "grad_norm": 4.372833251953125,
      "learning_rate": 4.693806541405706e-05,
      "loss": 1.1621,
      "step": 1760
    },
    {
      "epoch": 0.18474304278053935,
      "grad_norm": 3.5452797412872314,
      "learning_rate": 4.692066805845512e-05,
      "loss": 1.1782,
      "step": 1770
    },
    {
      "epoch": 0.18578678878494917,
      "grad_norm": 4.708840370178223,
      "learning_rate": 4.690327070285317e-05,
      "loss": 1.1284,
      "step": 1780
    },
    {
      "epoch": 0.18683053478935902,
      "grad_norm": 3.77376127243042,
      "learning_rate": 4.688587334725122e-05,
      "loss": 1.2355,
      "step": 1790
    },
    {
      "epoch": 0.18787428079376883,
      "grad_norm": 3.7511963844299316,
      "learning_rate": 4.6868475991649266e-05,
      "loss": 1.2004,
      "step": 1800
    },
    {
      "epoch": 0.18891802679817865,
      "grad_norm": 4.420706748962402,
      "learning_rate": 4.685107863604732e-05,
      "loss": 1.2091,
      "step": 1810
    },
    {
      "epoch": 0.1899617728025885,
      "grad_norm": 4.168908596038818,
      "learning_rate": 4.683368128044537e-05,
      "loss": 1.2688,
      "step": 1820
    },
    {
      "epoch": 0.19100551880699831,
      "grad_norm": 3.3633017539978027,
      "learning_rate": 4.681628392484342e-05,
      "loss": 1.2439,
      "step": 1830
    },
    {
      "epoch": 0.19204926481140813,
      "grad_norm": 4.204957962036133,
      "learning_rate": 4.679888656924148e-05,
      "loss": 1.1572,
      "step": 1840
    },
    {
      "epoch": 0.19309301081581798,
      "grad_norm": 3.8209192752838135,
      "learning_rate": 4.678148921363953e-05,
      "loss": 1.171,
      "step": 1850
    },
    {
      "epoch": 0.1941367568202278,
      "grad_norm": 4.449513912200928,
      "learning_rate": 4.6764091858037576e-05,
      "loss": 1.1733,
      "step": 1860
    },
    {
      "epoch": 0.19518050282463761,
      "grad_norm": 5.035628795623779,
      "learning_rate": 4.674669450243563e-05,
      "loss": 1.1618,
      "step": 1870
    },
    {
      "epoch": 0.19622424882904746,
      "grad_norm": 5.621617317199707,
      "learning_rate": 4.672929714683368e-05,
      "loss": 1.1652,
      "step": 1880
    },
    {
      "epoch": 0.19726799483345728,
      "grad_norm": 3.8960978984832764,
      "learning_rate": 4.671189979123173e-05,
      "loss": 1.1864,
      "step": 1890
    },
    {
      "epoch": 0.1983117408378671,
      "grad_norm": 4.035328388214111,
      "learning_rate": 4.669450243562979e-05,
      "loss": 1.2196,
      "step": 1900
    },
    {
      "epoch": 0.19935548684227694,
      "grad_norm": 4.436378479003906,
      "learning_rate": 4.6677105080027837e-05,
      "loss": 1.1779,
      "step": 1910
    },
    {
      "epoch": 0.20039923284668676,
      "grad_norm": 3.6593968868255615,
      "learning_rate": 4.6659707724425886e-05,
      "loss": 1.2061,
      "step": 1920
    },
    {
      "epoch": 0.20144297885109658,
      "grad_norm": 4.127246856689453,
      "learning_rate": 4.6642310368823935e-05,
      "loss": 1.163,
      "step": 1930
    },
    {
      "epoch": 0.20248672485550642,
      "grad_norm": 4.3095550537109375,
      "learning_rate": 4.662491301322199e-05,
      "loss": 1.2138,
      "step": 1940
    },
    {
      "epoch": 0.20353047085991624,
      "grad_norm": 4.21481990814209,
      "learning_rate": 4.660751565762004e-05,
      "loss": 1.1437,
      "step": 1950
    },
    {
      "epoch": 0.20457421686432606,
      "grad_norm": 4.279048442840576,
      "learning_rate": 4.659011830201809e-05,
      "loss": 1.1396,
      "step": 1960
    },
    {
      "epoch": 0.2056179628687359,
      "grad_norm": 3.6559321880340576,
      "learning_rate": 4.6572720946416146e-05,
      "loss": 1.188,
      "step": 1970
    },
    {
      "epoch": 0.20666170887314572,
      "grad_norm": 4.7967848777771,
      "learning_rate": 4.6555323590814196e-05,
      "loss": 1.1621,
      "step": 1980
    },
    {
      "epoch": 0.20770545487755554,
      "grad_norm": 4.583678722381592,
      "learning_rate": 4.6537926235212245e-05,
      "loss": 1.2086,
      "step": 1990
    },
    {
      "epoch": 0.20874920088196539,
      "grad_norm": 3.7450830936431885,
      "learning_rate": 4.65205288796103e-05,
      "loss": 1.1486,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 28740,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.65706482057216e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
