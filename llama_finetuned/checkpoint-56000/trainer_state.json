{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.922567093297846,
  "eval_steps": 500,
  "global_step": 56000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005218730022049134,
      "grad_norm": 6.072211265563965,
      "learning_rate": 4.9991301776177306e-05,
      "loss": 8.0094,
      "step": 10
    },
    {
      "epoch": 0.0010437460044098268,
      "grad_norm": 8.396883964538574,
      "learning_rate": 4.998260355235461e-05,
      "loss": 6.6265,
      "step": 20
    },
    {
      "epoch": 0.0015656190066147403,
      "grad_norm": 10.437066078186035,
      "learning_rate": 4.9974775150914186e-05,
      "loss": 5.2237,
      "step": 30
    },
    {
      "epoch": 0.0020874920088196536,
      "grad_norm": 2.514134407043457,
      "learning_rate": 4.9966076927091496e-05,
      "loss": 4.0856,
      "step": 40
    },
    {
      "epoch": 0.0026093650110245673,
      "grad_norm": 4.208181858062744,
      "learning_rate": 4.995737870326879e-05,
      "loss": 3.9928,
      "step": 50
    },
    {
      "epoch": 0.0031312380132294806,
      "grad_norm": 2.5834693908691406,
      "learning_rate": 4.99486804794461e-05,
      "loss": 2.8525,
      "step": 60
    },
    {
      "epoch": 0.003653111015434394,
      "grad_norm": 1.783102035522461,
      "learning_rate": 4.99399822556234e-05,
      "loss": 2.8085,
      "step": 70
    },
    {
      "epoch": 0.004174984017639307,
      "grad_norm": 1.525793194770813,
      "learning_rate": 4.9931284031800704e-05,
      "loss": 2.385,
      "step": 80
    },
    {
      "epoch": 0.0046968570198442205,
      "grad_norm": 1.2962183952331543,
      "learning_rate": 4.9922585807978014e-05,
      "loss": 2.3928,
      "step": 90
    },
    {
      "epoch": 0.005218730022049135,
      "grad_norm": 1.1659306287765503,
      "learning_rate": 4.991388758415532e-05,
      "loss": 2.1404,
      "step": 100
    },
    {
      "epoch": 0.005740603024254048,
      "grad_norm": 1.4313616752624512,
      "learning_rate": 4.990518936033262e-05,
      "loss": 2.2032,
      "step": 110
    },
    {
      "epoch": 0.006262476026458961,
      "grad_norm": 1.5873713493347168,
      "learning_rate": 4.9896491136509925e-05,
      "loss": 2.159,
      "step": 120
    },
    {
      "epoch": 0.0067843490286638745,
      "grad_norm": 2.549116849899292,
      "learning_rate": 4.988779291268723e-05,
      "loss": 1.9591,
      "step": 130
    },
    {
      "epoch": 0.007306222030868788,
      "grad_norm": 1.400861382484436,
      "learning_rate": 4.987909468886454e-05,
      "loss": 1.8803,
      "step": 140
    },
    {
      "epoch": 0.007828095033073702,
      "grad_norm": 1.8229321241378784,
      "learning_rate": 4.987039646504184e-05,
      "loss": 2.0724,
      "step": 150
    },
    {
      "epoch": 0.008349968035278614,
      "grad_norm": 1.8560459613800049,
      "learning_rate": 4.9861698241219146e-05,
      "loss": 1.9088,
      "step": 160
    },
    {
      "epoch": 0.008871841037483529,
      "grad_norm": 2.0486276149749756,
      "learning_rate": 4.985300001739645e-05,
      "loss": 1.9543,
      "step": 170
    },
    {
      "epoch": 0.009393714039688441,
      "grad_norm": 1.5926165580749512,
      "learning_rate": 4.984430179357375e-05,
      "loss": 1.7629,
      "step": 180
    },
    {
      "epoch": 0.009915587041893355,
      "grad_norm": 1.9361776113510132,
      "learning_rate": 4.983560356975106e-05,
      "loss": 1.7867,
      "step": 190
    },
    {
      "epoch": 0.01043746004409827,
      "grad_norm": 1.7514744997024536,
      "learning_rate": 4.982690534592837e-05,
      "loss": 1.8162,
      "step": 200
    },
    {
      "epoch": 0.010959333046303182,
      "grad_norm": 2.1112804412841797,
      "learning_rate": 4.981820712210567e-05,
      "loss": 1.8158,
      "step": 210
    },
    {
      "epoch": 0.011481206048508096,
      "grad_norm": 1.8055827617645264,
      "learning_rate": 4.9809508898282974e-05,
      "loss": 1.6036,
      "step": 220
    },
    {
      "epoch": 0.012003079050713008,
      "grad_norm": 2.484880208969116,
      "learning_rate": 4.980081067446028e-05,
      "loss": 1.7974,
      "step": 230
    },
    {
      "epoch": 0.012524952052917922,
      "grad_norm": 2.7431023120880127,
      "learning_rate": 4.979211245063759e-05,
      "loss": 1.832,
      "step": 240
    },
    {
      "epoch": 0.013046825055122837,
      "grad_norm": 1.926494836807251,
      "learning_rate": 4.978341422681489e-05,
      "loss": 1.6534,
      "step": 250
    },
    {
      "epoch": 0.013568698057327749,
      "grad_norm": 2.3357186317443848,
      "learning_rate": 4.977471600299219e-05,
      "loss": 1.6621,
      "step": 260
    },
    {
      "epoch": 0.014090571059532663,
      "grad_norm": 2.264129400253296,
      "learning_rate": 4.976601777916949e-05,
      "loss": 1.8219,
      "step": 270
    },
    {
      "epoch": 0.014612444061737576,
      "grad_norm": 2.2334940433502197,
      "learning_rate": 4.97573195553468e-05,
      "loss": 1.7326,
      "step": 280
    },
    {
      "epoch": 0.01513431706394249,
      "grad_norm": 2.529602527618408,
      "learning_rate": 4.9748621331524105e-05,
      "loss": 1.671,
      "step": 290
    },
    {
      "epoch": 0.015656190066147404,
      "grad_norm": 2.8113481998443604,
      "learning_rate": 4.973992310770141e-05,
      "loss": 1.5528,
      "step": 300
    },
    {
      "epoch": 0.016178063068352316,
      "grad_norm": 2.000282049179077,
      "learning_rate": 4.973122488387871e-05,
      "loss": 1.5937,
      "step": 310
    },
    {
      "epoch": 0.01669993607055723,
      "grad_norm": 2.1280839443206787,
      "learning_rate": 4.9722526660056016e-05,
      "loss": 1.7754,
      "step": 320
    },
    {
      "epoch": 0.017221809072762145,
      "grad_norm": 2.7706525325775146,
      "learning_rate": 4.971382843623332e-05,
      "loss": 1.5628,
      "step": 330
    },
    {
      "epoch": 0.017743682074967057,
      "grad_norm": 1.985141396522522,
      "learning_rate": 4.970513021241063e-05,
      "loss": 1.3251,
      "step": 340
    },
    {
      "epoch": 0.01826555507717197,
      "grad_norm": 2.879547119140625,
      "learning_rate": 4.9696431988587934e-05,
      "loss": 1.4561,
      "step": 350
    },
    {
      "epoch": 0.018787428079376882,
      "grad_norm": 3.2433981895446777,
      "learning_rate": 4.968773376476524e-05,
      "loss": 1.5564,
      "step": 360
    },
    {
      "epoch": 0.019309301081581798,
      "grad_norm": 2.5769972801208496,
      "learning_rate": 4.967903554094254e-05,
      "loss": 1.575,
      "step": 370
    },
    {
      "epoch": 0.01983117408378671,
      "grad_norm": 2.964627981185913,
      "learning_rate": 4.9670337317119844e-05,
      "loss": 1.4976,
      "step": 380
    },
    {
      "epoch": 0.020353047085991623,
      "grad_norm": 2.816707134246826,
      "learning_rate": 4.9661639093297154e-05,
      "loss": 1.6039,
      "step": 390
    },
    {
      "epoch": 0.02087492008819654,
      "grad_norm": 2.84751296043396,
      "learning_rate": 4.965294086947446e-05,
      "loss": 1.3937,
      "step": 400
    },
    {
      "epoch": 0.02139679309040145,
      "grad_norm": 2.47048282623291,
      "learning_rate": 4.964424264565176e-05,
      "loss": 1.4341,
      "step": 410
    },
    {
      "epoch": 0.021918666092606363,
      "grad_norm": 2.4889187812805176,
      "learning_rate": 4.9635544421829065e-05,
      "loss": 1.5128,
      "step": 420
    },
    {
      "epoch": 0.02244053909481128,
      "grad_norm": 2.6338510513305664,
      "learning_rate": 4.962684619800637e-05,
      "loss": 1.4898,
      "step": 430
    },
    {
      "epoch": 0.02296241209701619,
      "grad_norm": 3.555999279022217,
      "learning_rate": 4.961814797418368e-05,
      "loss": 1.4849,
      "step": 440
    },
    {
      "epoch": 0.023484285099221104,
      "grad_norm": 3.079092025756836,
      "learning_rate": 4.960944975036098e-05,
      "loss": 1.5565,
      "step": 450
    },
    {
      "epoch": 0.024006158101426017,
      "grad_norm": 2.8825345039367676,
      "learning_rate": 4.960075152653828e-05,
      "loss": 1.5737,
      "step": 460
    },
    {
      "epoch": 0.024528031103630932,
      "grad_norm": 2.562434673309326,
      "learning_rate": 4.959205330271558e-05,
      "loss": 1.5764,
      "step": 470
    },
    {
      "epoch": 0.025049904105835845,
      "grad_norm": 3.1025543212890625,
      "learning_rate": 4.958335507889289e-05,
      "loss": 1.5474,
      "step": 480
    },
    {
      "epoch": 0.025571777108040757,
      "grad_norm": 2.566416025161743,
      "learning_rate": 4.95746568550702e-05,
      "loss": 1.4634,
      "step": 490
    },
    {
      "epoch": 0.026093650110245673,
      "grad_norm": 2.905660629272461,
      "learning_rate": 4.95659586312475e-05,
      "loss": 1.4779,
      "step": 500
    },
    {
      "epoch": 0.026615523112450586,
      "grad_norm": 3.1174113750457764,
      "learning_rate": 4.9557260407424804e-05,
      "loss": 1.4373,
      "step": 510
    },
    {
      "epoch": 0.027137396114655498,
      "grad_norm": 3.0767996311187744,
      "learning_rate": 4.954856218360211e-05,
      "loss": 1.4947,
      "step": 520
    },
    {
      "epoch": 0.02765926911686041,
      "grad_norm": 3.706618547439575,
      "learning_rate": 4.953986395977942e-05,
      "loss": 1.5045,
      "step": 530
    },
    {
      "epoch": 0.028181142119065326,
      "grad_norm": 2.789731502532959,
      "learning_rate": 4.953116573595672e-05,
      "loss": 1.5351,
      "step": 540
    },
    {
      "epoch": 0.02870301512127024,
      "grad_norm": 2.9555823802948,
      "learning_rate": 4.9522467512134025e-05,
      "loss": 1.5006,
      "step": 550
    },
    {
      "epoch": 0.02922488812347515,
      "grad_norm": 3.3693909645080566,
      "learning_rate": 4.951376928831133e-05,
      "loss": 1.486,
      "step": 560
    },
    {
      "epoch": 0.029746761125680067,
      "grad_norm": 2.684452772140503,
      "learning_rate": 4.950507106448863e-05,
      "loss": 1.4131,
      "step": 570
    },
    {
      "epoch": 0.03026863412788498,
      "grad_norm": 2.76845645904541,
      "learning_rate": 4.9496372840665935e-05,
      "loss": 1.3389,
      "step": 580
    },
    {
      "epoch": 0.030790507130089892,
      "grad_norm": 2.9077000617980957,
      "learning_rate": 4.9487674616843246e-05,
      "loss": 1.4538,
      "step": 590
    },
    {
      "epoch": 0.03131238013229481,
      "grad_norm": 2.3675880432128906,
      "learning_rate": 4.947897639302055e-05,
      "loss": 1.4008,
      "step": 600
    },
    {
      "epoch": 0.03183425313449972,
      "grad_norm": 3.7411673069000244,
      "learning_rate": 4.947027816919785e-05,
      "loss": 1.4984,
      "step": 610
    },
    {
      "epoch": 0.03235612613670463,
      "grad_norm": 3.785940408706665,
      "learning_rate": 4.9461579945375156e-05,
      "loss": 1.4153,
      "step": 620
    },
    {
      "epoch": 0.03287799913890955,
      "grad_norm": 3.3681752681732178,
      "learning_rate": 4.945288172155246e-05,
      "loss": 1.4503,
      "step": 630
    },
    {
      "epoch": 0.03339987214111446,
      "grad_norm": 3.8524513244628906,
      "learning_rate": 4.944418349772977e-05,
      "loss": 1.403,
      "step": 640
    },
    {
      "epoch": 0.03392174514331937,
      "grad_norm": 2.4813778400421143,
      "learning_rate": 4.9435485273907074e-05,
      "loss": 1.4483,
      "step": 650
    },
    {
      "epoch": 0.03444361814552429,
      "grad_norm": 3.8608410358428955,
      "learning_rate": 4.942678705008438e-05,
      "loss": 1.3119,
      "step": 660
    },
    {
      "epoch": 0.0349654911477292,
      "grad_norm": 3.211374282836914,
      "learning_rate": 4.9418088826261674e-05,
      "loss": 1.3018,
      "step": 670
    },
    {
      "epoch": 0.035487364149934114,
      "grad_norm": 3.7182884216308594,
      "learning_rate": 4.9409390602438985e-05,
      "loss": 1.2582,
      "step": 680
    },
    {
      "epoch": 0.03600923715213903,
      "grad_norm": 4.17170524597168,
      "learning_rate": 4.940069237861629e-05,
      "loss": 1.2887,
      "step": 690
    },
    {
      "epoch": 0.03653111015434394,
      "grad_norm": 4.07683801651001,
      "learning_rate": 4.939199415479359e-05,
      "loss": 1.2388,
      "step": 700
    },
    {
      "epoch": 0.037052983156548855,
      "grad_norm": 3.8603835105895996,
      "learning_rate": 4.9383295930970895e-05,
      "loss": 1.4293,
      "step": 710
    },
    {
      "epoch": 0.037574856158753764,
      "grad_norm": 2.2367632389068604,
      "learning_rate": 4.93745977071482e-05,
      "loss": 1.3899,
      "step": 720
    },
    {
      "epoch": 0.03809672916095868,
      "grad_norm": 3.7765889167785645,
      "learning_rate": 4.936589948332551e-05,
      "loss": 1.4199,
      "step": 730
    },
    {
      "epoch": 0.038618602163163596,
      "grad_norm": 3.8727071285247803,
      "learning_rate": 4.935720125950281e-05,
      "loss": 1.4298,
      "step": 740
    },
    {
      "epoch": 0.039140475165368505,
      "grad_norm": 3.4822678565979004,
      "learning_rate": 4.9348503035680116e-05,
      "loss": 1.2592,
      "step": 750
    },
    {
      "epoch": 0.03966234816757342,
      "grad_norm": 3.2666139602661133,
      "learning_rate": 4.933980481185742e-05,
      "loss": 1.36,
      "step": 760
    },
    {
      "epoch": 0.040184221169778336,
      "grad_norm": 3.443748712539673,
      "learning_rate": 4.933110658803472e-05,
      "loss": 1.4378,
      "step": 770
    },
    {
      "epoch": 0.040706094171983245,
      "grad_norm": 3.874762773513794,
      "learning_rate": 4.9322408364212034e-05,
      "loss": 1.4232,
      "step": 780
    },
    {
      "epoch": 0.04122796717418816,
      "grad_norm": 2.795255422592163,
      "learning_rate": 4.931371014038934e-05,
      "loss": 1.3881,
      "step": 790
    },
    {
      "epoch": 0.04174984017639308,
      "grad_norm": 4.294288635253906,
      "learning_rate": 4.930501191656664e-05,
      "loss": 1.3659,
      "step": 800
    },
    {
      "epoch": 0.042271713178597986,
      "grad_norm": 3.680891513824463,
      "learning_rate": 4.9296313692743944e-05,
      "loss": 1.4321,
      "step": 810
    },
    {
      "epoch": 0.0427935861808029,
      "grad_norm": 3.731255531311035,
      "learning_rate": 4.928761546892125e-05,
      "loss": 1.3844,
      "step": 820
    },
    {
      "epoch": 0.04331545918300782,
      "grad_norm": 3.671578884124756,
      "learning_rate": 4.927891724509856e-05,
      "loss": 1.3384,
      "step": 830
    },
    {
      "epoch": 0.04383733218521273,
      "grad_norm": 3.5626227855682373,
      "learning_rate": 4.927021902127586e-05,
      "loss": 1.3159,
      "step": 840
    },
    {
      "epoch": 0.04435920518741764,
      "grad_norm": 3.2102246284484863,
      "learning_rate": 4.9261520797453165e-05,
      "loss": 1.3164,
      "step": 850
    },
    {
      "epoch": 0.04488107818962256,
      "grad_norm": 3.4637906551361084,
      "learning_rate": 4.925282257363047e-05,
      "loss": 1.4429,
      "step": 860
    },
    {
      "epoch": 0.04540295119182747,
      "grad_norm": 3.9940083026885986,
      "learning_rate": 4.924412434980777e-05,
      "loss": 1.5367,
      "step": 870
    },
    {
      "epoch": 0.04592482419403238,
      "grad_norm": 3.2253408432006836,
      "learning_rate": 4.9235426125985076e-05,
      "loss": 1.327,
      "step": 880
    },
    {
      "epoch": 0.04644669719623729,
      "grad_norm": 3.483914852142334,
      "learning_rate": 4.922672790216238e-05,
      "loss": 1.3737,
      "step": 890
    },
    {
      "epoch": 0.04696857019844221,
      "grad_norm": 3.56736421585083,
      "learning_rate": 4.921802967833968e-05,
      "loss": 1.3099,
      "step": 900
    },
    {
      "epoch": 0.047490443200647124,
      "grad_norm": 3.6014065742492676,
      "learning_rate": 4.9209331454516986e-05,
      "loss": 1.3485,
      "step": 910
    },
    {
      "epoch": 0.04801231620285203,
      "grad_norm": 3.8285446166992188,
      "learning_rate": 4.920063323069429e-05,
      "loss": 1.2073,
      "step": 920
    },
    {
      "epoch": 0.04853418920505695,
      "grad_norm": 3.2428622245788574,
      "learning_rate": 4.91919350068716e-05,
      "loss": 1.3065,
      "step": 930
    },
    {
      "epoch": 0.049056062207261865,
      "grad_norm": 3.1871824264526367,
      "learning_rate": 4.9183236783048904e-05,
      "loss": 1.2035,
      "step": 940
    },
    {
      "epoch": 0.049577935209466774,
      "grad_norm": 3.264211893081665,
      "learning_rate": 4.917453855922621e-05,
      "loss": 1.2962,
      "step": 950
    },
    {
      "epoch": 0.05009980821167169,
      "grad_norm": 3.34749174118042,
      "learning_rate": 4.916584033540351e-05,
      "loss": 1.2284,
      "step": 960
    },
    {
      "epoch": 0.050621681213876606,
      "grad_norm": 3.707674264907837,
      "learning_rate": 4.9157142111580815e-05,
      "loss": 1.3551,
      "step": 970
    },
    {
      "epoch": 0.051143554216081515,
      "grad_norm": 3.1903958320617676,
      "learning_rate": 4.9148443887758125e-05,
      "loss": 1.3547,
      "step": 980
    },
    {
      "epoch": 0.05166542721828643,
      "grad_norm": 4.1486005783081055,
      "learning_rate": 4.913974566393543e-05,
      "loss": 1.3458,
      "step": 990
    },
    {
      "epoch": 0.052187300220491346,
      "grad_norm": 3.9222524166107178,
      "learning_rate": 4.913104744011273e-05,
      "loss": 1.4112,
      "step": 1000
    },
    {
      "epoch": 0.052709173222696255,
      "grad_norm": 3.364733934402466,
      "learning_rate": 4.9122349216290036e-05,
      "loss": 1.3431,
      "step": 1010
    },
    {
      "epoch": 0.05323104622490117,
      "grad_norm": 3.772291421890259,
      "learning_rate": 4.911365099246734e-05,
      "loss": 1.4078,
      "step": 1020
    },
    {
      "epoch": 0.05375291922710609,
      "grad_norm": 4.183650016784668,
      "learning_rate": 4.910495276864465e-05,
      "loss": 1.2989,
      "step": 1030
    },
    {
      "epoch": 0.054274792229310996,
      "grad_norm": 4.745306491851807,
      "learning_rate": 4.909625454482195e-05,
      "loss": 1.3608,
      "step": 1040
    },
    {
      "epoch": 0.05479666523151591,
      "grad_norm": 3.2633469104766846,
      "learning_rate": 4.9087556320999256e-05,
      "loss": 1.2556,
      "step": 1050
    },
    {
      "epoch": 0.05531853823372082,
      "grad_norm": 3.5928361415863037,
      "learning_rate": 4.907885809717656e-05,
      "loss": 1.2536,
      "step": 1060
    },
    {
      "epoch": 0.05584041123592574,
      "grad_norm": 3.542764902114868,
      "learning_rate": 4.9070159873353864e-05,
      "loss": 1.2623,
      "step": 1070
    },
    {
      "epoch": 0.05636228423813065,
      "grad_norm": 3.7368040084838867,
      "learning_rate": 4.9061461649531174e-05,
      "loss": 1.2306,
      "step": 1080
    },
    {
      "epoch": 0.05688415724033556,
      "grad_norm": 2.860596179962158,
      "learning_rate": 4.905276342570847e-05,
      "loss": 1.2599,
      "step": 1090
    },
    {
      "epoch": 0.05740603024254048,
      "grad_norm": 4.641060829162598,
      "learning_rate": 4.9044065201885774e-05,
      "loss": 1.2147,
      "step": 1100
    },
    {
      "epoch": 0.05792790324474539,
      "grad_norm": 3.8180296421051025,
      "learning_rate": 4.903536697806308e-05,
      "loss": 1.2566,
      "step": 1110
    },
    {
      "epoch": 0.0584497762469503,
      "grad_norm": 3.271965265274048,
      "learning_rate": 4.902666875424038e-05,
      "loss": 1.331,
      "step": 1120
    },
    {
      "epoch": 0.05897164924915522,
      "grad_norm": 2.954723596572876,
      "learning_rate": 4.901797053041769e-05,
      "loss": 1.2685,
      "step": 1130
    },
    {
      "epoch": 0.059493522251360134,
      "grad_norm": 3.5536859035491943,
      "learning_rate": 4.9009272306594995e-05,
      "loss": 1.3093,
      "step": 1140
    },
    {
      "epoch": 0.06001539525356504,
      "grad_norm": 4.2792534828186035,
      "learning_rate": 4.90005740827723e-05,
      "loss": 1.2766,
      "step": 1150
    },
    {
      "epoch": 0.06053726825576996,
      "grad_norm": 4.9532318115234375,
      "learning_rate": 4.89918758589496e-05,
      "loss": 1.5277,
      "step": 1160
    },
    {
      "epoch": 0.061059141257974875,
      "grad_norm": 4.107179164886475,
      "learning_rate": 4.8983177635126906e-05,
      "loss": 1.4071,
      "step": 1170
    },
    {
      "epoch": 0.061581014260179784,
      "grad_norm": 3.929865598678589,
      "learning_rate": 4.8974479411304216e-05,
      "loss": 1.3921,
      "step": 1180
    },
    {
      "epoch": 0.0621028872623847,
      "grad_norm": 2.712247848510742,
      "learning_rate": 4.896578118748152e-05,
      "loss": 1.2874,
      "step": 1190
    },
    {
      "epoch": 0.06262476026458962,
      "grad_norm": 3.7718446254730225,
      "learning_rate": 4.895708296365882e-05,
      "loss": 1.3135,
      "step": 1200
    },
    {
      "epoch": 0.06314663326679452,
      "grad_norm": 4.0610504150390625,
      "learning_rate": 4.894838473983613e-05,
      "loss": 1.1908,
      "step": 1210
    },
    {
      "epoch": 0.06366850626899943,
      "grad_norm": 5.282115936279297,
      "learning_rate": 4.893968651601343e-05,
      "loss": 1.3244,
      "step": 1220
    },
    {
      "epoch": 0.06419037927120436,
      "grad_norm": 4.293414115905762,
      "learning_rate": 4.893098829219074e-05,
      "loss": 1.4172,
      "step": 1230
    },
    {
      "epoch": 0.06471225227340927,
      "grad_norm": 4.009479999542236,
      "learning_rate": 4.8922290068368044e-05,
      "loss": 1.4451,
      "step": 1240
    },
    {
      "epoch": 0.06523412527561417,
      "grad_norm": 3.891448497772217,
      "learning_rate": 4.891359184454535e-05,
      "loss": 1.4947,
      "step": 1250
    },
    {
      "epoch": 0.0657559982778191,
      "grad_norm": 3.9627928733825684,
      "learning_rate": 4.890489362072265e-05,
      "loss": 1.3217,
      "step": 1260
    },
    {
      "epoch": 0.066277871280024,
      "grad_norm": 3.123605489730835,
      "learning_rate": 4.8896195396899955e-05,
      "loss": 1.2114,
      "step": 1270
    },
    {
      "epoch": 0.06679974428222892,
      "grad_norm": 4.314133644104004,
      "learning_rate": 4.8887497173077265e-05,
      "loss": 1.4255,
      "step": 1280
    },
    {
      "epoch": 0.06732161728443384,
      "grad_norm": 2.9247288703918457,
      "learning_rate": 4.887879894925457e-05,
      "loss": 1.2163,
      "step": 1290
    },
    {
      "epoch": 0.06784349028663875,
      "grad_norm": 5.255885124206543,
      "learning_rate": 4.8870100725431866e-05,
      "loss": 1.2467,
      "step": 1300
    },
    {
      "epoch": 0.06836536328884366,
      "grad_norm": 4.797480583190918,
      "learning_rate": 4.886140250160917e-05,
      "loss": 1.2949,
      "step": 1310
    },
    {
      "epoch": 0.06888723629104858,
      "grad_norm": 3.6118366718292236,
      "learning_rate": 4.885270427778648e-05,
      "loss": 1.2959,
      "step": 1320
    },
    {
      "epoch": 0.06940910929325349,
      "grad_norm": 3.9599127769470215,
      "learning_rate": 4.884400605396378e-05,
      "loss": 1.4399,
      "step": 1330
    },
    {
      "epoch": 0.0699309822954584,
      "grad_norm": 4.757137775421143,
      "learning_rate": 4.8835307830141087e-05,
      "loss": 1.1279,
      "step": 1340
    },
    {
      "epoch": 0.07045285529766332,
      "grad_norm": 4.012228012084961,
      "learning_rate": 4.882660960631839e-05,
      "loss": 1.3244,
      "step": 1350
    },
    {
      "epoch": 0.07097472829986823,
      "grad_norm": 3.5286290645599365,
      "learning_rate": 4.8817911382495694e-05,
      "loss": 1.1312,
      "step": 1360
    },
    {
      "epoch": 0.07149660130207314,
      "grad_norm": 3.702139139175415,
      "learning_rate": 4.8809213158673004e-05,
      "loss": 1.2337,
      "step": 1370
    },
    {
      "epoch": 0.07201847430427806,
      "grad_norm": 4.338963031768799,
      "learning_rate": 4.880051493485031e-05,
      "loss": 1.2138,
      "step": 1380
    },
    {
      "epoch": 0.07254034730648297,
      "grad_norm": 4.957271099090576,
      "learning_rate": 4.879181671102761e-05,
      "loss": 1.3277,
      "step": 1390
    },
    {
      "epoch": 0.07306222030868788,
      "grad_norm": 4.705838203430176,
      "learning_rate": 4.8783118487204915e-05,
      "loss": 1.2128,
      "step": 1400
    },
    {
      "epoch": 0.0735840933108928,
      "grad_norm": 3.5775306224823,
      "learning_rate": 4.877442026338222e-05,
      "loss": 1.2164,
      "step": 1410
    },
    {
      "epoch": 0.07410596631309771,
      "grad_norm": 4.337247371673584,
      "learning_rate": 4.876572203955952e-05,
      "loss": 1.2875,
      "step": 1420
    },
    {
      "epoch": 0.07462783931530262,
      "grad_norm": 5.842692852020264,
      "learning_rate": 4.875702381573683e-05,
      "loss": 1.2125,
      "step": 1430
    },
    {
      "epoch": 0.07514971231750753,
      "grad_norm": 4.187148571014404,
      "learning_rate": 4.8748325591914136e-05,
      "loss": 1.2965,
      "step": 1440
    },
    {
      "epoch": 0.07567158531971245,
      "grad_norm": 4.55433988571167,
      "learning_rate": 4.873962736809144e-05,
      "loss": 1.138,
      "step": 1450
    },
    {
      "epoch": 0.07619345832191736,
      "grad_norm": 3.756624937057495,
      "learning_rate": 4.873092914426874e-05,
      "loss": 1.3209,
      "step": 1460
    },
    {
      "epoch": 0.07671533132412227,
      "grad_norm": 5.172237396240234,
      "learning_rate": 4.8722230920446046e-05,
      "loss": 1.2707,
      "step": 1470
    },
    {
      "epoch": 0.07723720432632719,
      "grad_norm": 4.758967399597168,
      "learning_rate": 4.8713532696623357e-05,
      "loss": 1.2921,
      "step": 1480
    },
    {
      "epoch": 0.0777590773285321,
      "grad_norm": 4.1253862380981445,
      "learning_rate": 4.870483447280066e-05,
      "loss": 1.3123,
      "step": 1490
    },
    {
      "epoch": 0.07828095033073701,
      "grad_norm": 4.072080135345459,
      "learning_rate": 4.8696136248977964e-05,
      "loss": 1.3804,
      "step": 1500
    },
    {
      "epoch": 0.07880282333294193,
      "grad_norm": 3.716637372970581,
      "learning_rate": 4.868743802515526e-05,
      "loss": 1.1893,
      "step": 1510
    },
    {
      "epoch": 0.07932469633514684,
      "grad_norm": 3.958717107772827,
      "learning_rate": 4.867873980133257e-05,
      "loss": 1.3927,
      "step": 1520
    },
    {
      "epoch": 0.07984656933735175,
      "grad_norm": 4.942791938781738,
      "learning_rate": 4.8670041577509874e-05,
      "loss": 1.3271,
      "step": 1530
    },
    {
      "epoch": 0.08036844233955667,
      "grad_norm": 3.464057207107544,
      "learning_rate": 4.866134335368718e-05,
      "loss": 1.1079,
      "step": 1540
    },
    {
      "epoch": 0.08089031534176158,
      "grad_norm": 3.8934249877929688,
      "learning_rate": 4.865264512986448e-05,
      "loss": 1.1738,
      "step": 1550
    },
    {
      "epoch": 0.08141218834396649,
      "grad_norm": 4.100334644317627,
      "learning_rate": 4.8643946906041785e-05,
      "loss": 1.276,
      "step": 1560
    },
    {
      "epoch": 0.08193406134617141,
      "grad_norm": 4.275533199310303,
      "learning_rate": 4.8635248682219095e-05,
      "loss": 1.3067,
      "step": 1570
    },
    {
      "epoch": 0.08245593434837632,
      "grad_norm": 4.382184028625488,
      "learning_rate": 4.86265504583964e-05,
      "loss": 1.2379,
      "step": 1580
    },
    {
      "epoch": 0.08297780735058123,
      "grad_norm": 3.868302583694458,
      "learning_rate": 4.86178522345737e-05,
      "loss": 1.2018,
      "step": 1590
    },
    {
      "epoch": 0.08349968035278615,
      "grad_norm": 3.393676996231079,
      "learning_rate": 4.8609154010751006e-05,
      "loss": 1.2835,
      "step": 1600
    },
    {
      "epoch": 0.08402155335499106,
      "grad_norm": 4.360479354858398,
      "learning_rate": 4.860045578692831e-05,
      "loss": 1.2893,
      "step": 1610
    },
    {
      "epoch": 0.08454342635719597,
      "grad_norm": 4.529013633728027,
      "learning_rate": 4.859175756310562e-05,
      "loss": 1.2825,
      "step": 1620
    },
    {
      "epoch": 0.0850652993594009,
      "grad_norm": 4.597740173339844,
      "learning_rate": 4.858305933928292e-05,
      "loss": 1.3303,
      "step": 1630
    },
    {
      "epoch": 0.0855871723616058,
      "grad_norm": 3.809920310974121,
      "learning_rate": 4.857436111546023e-05,
      "loss": 1.1929,
      "step": 1640
    },
    {
      "epoch": 0.08610904536381071,
      "grad_norm": 4.639028072357178,
      "learning_rate": 4.856566289163753e-05,
      "loss": 1.2157,
      "step": 1650
    },
    {
      "epoch": 0.08663091836601564,
      "grad_norm": 3.8639066219329834,
      "learning_rate": 4.8556964667814834e-05,
      "loss": 1.2429,
      "step": 1660
    },
    {
      "epoch": 0.08715279136822054,
      "grad_norm": 4.506718158721924,
      "learning_rate": 4.854826644399214e-05,
      "loss": 1.2052,
      "step": 1670
    },
    {
      "epoch": 0.08767466437042545,
      "grad_norm": 4.389774799346924,
      "learning_rate": 4.853956822016945e-05,
      "loss": 1.1678,
      "step": 1680
    },
    {
      "epoch": 0.08819653737263038,
      "grad_norm": 3.9134867191314697,
      "learning_rate": 4.853086999634675e-05,
      "loss": 1.2795,
      "step": 1690
    },
    {
      "epoch": 0.08871841037483529,
      "grad_norm": 4.344224452972412,
      "learning_rate": 4.8522171772524055e-05,
      "loss": 1.2156,
      "step": 1700
    },
    {
      "epoch": 0.0892402833770402,
      "grad_norm": 3.555469512939453,
      "learning_rate": 4.851347354870135e-05,
      "loss": 1.1517,
      "step": 1710
    },
    {
      "epoch": 0.08976215637924512,
      "grad_norm": 3.9610745906829834,
      "learning_rate": 4.850477532487866e-05,
      "loss": 1.1467,
      "step": 1720
    },
    {
      "epoch": 0.09028402938145003,
      "grad_norm": 4.191634178161621,
      "learning_rate": 4.8496077101055966e-05,
      "loss": 1.3682,
      "step": 1730
    },
    {
      "epoch": 0.09080590238365494,
      "grad_norm": 3.698437213897705,
      "learning_rate": 4.848737887723327e-05,
      "loss": 1.0617,
      "step": 1740
    },
    {
      "epoch": 0.09132777538585986,
      "grad_norm": 4.0251922607421875,
      "learning_rate": 4.847868065341057e-05,
      "loss": 1.1664,
      "step": 1750
    },
    {
      "epoch": 0.09184964838806477,
      "grad_norm": 4.037930488586426,
      "learning_rate": 4.8469982429587876e-05,
      "loss": 1.2224,
      "step": 1760
    },
    {
      "epoch": 0.09237152139026968,
      "grad_norm": 5.38945198059082,
      "learning_rate": 4.8461284205765187e-05,
      "loss": 1.2446,
      "step": 1770
    },
    {
      "epoch": 0.09289339439247458,
      "grad_norm": 4.135680198669434,
      "learning_rate": 4.845258598194249e-05,
      "loss": 1.1989,
      "step": 1780
    },
    {
      "epoch": 0.09341526739467951,
      "grad_norm": 3.4719088077545166,
      "learning_rate": 4.8443887758119794e-05,
      "loss": 1.2609,
      "step": 1790
    },
    {
      "epoch": 0.09393714039688442,
      "grad_norm": 3.047821044921875,
      "learning_rate": 4.84351895342971e-05,
      "loss": 1.2301,
      "step": 1800
    },
    {
      "epoch": 0.09445901339908933,
      "grad_norm": 5.195559978485107,
      "learning_rate": 4.84264913104744e-05,
      "loss": 1.2289,
      "step": 1810
    },
    {
      "epoch": 0.09498088640129425,
      "grad_norm": 4.517579555511475,
      "learning_rate": 4.841779308665171e-05,
      "loss": 1.2121,
      "step": 1820
    },
    {
      "epoch": 0.09550275940349916,
      "grad_norm": 4.894623756408691,
      "learning_rate": 4.8409094862829015e-05,
      "loss": 1.1901,
      "step": 1830
    },
    {
      "epoch": 0.09602463240570407,
      "grad_norm": 4.755618095397949,
      "learning_rate": 4.840039663900632e-05,
      "loss": 1.1858,
      "step": 1840
    },
    {
      "epoch": 0.09654650540790899,
      "grad_norm": 6.124118804931641,
      "learning_rate": 4.839169841518362e-05,
      "loss": 1.218,
      "step": 1850
    },
    {
      "epoch": 0.0970683784101139,
      "grad_norm": 4.467469215393066,
      "learning_rate": 4.8383000191360925e-05,
      "loss": 1.3208,
      "step": 1860
    },
    {
      "epoch": 0.09759025141231881,
      "grad_norm": 2.9466326236724854,
      "learning_rate": 4.8374301967538236e-05,
      "loss": 1.1751,
      "step": 1870
    },
    {
      "epoch": 0.09811212441452373,
      "grad_norm": 4.664296627044678,
      "learning_rate": 4.836560374371554e-05,
      "loss": 1.1846,
      "step": 1880
    },
    {
      "epoch": 0.09863399741672864,
      "grad_norm": 3.747501850128174,
      "learning_rate": 4.835690551989284e-05,
      "loss": 1.2192,
      "step": 1890
    },
    {
      "epoch": 0.09915587041893355,
      "grad_norm": 3.948155403137207,
      "learning_rate": 4.8348207296070146e-05,
      "loss": 1.1557,
      "step": 1900
    },
    {
      "epoch": 0.09967774342113847,
      "grad_norm": 4.653749942779541,
      "learning_rate": 4.833950907224745e-05,
      "loss": 1.1371,
      "step": 1910
    },
    {
      "epoch": 0.10019961642334338,
      "grad_norm": 4.779521942138672,
      "learning_rate": 4.833081084842475e-05,
      "loss": 1.3134,
      "step": 1920
    },
    {
      "epoch": 0.10072148942554829,
      "grad_norm": 5.187239646911621,
      "learning_rate": 4.832211262460206e-05,
      "loss": 1.2265,
      "step": 1930
    },
    {
      "epoch": 0.10124336242775321,
      "grad_norm": 3.765813112258911,
      "learning_rate": 4.831341440077936e-05,
      "loss": 1.1534,
      "step": 1940
    },
    {
      "epoch": 0.10176523542995812,
      "grad_norm": 4.5228447914123535,
      "learning_rate": 4.8304716176956664e-05,
      "loss": 1.1738,
      "step": 1950
    },
    {
      "epoch": 0.10228710843216303,
      "grad_norm": 5.355452537536621,
      "learning_rate": 4.829601795313397e-05,
      "loss": 1.1251,
      "step": 1960
    },
    {
      "epoch": 0.10280898143436795,
      "grad_norm": 4.5893354415893555,
      "learning_rate": 4.828731972931128e-05,
      "loss": 1.2137,
      "step": 1970
    },
    {
      "epoch": 0.10333085443657286,
      "grad_norm": 4.192361354827881,
      "learning_rate": 4.827862150548858e-05,
      "loss": 1.1905,
      "step": 1980
    },
    {
      "epoch": 0.10385272743877777,
      "grad_norm": 3.066883087158203,
      "learning_rate": 4.8269923281665885e-05,
      "loss": 1.1507,
      "step": 1990
    },
    {
      "epoch": 0.10437460044098269,
      "grad_norm": 4.855025291442871,
      "learning_rate": 4.826122505784319e-05,
      "loss": 1.0518,
      "step": 2000
    },
    {
      "epoch": 0.1048964734431876,
      "grad_norm": 4.221827983856201,
      "learning_rate": 4.825252683402049e-05,
      "loss": 1.1458,
      "step": 2010
    },
    {
      "epoch": 0.10541834644539251,
      "grad_norm": 3.633321523666382,
      "learning_rate": 4.82438286101978e-05,
      "loss": 1.2125,
      "step": 2020
    },
    {
      "epoch": 0.10594021944759743,
      "grad_norm": 5.642920017242432,
      "learning_rate": 4.8235130386375106e-05,
      "loss": 1.3066,
      "step": 2030
    },
    {
      "epoch": 0.10646209244980234,
      "grad_norm": 4.150033473968506,
      "learning_rate": 4.822643216255241e-05,
      "loss": 1.2514,
      "step": 2040
    },
    {
      "epoch": 0.10698396545200725,
      "grad_norm": 4.425230503082275,
      "learning_rate": 4.821773393872971e-05,
      "loss": 1.239,
      "step": 2050
    },
    {
      "epoch": 0.10750583845421217,
      "grad_norm": 4.385861873626709,
      "learning_rate": 4.8209035714907017e-05,
      "loss": 1.24,
      "step": 2060
    },
    {
      "epoch": 0.10802771145641708,
      "grad_norm": 4.575933456420898,
      "learning_rate": 4.820033749108433e-05,
      "loss": 1.2074,
      "step": 2070
    },
    {
      "epoch": 0.10854958445862199,
      "grad_norm": 4.968883037567139,
      "learning_rate": 4.819163926726163e-05,
      "loss": 1.2372,
      "step": 2080
    },
    {
      "epoch": 0.1090714574608269,
      "grad_norm": 3.841224431991577,
      "learning_rate": 4.8182941043438934e-05,
      "loss": 1.2357,
      "step": 2090
    },
    {
      "epoch": 0.10959333046303182,
      "grad_norm": 4.18332052230835,
      "learning_rate": 4.817424281961624e-05,
      "loss": 1.1854,
      "step": 2100
    },
    {
      "epoch": 0.11011520346523673,
      "grad_norm": 4.557206153869629,
      "learning_rate": 4.816554459579354e-05,
      "loss": 1.2471,
      "step": 2110
    },
    {
      "epoch": 0.11063707646744164,
      "grad_norm": 5.221555709838867,
      "learning_rate": 4.815684637197085e-05,
      "loss": 1.2941,
      "step": 2120
    },
    {
      "epoch": 0.11115894946964656,
      "grad_norm": 4.684722900390625,
      "learning_rate": 4.814814814814815e-05,
      "loss": 1.2043,
      "step": 2130
    },
    {
      "epoch": 0.11168082247185147,
      "grad_norm": 3.8434078693389893,
      "learning_rate": 4.813944992432545e-05,
      "loss": 1.1449,
      "step": 2140
    },
    {
      "epoch": 0.11220269547405638,
      "grad_norm": 4.257493495941162,
      "learning_rate": 4.8130751700502755e-05,
      "loss": 1.2251,
      "step": 2150
    },
    {
      "epoch": 0.1127245684762613,
      "grad_norm": 5.0902018547058105,
      "learning_rate": 4.8122053476680066e-05,
      "loss": 1.2071,
      "step": 2160
    },
    {
      "epoch": 0.11324644147846621,
      "grad_norm": 6.53706169128418,
      "learning_rate": 4.811335525285737e-05,
      "loss": 1.3246,
      "step": 2170
    },
    {
      "epoch": 0.11376831448067112,
      "grad_norm": 4.330099582672119,
      "learning_rate": 4.810465702903467e-05,
      "loss": 1.1855,
      "step": 2180
    },
    {
      "epoch": 0.11429018748287605,
      "grad_norm": 4.504106521606445,
      "learning_rate": 4.8095958805211976e-05,
      "loss": 1.0654,
      "step": 2190
    },
    {
      "epoch": 0.11481206048508096,
      "grad_norm": 4.875461578369141,
      "learning_rate": 4.808726058138928e-05,
      "loss": 1.1957,
      "step": 2200
    },
    {
      "epoch": 0.11533393348728586,
      "grad_norm": 3.9934158325195312,
      "learning_rate": 4.807856235756658e-05,
      "loss": 1.0615,
      "step": 2210
    },
    {
      "epoch": 0.11585580648949079,
      "grad_norm": 4.823777675628662,
      "learning_rate": 4.8069864133743894e-05,
      "loss": 1.2211,
      "step": 2220
    },
    {
      "epoch": 0.1163776794916957,
      "grad_norm": 4.581608295440674,
      "learning_rate": 4.80611659099212e-05,
      "loss": 1.226,
      "step": 2230
    },
    {
      "epoch": 0.1168995524939006,
      "grad_norm": 4.502267360687256,
      "learning_rate": 4.80524676860985e-05,
      "loss": 1.1317,
      "step": 2240
    },
    {
      "epoch": 0.11742142549610553,
      "grad_norm": 3.865330696105957,
      "learning_rate": 4.8043769462275804e-05,
      "loss": 1.0189,
      "step": 2250
    },
    {
      "epoch": 0.11794329849831044,
      "grad_norm": 4.979467391967773,
      "learning_rate": 4.803507123845311e-05,
      "loss": 1.2279,
      "step": 2260
    },
    {
      "epoch": 0.11846517150051535,
      "grad_norm": 4.330240726470947,
      "learning_rate": 4.802637301463042e-05,
      "loss": 1.1479,
      "step": 2270
    },
    {
      "epoch": 0.11898704450272027,
      "grad_norm": 4.788428783416748,
      "learning_rate": 4.801767479080772e-05,
      "loss": 1.0477,
      "step": 2280
    },
    {
      "epoch": 0.11950891750492518,
      "grad_norm": 4.228728771209717,
      "learning_rate": 4.8008976566985025e-05,
      "loss": 1.2637,
      "step": 2290
    },
    {
      "epoch": 0.12003079050713009,
      "grad_norm": 4.564756870269775,
      "learning_rate": 4.800027834316233e-05,
      "loss": 1.3378,
      "step": 2300
    },
    {
      "epoch": 0.12055266350933501,
      "grad_norm": 4.022025108337402,
      "learning_rate": 4.799158011933963e-05,
      "loss": 1.227,
      "step": 2310
    },
    {
      "epoch": 0.12107453651153992,
      "grad_norm": 4.231947422027588,
      "learning_rate": 4.798288189551694e-05,
      "loss": 1.1671,
      "step": 2320
    },
    {
      "epoch": 0.12159640951374483,
      "grad_norm": 6.121474742889404,
      "learning_rate": 4.7974183671694246e-05,
      "loss": 1.0817,
      "step": 2330
    },
    {
      "epoch": 0.12211828251594975,
      "grad_norm": 4.048398494720459,
      "learning_rate": 4.796548544787154e-05,
      "loss": 1.124,
      "step": 2340
    },
    {
      "epoch": 0.12264015551815466,
      "grad_norm": 5.232346057891846,
      "learning_rate": 4.7956787224048847e-05,
      "loss": 1.1699,
      "step": 2350
    },
    {
      "epoch": 0.12316202852035957,
      "grad_norm": 3.6558196544647217,
      "learning_rate": 4.794808900022616e-05,
      "loss": 1.1737,
      "step": 2360
    },
    {
      "epoch": 0.12368390152256449,
      "grad_norm": 5.440811634063721,
      "learning_rate": 4.793939077640346e-05,
      "loss": 1.1377,
      "step": 2370
    },
    {
      "epoch": 0.1242057745247694,
      "grad_norm": 5.424065113067627,
      "learning_rate": 4.7930692552580764e-05,
      "loss": 1.2126,
      "step": 2380
    },
    {
      "epoch": 0.12472764752697431,
      "grad_norm": 3.5665159225463867,
      "learning_rate": 4.792199432875807e-05,
      "loss": 1.1403,
      "step": 2390
    },
    {
      "epoch": 0.12524952052917923,
      "grad_norm": 3.7336390018463135,
      "learning_rate": 4.791329610493537e-05,
      "loss": 1.1805,
      "step": 2400
    },
    {
      "epoch": 0.12577139353138414,
      "grad_norm": 3.9068591594696045,
      "learning_rate": 4.790459788111268e-05,
      "loss": 1.1982,
      "step": 2410
    },
    {
      "epoch": 0.12629326653358905,
      "grad_norm": 5.486688613891602,
      "learning_rate": 4.7895899657289985e-05,
      "loss": 1.3028,
      "step": 2420
    },
    {
      "epoch": 0.12681513953579396,
      "grad_norm": 5.495454788208008,
      "learning_rate": 4.788720143346729e-05,
      "loss": 1.2487,
      "step": 2430
    },
    {
      "epoch": 0.12733701253799887,
      "grad_norm": 4.4906206130981445,
      "learning_rate": 4.787850320964459e-05,
      "loss": 1.2838,
      "step": 2440
    },
    {
      "epoch": 0.1278588855402038,
      "grad_norm": 4.751700401306152,
      "learning_rate": 4.7869804985821896e-05,
      "loss": 1.2018,
      "step": 2450
    },
    {
      "epoch": 0.1283807585424087,
      "grad_norm": 4.560762882232666,
      "learning_rate": 4.7861106761999206e-05,
      "loss": 1.1967,
      "step": 2460
    },
    {
      "epoch": 0.12890263154461362,
      "grad_norm": 4.877684116363525,
      "learning_rate": 4.785240853817651e-05,
      "loss": 1.1104,
      "step": 2470
    },
    {
      "epoch": 0.12942450454681853,
      "grad_norm": 4.9870991706848145,
      "learning_rate": 4.784371031435381e-05,
      "loss": 1.1366,
      "step": 2480
    },
    {
      "epoch": 0.12994637754902344,
      "grad_norm": 4.178779602050781,
      "learning_rate": 4.783501209053112e-05,
      "loss": 1.093,
      "step": 2490
    },
    {
      "epoch": 0.13046825055122835,
      "grad_norm": 4.271200180053711,
      "learning_rate": 4.782631386670842e-05,
      "loss": 1.1496,
      "step": 2500
    },
    {
      "epoch": 0.13099012355343329,
      "grad_norm": 3.8544676303863525,
      "learning_rate": 4.7817615642885724e-05,
      "loss": 1.1252,
      "step": 2510
    },
    {
      "epoch": 0.1315119965556382,
      "grad_norm": 2.9354357719421387,
      "learning_rate": 4.7808917419063034e-05,
      "loss": 1.1149,
      "step": 2520
    },
    {
      "epoch": 0.1320338695578431,
      "grad_norm": 5.205026626586914,
      "learning_rate": 4.780021919524034e-05,
      "loss": 1.2178,
      "step": 2530
    },
    {
      "epoch": 0.132555742560048,
      "grad_norm": 4.249312400817871,
      "learning_rate": 4.779152097141764e-05,
      "loss": 1.1953,
      "step": 2540
    },
    {
      "epoch": 0.13307761556225292,
      "grad_norm": 5.1366658210754395,
      "learning_rate": 4.778282274759494e-05,
      "loss": 1.1377,
      "step": 2550
    },
    {
      "epoch": 0.13359948856445783,
      "grad_norm": 5.468172073364258,
      "learning_rate": 4.777412452377225e-05,
      "loss": 1.2361,
      "step": 2560
    },
    {
      "epoch": 0.13412136156666274,
      "grad_norm": 5.157582759857178,
      "learning_rate": 4.776542629994955e-05,
      "loss": 1.1954,
      "step": 2570
    },
    {
      "epoch": 0.13464323456886768,
      "grad_norm": 4.131276607513428,
      "learning_rate": 4.7756728076126855e-05,
      "loss": 1.1041,
      "step": 2580
    },
    {
      "epoch": 0.13516510757107258,
      "grad_norm": 4.1245503425598145,
      "learning_rate": 4.774802985230416e-05,
      "loss": 1.1727,
      "step": 2590
    },
    {
      "epoch": 0.1356869805732775,
      "grad_norm": 4.506617546081543,
      "learning_rate": 4.773933162848146e-05,
      "loss": 1.2264,
      "step": 2600
    },
    {
      "epoch": 0.1362088535754824,
      "grad_norm": 4.214341640472412,
      "learning_rate": 4.773063340465877e-05,
      "loss": 1.0952,
      "step": 2610
    },
    {
      "epoch": 0.1367307265776873,
      "grad_norm": 4.396927356719971,
      "learning_rate": 4.7721935180836076e-05,
      "loss": 1.2818,
      "step": 2620
    },
    {
      "epoch": 0.13725259957989222,
      "grad_norm": 4.1905035972595215,
      "learning_rate": 4.771323695701338e-05,
      "loss": 1.1107,
      "step": 2630
    },
    {
      "epoch": 0.13777447258209716,
      "grad_norm": 4.215616226196289,
      "learning_rate": 4.7704538733190683e-05,
      "loss": 0.9841,
      "step": 2640
    },
    {
      "epoch": 0.13829634558430207,
      "grad_norm": 4.9157304763793945,
      "learning_rate": 4.769584050936799e-05,
      "loss": 1.0253,
      "step": 2650
    },
    {
      "epoch": 0.13881821858650698,
      "grad_norm": 4.008358955383301,
      "learning_rate": 4.76871422855453e-05,
      "loss": 1.2349,
      "step": 2660
    },
    {
      "epoch": 0.13934009158871188,
      "grad_norm": 4.87917423248291,
      "learning_rate": 4.76784440617226e-05,
      "loss": 1.1788,
      "step": 2670
    },
    {
      "epoch": 0.1398619645909168,
      "grad_norm": 4.75648307800293,
      "learning_rate": 4.7669745837899904e-05,
      "loss": 1.1589,
      "step": 2680
    },
    {
      "epoch": 0.1403838375931217,
      "grad_norm": 5.844627380371094,
      "learning_rate": 4.766104761407721e-05,
      "loss": 1.1502,
      "step": 2690
    },
    {
      "epoch": 0.14090571059532664,
      "grad_norm": 4.728951454162598,
      "learning_rate": 4.765234939025451e-05,
      "loss": 1.1457,
      "step": 2700
    },
    {
      "epoch": 0.14142758359753155,
      "grad_norm": 3.510605573654175,
      "learning_rate": 4.764365116643182e-05,
      "loss": 1.0342,
      "step": 2710
    },
    {
      "epoch": 0.14194945659973646,
      "grad_norm": 3.786940097808838,
      "learning_rate": 4.7634952942609125e-05,
      "loss": 1.1396,
      "step": 2720
    },
    {
      "epoch": 0.14247132960194137,
      "grad_norm": 4.975719451904297,
      "learning_rate": 4.762625471878643e-05,
      "loss": 1.1356,
      "step": 2730
    },
    {
      "epoch": 0.14299320260414627,
      "grad_norm": 4.588001251220703,
      "learning_rate": 4.761755649496373e-05,
      "loss": 1.1628,
      "step": 2740
    },
    {
      "epoch": 0.14351507560635118,
      "grad_norm": 4.256974220275879,
      "learning_rate": 4.7608858271141036e-05,
      "loss": 1.2266,
      "step": 2750
    },
    {
      "epoch": 0.14403694860855612,
      "grad_norm": 3.9952282905578613,
      "learning_rate": 4.760016004731834e-05,
      "loss": 1.1204,
      "step": 2760
    },
    {
      "epoch": 0.14455882161076103,
      "grad_norm": 4.456197261810303,
      "learning_rate": 4.759146182349564e-05,
      "loss": 1.2084,
      "step": 2770
    },
    {
      "epoch": 0.14508069461296594,
      "grad_norm": 3.725144624710083,
      "learning_rate": 4.758276359967295e-05,
      "loss": 1.1336,
      "step": 2780
    },
    {
      "epoch": 0.14560256761517085,
      "grad_norm": 5.440924644470215,
      "learning_rate": 4.757406537585025e-05,
      "loss": 1.2781,
      "step": 2790
    },
    {
      "epoch": 0.14612444061737576,
      "grad_norm": 6.493587970733643,
      "learning_rate": 4.7565367152027554e-05,
      "loss": 1.0352,
      "step": 2800
    },
    {
      "epoch": 0.14664631361958066,
      "grad_norm": 3.973708391189575,
      "learning_rate": 4.7556668928204864e-05,
      "loss": 1.1616,
      "step": 2810
    },
    {
      "epoch": 0.1471681866217856,
      "grad_norm": 4.553053379058838,
      "learning_rate": 4.754797070438217e-05,
      "loss": 1.2139,
      "step": 2820
    },
    {
      "epoch": 0.1476900596239905,
      "grad_norm": 3.588089942932129,
      "learning_rate": 4.753927248055947e-05,
      "loss": 1.168,
      "step": 2830
    },
    {
      "epoch": 0.14821193262619542,
      "grad_norm": 5.543886184692383,
      "learning_rate": 4.7530574256736775e-05,
      "loss": 1.257,
      "step": 2840
    },
    {
      "epoch": 0.14873380562840033,
      "grad_norm": 4.862786293029785,
      "learning_rate": 4.752187603291408e-05,
      "loss": 1.0746,
      "step": 2850
    },
    {
      "epoch": 0.14925567863060524,
      "grad_norm": 3.1871819496154785,
      "learning_rate": 4.751317780909139e-05,
      "loss": 1.0974,
      "step": 2860
    },
    {
      "epoch": 0.14977755163281015,
      "grad_norm": 3.912877082824707,
      "learning_rate": 4.750447958526869e-05,
      "loss": 1.1768,
      "step": 2870
    },
    {
      "epoch": 0.15029942463501506,
      "grad_norm": 4.881798267364502,
      "learning_rate": 4.7495781361445996e-05,
      "loss": 1.2452,
      "step": 2880
    },
    {
      "epoch": 0.15082129763722,
      "grad_norm": 5.00463342666626,
      "learning_rate": 4.74870831376233e-05,
      "loss": 1.1936,
      "step": 2890
    },
    {
      "epoch": 0.1513431706394249,
      "grad_norm": 4.105876922607422,
      "learning_rate": 4.74783849138006e-05,
      "loss": 1.119,
      "step": 2900
    },
    {
      "epoch": 0.1518650436416298,
      "grad_norm": 4.8051347732543945,
      "learning_rate": 4.746968668997791e-05,
      "loss": 1.1535,
      "step": 2910
    },
    {
      "epoch": 0.15238691664383472,
      "grad_norm": 4.909847259521484,
      "learning_rate": 4.746098846615522e-05,
      "loss": 1.0553,
      "step": 2920
    },
    {
      "epoch": 0.15290878964603963,
      "grad_norm": 3.8266549110412598,
      "learning_rate": 4.745229024233252e-05,
      "loss": 1.1272,
      "step": 2930
    },
    {
      "epoch": 0.15343066264824454,
      "grad_norm": 4.843506813049316,
      "learning_rate": 4.7443592018509824e-05,
      "loss": 1.2317,
      "step": 2940
    },
    {
      "epoch": 0.15395253565044947,
      "grad_norm": 5.63360595703125,
      "learning_rate": 4.743489379468713e-05,
      "loss": 1.1818,
      "step": 2950
    },
    {
      "epoch": 0.15447440865265438,
      "grad_norm": 4.708961486816406,
      "learning_rate": 4.742619557086443e-05,
      "loss": 1.2808,
      "step": 2960
    },
    {
      "epoch": 0.1549962816548593,
      "grad_norm": 5.5624613761901855,
      "learning_rate": 4.7417497347041734e-05,
      "loss": 1.1301,
      "step": 2970
    },
    {
      "epoch": 0.1555181546570642,
      "grad_norm": 4.119113445281982,
      "learning_rate": 4.740879912321904e-05,
      "loss": 1.2418,
      "step": 2980
    },
    {
      "epoch": 0.1560400276592691,
      "grad_norm": 5.127573013305664,
      "learning_rate": 4.740010089939634e-05,
      "loss": 1.1134,
      "step": 2990
    },
    {
      "epoch": 0.15656190066147402,
      "grad_norm": 3.519900321960449,
      "learning_rate": 4.7391402675573645e-05,
      "loss": 1.2031,
      "step": 3000
    },
    {
      "epoch": 0.15708377366367896,
      "grad_norm": 4.421950817108154,
      "learning_rate": 4.7382704451750955e-05,
      "loss": 1.1768,
      "step": 3010
    },
    {
      "epoch": 0.15760564666588386,
      "grad_norm": 4.879246711730957,
      "learning_rate": 4.737400622792826e-05,
      "loss": 1.0751,
      "step": 3020
    },
    {
      "epoch": 0.15812751966808877,
      "grad_norm": 5.3292412757873535,
      "learning_rate": 4.736530800410556e-05,
      "loss": 1.0633,
      "step": 3030
    },
    {
      "epoch": 0.15864939267029368,
      "grad_norm": 5.707545280456543,
      "learning_rate": 4.7356609780282866e-05,
      "loss": 1.2024,
      "step": 3040
    },
    {
      "epoch": 0.1591712656724986,
      "grad_norm": 4.286074161529541,
      "learning_rate": 4.734791155646017e-05,
      "loss": 1.1361,
      "step": 3050
    },
    {
      "epoch": 0.1596931386747035,
      "grad_norm": 4.932264804840088,
      "learning_rate": 4.733921333263748e-05,
      "loss": 1.1397,
      "step": 3060
    },
    {
      "epoch": 0.16021501167690844,
      "grad_norm": 4.742906093597412,
      "learning_rate": 4.7330515108814783e-05,
      "loss": 1.0723,
      "step": 3070
    },
    {
      "epoch": 0.16073688467911335,
      "grad_norm": 5.5531158447265625,
      "learning_rate": 4.732181688499209e-05,
      "loss": 1.1381,
      "step": 3080
    },
    {
      "epoch": 0.16125875768131825,
      "grad_norm": 5.218398571014404,
      "learning_rate": 4.731311866116939e-05,
      "loss": 1.1069,
      "step": 3090
    },
    {
      "epoch": 0.16178063068352316,
      "grad_norm": 6.07314395904541,
      "learning_rate": 4.7304420437346694e-05,
      "loss": 1.1935,
      "step": 3100
    },
    {
      "epoch": 0.16230250368572807,
      "grad_norm": 3.555516242980957,
      "learning_rate": 4.7295722213524004e-05,
      "loss": 1.1345,
      "step": 3110
    },
    {
      "epoch": 0.16282437668793298,
      "grad_norm": 5.146412372589111,
      "learning_rate": 4.728702398970131e-05,
      "loss": 1.0871,
      "step": 3120
    },
    {
      "epoch": 0.16334624969013792,
      "grad_norm": 4.4539875984191895,
      "learning_rate": 4.727832576587861e-05,
      "loss": 1.1729,
      "step": 3130
    },
    {
      "epoch": 0.16386812269234283,
      "grad_norm": 5.7325825691223145,
      "learning_rate": 4.7269627542055915e-05,
      "loss": 1.1447,
      "step": 3140
    },
    {
      "epoch": 0.16438999569454774,
      "grad_norm": 5.411395072937012,
      "learning_rate": 4.726092931823322e-05,
      "loss": 1.3402,
      "step": 3150
    },
    {
      "epoch": 0.16491186869675264,
      "grad_norm": 4.758984565734863,
      "learning_rate": 4.725223109441053e-05,
      "loss": 1.1053,
      "step": 3160
    },
    {
      "epoch": 0.16543374169895755,
      "grad_norm": 5.153898239135742,
      "learning_rate": 4.7243532870587826e-05,
      "loss": 1.1154,
      "step": 3170
    },
    {
      "epoch": 0.16595561470116246,
      "grad_norm": 5.063979625701904,
      "learning_rate": 4.723483464676513e-05,
      "loss": 1.1282,
      "step": 3180
    },
    {
      "epoch": 0.16647748770336737,
      "grad_norm": 4.717949867248535,
      "learning_rate": 4.722613642294243e-05,
      "loss": 1.2033,
      "step": 3190
    },
    {
      "epoch": 0.1669993607055723,
      "grad_norm": 4.196742057800293,
      "learning_rate": 4.721743819911974e-05,
      "loss": 1.0929,
      "step": 3200
    },
    {
      "epoch": 0.16752123370777722,
      "grad_norm": 4.960765838623047,
      "learning_rate": 4.720873997529705e-05,
      "loss": 1.1205,
      "step": 3210
    },
    {
      "epoch": 0.16804310670998213,
      "grad_norm": 5.071562767028809,
      "learning_rate": 4.720004175147435e-05,
      "loss": 1.1838,
      "step": 3220
    },
    {
      "epoch": 0.16856497971218704,
      "grad_norm": 4.666912078857422,
      "learning_rate": 4.7191343527651654e-05,
      "loss": 1.161,
      "step": 3230
    },
    {
      "epoch": 0.16908685271439194,
      "grad_norm": 4.707261562347412,
      "learning_rate": 4.718264530382896e-05,
      "loss": 1.1365,
      "step": 3240
    },
    {
      "epoch": 0.16960872571659685,
      "grad_norm": 3.617157459259033,
      "learning_rate": 4.717394708000627e-05,
      "loss": 1.0399,
      "step": 3250
    },
    {
      "epoch": 0.1701305987188018,
      "grad_norm": 3.880596876144409,
      "learning_rate": 4.716524885618357e-05,
      "loss": 1.1087,
      "step": 3260
    },
    {
      "epoch": 0.1706524717210067,
      "grad_norm": 4.305954456329346,
      "learning_rate": 4.7156550632360875e-05,
      "loss": 1.1181,
      "step": 3270
    },
    {
      "epoch": 0.1711743447232116,
      "grad_norm": 4.363276481628418,
      "learning_rate": 4.714785240853818e-05,
      "loss": 1.2124,
      "step": 3280
    },
    {
      "epoch": 0.17169621772541652,
      "grad_norm": 4.592561721801758,
      "learning_rate": 4.713915418471548e-05,
      "loss": 1.1475,
      "step": 3290
    },
    {
      "epoch": 0.17221809072762143,
      "grad_norm": 4.38346529006958,
      "learning_rate": 4.7130455960892785e-05,
      "loss": 1.1577,
      "step": 3300
    },
    {
      "epoch": 0.17273996372982633,
      "grad_norm": 5.092045783996582,
      "learning_rate": 4.7121757737070096e-05,
      "loss": 1.1668,
      "step": 3310
    },
    {
      "epoch": 0.17326183673203127,
      "grad_norm": 5.037589073181152,
      "learning_rate": 4.71130595132474e-05,
      "loss": 1.1506,
      "step": 3320
    },
    {
      "epoch": 0.17378370973423618,
      "grad_norm": 5.385667324066162,
      "learning_rate": 4.71043612894247e-05,
      "loss": 1.103,
      "step": 3330
    },
    {
      "epoch": 0.1743055827364411,
      "grad_norm": 3.9603731632232666,
      "learning_rate": 4.7095663065602006e-05,
      "loss": 1.2136,
      "step": 3340
    },
    {
      "epoch": 0.174827455738646,
      "grad_norm": 4.473390579223633,
      "learning_rate": 4.708696484177931e-05,
      "loss": 1.1631,
      "step": 3350
    },
    {
      "epoch": 0.1753493287408509,
      "grad_norm": 4.24086856842041,
      "learning_rate": 4.707826661795662e-05,
      "loss": 1.1519,
      "step": 3360
    },
    {
      "epoch": 0.17587120174305582,
      "grad_norm": 5.05208158493042,
      "learning_rate": 4.7069568394133924e-05,
      "loss": 1.1733,
      "step": 3370
    },
    {
      "epoch": 0.17639307474526075,
      "grad_norm": 4.563990592956543,
      "learning_rate": 4.706087017031122e-05,
      "loss": 1.111,
      "step": 3380
    },
    {
      "epoch": 0.17691494774746566,
      "grad_norm": 4.772731781005859,
      "learning_rate": 4.7052171946488524e-05,
      "loss": 1.0792,
      "step": 3390
    },
    {
      "epoch": 0.17743682074967057,
      "grad_norm": 4.251781940460205,
      "learning_rate": 4.7043473722665834e-05,
      "loss": 1.1354,
      "step": 3400
    },
    {
      "epoch": 0.17795869375187548,
      "grad_norm": 4.878457546234131,
      "learning_rate": 4.703477549884314e-05,
      "loss": 1.1922,
      "step": 3410
    },
    {
      "epoch": 0.1784805667540804,
      "grad_norm": 3.908445119857788,
      "learning_rate": 4.702607727502044e-05,
      "loss": 1.1088,
      "step": 3420
    },
    {
      "epoch": 0.1790024397562853,
      "grad_norm": 4.272200584411621,
      "learning_rate": 4.7017379051197745e-05,
      "loss": 1.2622,
      "step": 3430
    },
    {
      "epoch": 0.17952431275849023,
      "grad_norm": 4.249223232269287,
      "learning_rate": 4.700868082737505e-05,
      "loss": 1.1257,
      "step": 3440
    },
    {
      "epoch": 0.18004618576069514,
      "grad_norm": 4.618061542510986,
      "learning_rate": 4.699998260355236e-05,
      "loss": 1.1328,
      "step": 3450
    },
    {
      "epoch": 0.18056805876290005,
      "grad_norm": 4.794341564178467,
      "learning_rate": 4.699128437972966e-05,
      "loss": 1.1321,
      "step": 3460
    },
    {
      "epoch": 0.18108993176510496,
      "grad_norm": 3.956947088241577,
      "learning_rate": 4.6982586155906966e-05,
      "loss": 1.0776,
      "step": 3470
    },
    {
      "epoch": 0.18161180476730987,
      "grad_norm": 4.14027738571167,
      "learning_rate": 4.697388793208427e-05,
      "loss": 1.1892,
      "step": 3480
    },
    {
      "epoch": 0.18213367776951478,
      "grad_norm": 5.167445659637451,
      "learning_rate": 4.696518970826157e-05,
      "loss": 1.1678,
      "step": 3490
    },
    {
      "epoch": 0.18265555077171972,
      "grad_norm": 4.416545391082764,
      "learning_rate": 4.6956491484438883e-05,
      "loss": 1.231,
      "step": 3500
    },
    {
      "epoch": 0.18317742377392462,
      "grad_norm": 5.148200988769531,
      "learning_rate": 4.694779326061619e-05,
      "loss": 1.1995,
      "step": 3510
    },
    {
      "epoch": 0.18369929677612953,
      "grad_norm": 3.7194266319274902,
      "learning_rate": 4.693909503679349e-05,
      "loss": 1.0445,
      "step": 3520
    },
    {
      "epoch": 0.18422116977833444,
      "grad_norm": 4.506605625152588,
      "learning_rate": 4.6930396812970794e-05,
      "loss": 1.163,
      "step": 3530
    },
    {
      "epoch": 0.18474304278053935,
      "grad_norm": 3.6773948669433594,
      "learning_rate": 4.69216985891481e-05,
      "loss": 1.1031,
      "step": 3540
    },
    {
      "epoch": 0.18526491578274426,
      "grad_norm": 4.506981372833252,
      "learning_rate": 4.691300036532541e-05,
      "loss": 1.0843,
      "step": 3550
    },
    {
      "epoch": 0.18578678878494917,
      "grad_norm": 3.8276145458221436,
      "learning_rate": 4.690430214150271e-05,
      "loss": 1.1995,
      "step": 3560
    },
    {
      "epoch": 0.1863086617871541,
      "grad_norm": 4.834096908569336,
      "learning_rate": 4.6895603917680015e-05,
      "loss": 1.1463,
      "step": 3570
    },
    {
      "epoch": 0.18683053478935902,
      "grad_norm": 4.603427410125732,
      "learning_rate": 4.688690569385732e-05,
      "loss": 1.1369,
      "step": 3580
    },
    {
      "epoch": 0.18735240779156392,
      "grad_norm": 4.460635185241699,
      "learning_rate": 4.6878207470034615e-05,
      "loss": 1.0613,
      "step": 3590
    },
    {
      "epoch": 0.18787428079376883,
      "grad_norm": 4.338809013366699,
      "learning_rate": 4.6869509246211926e-05,
      "loss": 1.1998,
      "step": 3600
    },
    {
      "epoch": 0.18839615379597374,
      "grad_norm": 5.342703819274902,
      "learning_rate": 4.686081102238923e-05,
      "loss": 1.1351,
      "step": 3610
    },
    {
      "epoch": 0.18891802679817865,
      "grad_norm": 5.848031520843506,
      "learning_rate": 4.685211279856653e-05,
      "loss": 1.0664,
      "step": 3620
    },
    {
      "epoch": 0.1894398998003836,
      "grad_norm": 5.649608135223389,
      "learning_rate": 4.6843414574743836e-05,
      "loss": 1.2987,
      "step": 3630
    },
    {
      "epoch": 0.1899617728025885,
      "grad_norm": 5.472653865814209,
      "learning_rate": 4.683471635092114e-05,
      "loss": 1.2142,
      "step": 3640
    },
    {
      "epoch": 0.1904836458047934,
      "grad_norm": 4.545722961425781,
      "learning_rate": 4.682601812709845e-05,
      "loss": 1.2065,
      "step": 3650
    },
    {
      "epoch": 0.19100551880699831,
      "grad_norm": 3.450028419494629,
      "learning_rate": 4.6817319903275754e-05,
      "loss": 1.0817,
      "step": 3660
    },
    {
      "epoch": 0.19152739180920322,
      "grad_norm": 5.0458221435546875,
      "learning_rate": 4.680862167945306e-05,
      "loss": 1.1611,
      "step": 3670
    },
    {
      "epoch": 0.19204926481140813,
      "grad_norm": 5.092627048492432,
      "learning_rate": 4.679992345563036e-05,
      "loss": 1.0258,
      "step": 3680
    },
    {
      "epoch": 0.19257113781361307,
      "grad_norm": 4.81989049911499,
      "learning_rate": 4.6791225231807664e-05,
      "loss": 1.2001,
      "step": 3690
    },
    {
      "epoch": 0.19309301081581798,
      "grad_norm": 4.388401031494141,
      "learning_rate": 4.6782527007984975e-05,
      "loss": 1.1001,
      "step": 3700
    },
    {
      "epoch": 0.1936148838180229,
      "grad_norm": 5.14132833480835,
      "learning_rate": 4.677382878416228e-05,
      "loss": 1.1163,
      "step": 3710
    },
    {
      "epoch": 0.1941367568202278,
      "grad_norm": 5.535025119781494,
      "learning_rate": 4.676513056033958e-05,
      "loss": 1.3127,
      "step": 3720
    },
    {
      "epoch": 0.1946586298224327,
      "grad_norm": 4.232290267944336,
      "learning_rate": 4.6756432336516885e-05,
      "loss": 1.2382,
      "step": 3730
    },
    {
      "epoch": 0.19518050282463761,
      "grad_norm": 5.2539262771606445,
      "learning_rate": 4.674773411269419e-05,
      "loss": 1.1947,
      "step": 3740
    },
    {
      "epoch": 0.19570237582684255,
      "grad_norm": 4.924971580505371,
      "learning_rate": 4.67390358888715e-05,
      "loss": 1.0955,
      "step": 3750
    },
    {
      "epoch": 0.19622424882904746,
      "grad_norm": 6.317643642425537,
      "learning_rate": 4.67303376650488e-05,
      "loss": 1.1549,
      "step": 3760
    },
    {
      "epoch": 0.19674612183125237,
      "grad_norm": 4.9614458084106445,
      "learning_rate": 4.6721639441226106e-05,
      "loss": 1.0358,
      "step": 3770
    },
    {
      "epoch": 0.19726799483345728,
      "grad_norm": 4.132730007171631,
      "learning_rate": 4.671294121740341e-05,
      "loss": 1.1459,
      "step": 3780
    },
    {
      "epoch": 0.1977898678356622,
      "grad_norm": 4.374362468719482,
      "learning_rate": 4.6704242993580714e-05,
      "loss": 0.9882,
      "step": 3790
    },
    {
      "epoch": 0.1983117408378671,
      "grad_norm": 4.218964099884033,
      "learning_rate": 4.669554476975802e-05,
      "loss": 1.1923,
      "step": 3800
    },
    {
      "epoch": 0.19883361384007203,
      "grad_norm": 3.8618295192718506,
      "learning_rate": 4.668684654593532e-05,
      "loss": 1.1755,
      "step": 3810
    },
    {
      "epoch": 0.19935548684227694,
      "grad_norm": 3.8202972412109375,
      "learning_rate": 4.6678148322112624e-05,
      "loss": 1.1486,
      "step": 3820
    },
    {
      "epoch": 0.19987735984448185,
      "grad_norm": 5.083072662353516,
      "learning_rate": 4.666945009828993e-05,
      "loss": 1.1877,
      "step": 3830
    },
    {
      "epoch": 0.20039923284668676,
      "grad_norm": 3.948819637298584,
      "learning_rate": 4.666075187446723e-05,
      "loss": 1.1007,
      "step": 3840
    },
    {
      "epoch": 0.20092110584889167,
      "grad_norm": 5.193563938140869,
      "learning_rate": 4.665205365064454e-05,
      "loss": 1.184,
      "step": 3850
    },
    {
      "epoch": 0.20144297885109658,
      "grad_norm": 4.135252475738525,
      "learning_rate": 4.6643355426821845e-05,
      "loss": 1.1027,
      "step": 3860
    },
    {
      "epoch": 0.20196485185330149,
      "grad_norm": 4.951305866241455,
      "learning_rate": 4.663465720299915e-05,
      "loss": 1.0584,
      "step": 3870
    },
    {
      "epoch": 0.20248672485550642,
      "grad_norm": 4.464635848999023,
      "learning_rate": 4.662595897917645e-05,
      "loss": 1.1201,
      "step": 3880
    },
    {
      "epoch": 0.20300859785771133,
      "grad_norm": 3.803647041320801,
      "learning_rate": 4.6617260755353756e-05,
      "loss": 1.1631,
      "step": 3890
    },
    {
      "epoch": 0.20353047085991624,
      "grad_norm": 5.150356769561768,
      "learning_rate": 4.6608562531531066e-05,
      "loss": 1.0762,
      "step": 3900
    },
    {
      "epoch": 0.20405234386212115,
      "grad_norm": 3.705852508544922,
      "learning_rate": 4.659986430770837e-05,
      "loss": 1.0807,
      "step": 3910
    },
    {
      "epoch": 0.20457421686432606,
      "grad_norm": 5.0204267501831055,
      "learning_rate": 4.659116608388567e-05,
      "loss": 1.0842,
      "step": 3920
    },
    {
      "epoch": 0.20509608986653097,
      "grad_norm": 6.162471294403076,
      "learning_rate": 4.658246786006298e-05,
      "loss": 1.1323,
      "step": 3930
    },
    {
      "epoch": 0.2056179628687359,
      "grad_norm": 4.366540908813477,
      "learning_rate": 4.657376963624028e-05,
      "loss": 1.0545,
      "step": 3940
    },
    {
      "epoch": 0.2061398358709408,
      "grad_norm": 5.273899555206299,
      "learning_rate": 4.656507141241759e-05,
      "loss": 1.2502,
      "step": 3950
    },
    {
      "epoch": 0.20666170887314572,
      "grad_norm": 5.3204755783081055,
      "learning_rate": 4.6556373188594894e-05,
      "loss": 1.0353,
      "step": 3960
    },
    {
      "epoch": 0.20718358187535063,
      "grad_norm": 4.088191509246826,
      "learning_rate": 4.65476749647722e-05,
      "loss": 1.057,
      "step": 3970
    },
    {
      "epoch": 0.20770545487755554,
      "grad_norm": 5.485529899597168,
      "learning_rate": 4.65389767409495e-05,
      "loss": 1.0747,
      "step": 3980
    },
    {
      "epoch": 0.20822732787976045,
      "grad_norm": 5.522958755493164,
      "learning_rate": 4.6530278517126805e-05,
      "loss": 1.0504,
      "step": 3990
    },
    {
      "epoch": 0.20874920088196539,
      "grad_norm": 4.947297096252441,
      "learning_rate": 4.6521580293304115e-05,
      "loss": 1.1245,
      "step": 4000
    },
    {
      "epoch": 0.2092710738841703,
      "grad_norm": 5.680069446563721,
      "learning_rate": 4.651288206948141e-05,
      "loss": 1.0685,
      "step": 4010
    },
    {
      "epoch": 0.2097929468863752,
      "grad_norm": 3.702791213989258,
      "learning_rate": 4.6504183845658715e-05,
      "loss": 1.0906,
      "step": 4020
    },
    {
      "epoch": 0.2103148198885801,
      "grad_norm": 4.575181007385254,
      "learning_rate": 4.649548562183602e-05,
      "loss": 1.1269,
      "step": 4030
    },
    {
      "epoch": 0.21083669289078502,
      "grad_norm": 4.648443698883057,
      "learning_rate": 4.648678739801333e-05,
      "loss": 1.0725,
      "step": 4040
    },
    {
      "epoch": 0.21135856589298993,
      "grad_norm": 5.077942848205566,
      "learning_rate": 4.647808917419063e-05,
      "loss": 1.1753,
      "step": 4050
    },
    {
      "epoch": 0.21188043889519487,
      "grad_norm": 4.196751117706299,
      "learning_rate": 4.6469390950367936e-05,
      "loss": 1.151,
      "step": 4060
    },
    {
      "epoch": 0.21240231189739978,
      "grad_norm": 4.53282356262207,
      "learning_rate": 4.646069272654524e-05,
      "loss": 1.1321,
      "step": 4070
    },
    {
      "epoch": 0.21292418489960468,
      "grad_norm": 4.526268005371094,
      "learning_rate": 4.6451994502722544e-05,
      "loss": 1.171,
      "step": 4080
    },
    {
      "epoch": 0.2134460579018096,
      "grad_norm": 4.794793605804443,
      "learning_rate": 4.644329627889985e-05,
      "loss": 1.0979,
      "step": 4090
    },
    {
      "epoch": 0.2139679309040145,
      "grad_norm": 4.938503742218018,
      "learning_rate": 4.643459805507716e-05,
      "loss": 1.1215,
      "step": 4100
    },
    {
      "epoch": 0.2144898039062194,
      "grad_norm": 4.415352821350098,
      "learning_rate": 4.642589983125446e-05,
      "loss": 1.1759,
      "step": 4110
    },
    {
      "epoch": 0.21501167690842435,
      "grad_norm": 4.196761608123779,
      "learning_rate": 4.6417201607431765e-05,
      "loss": 0.9322,
      "step": 4120
    },
    {
      "epoch": 0.21553354991062926,
      "grad_norm": 4.774365425109863,
      "learning_rate": 4.640850338360907e-05,
      "loss": 1.2541,
      "step": 4130
    },
    {
      "epoch": 0.21605542291283417,
      "grad_norm": 5.025698661804199,
      "learning_rate": 4.639980515978637e-05,
      "loss": 1.1255,
      "step": 4140
    },
    {
      "epoch": 0.21657729591503908,
      "grad_norm": 4.082160949707031,
      "learning_rate": 4.639110693596368e-05,
      "loss": 0.935,
      "step": 4150
    },
    {
      "epoch": 0.21709916891724398,
      "grad_norm": 5.473910808563232,
      "learning_rate": 4.6382408712140985e-05,
      "loss": 1.0811,
      "step": 4160
    },
    {
      "epoch": 0.2176210419194489,
      "grad_norm": 3.727585554122925,
      "learning_rate": 4.637371048831829e-05,
      "loss": 1.0641,
      "step": 4170
    },
    {
      "epoch": 0.2181429149216538,
      "grad_norm": 5.708925724029541,
      "learning_rate": 4.636501226449559e-05,
      "loss": 1.113,
      "step": 4180
    },
    {
      "epoch": 0.21866478792385874,
      "grad_norm": 4.494616508483887,
      "learning_rate": 4.6356314040672896e-05,
      "loss": 1.1062,
      "step": 4190
    },
    {
      "epoch": 0.21918666092606365,
      "grad_norm": 4.15109395980835,
      "learning_rate": 4.6347615816850206e-05,
      "loss": 1.1514,
      "step": 4200
    },
    {
      "epoch": 0.21970853392826856,
      "grad_norm": 5.162577152252197,
      "learning_rate": 4.63389175930275e-05,
      "loss": 1.1133,
      "step": 4210
    },
    {
      "epoch": 0.22023040693047347,
      "grad_norm": 3.8947856426239014,
      "learning_rate": 4.633021936920481e-05,
      "loss": 1.0849,
      "step": 4220
    },
    {
      "epoch": 0.22075227993267837,
      "grad_norm": 4.975423812866211,
      "learning_rate": 4.632152114538211e-05,
      "loss": 1.0784,
      "step": 4230
    },
    {
      "epoch": 0.22127415293488328,
      "grad_norm": 4.537469863891602,
      "learning_rate": 4.631282292155942e-05,
      "loss": 1.0763,
      "step": 4240
    },
    {
      "epoch": 0.22179602593708822,
      "grad_norm": 4.4495744705200195,
      "learning_rate": 4.6304124697736724e-05,
      "loss": 1.2207,
      "step": 4250
    },
    {
      "epoch": 0.22231789893929313,
      "grad_norm": 5.623624324798584,
      "learning_rate": 4.629542647391403e-05,
      "loss": 1.0986,
      "step": 4260
    },
    {
      "epoch": 0.22283977194149804,
      "grad_norm": 3.9233996868133545,
      "learning_rate": 4.628672825009133e-05,
      "loss": 1.186,
      "step": 4270
    },
    {
      "epoch": 0.22336164494370295,
      "grad_norm": 4.412266254425049,
      "learning_rate": 4.6278030026268635e-05,
      "loss": 1.0786,
      "step": 4280
    },
    {
      "epoch": 0.22388351794590786,
      "grad_norm": 4.503261089324951,
      "learning_rate": 4.6269331802445945e-05,
      "loss": 1.007,
      "step": 4290
    },
    {
      "epoch": 0.22440539094811277,
      "grad_norm": 4.629826068878174,
      "learning_rate": 4.626063357862325e-05,
      "loss": 1.0905,
      "step": 4300
    },
    {
      "epoch": 0.2249272639503177,
      "grad_norm": 4.717471122741699,
      "learning_rate": 4.625193535480055e-05,
      "loss": 1.1207,
      "step": 4310
    },
    {
      "epoch": 0.2254491369525226,
      "grad_norm": 5.082294464111328,
      "learning_rate": 4.6243237130977856e-05,
      "loss": 1.1676,
      "step": 4320
    },
    {
      "epoch": 0.22597100995472752,
      "grad_norm": 4.701741695404053,
      "learning_rate": 4.623453890715516e-05,
      "loss": 1.226,
      "step": 4330
    },
    {
      "epoch": 0.22649288295693243,
      "grad_norm": 4.71090030670166,
      "learning_rate": 4.622584068333247e-05,
      "loss": 1.1113,
      "step": 4340
    },
    {
      "epoch": 0.22701475595913734,
      "grad_norm": 4.577610015869141,
      "learning_rate": 4.621714245950977e-05,
      "loss": 1.1599,
      "step": 4350
    },
    {
      "epoch": 0.22753662896134225,
      "grad_norm": 3.5279555320739746,
      "learning_rate": 4.620844423568708e-05,
      "loss": 1.0825,
      "step": 4360
    },
    {
      "epoch": 0.22805850196354718,
      "grad_norm": 4.7844648361206055,
      "learning_rate": 4.619974601186438e-05,
      "loss": 0.9976,
      "step": 4370
    },
    {
      "epoch": 0.2285803749657521,
      "grad_norm": 5.4630303382873535,
      "learning_rate": 4.6191047788041684e-05,
      "loss": 1.1481,
      "step": 4380
    },
    {
      "epoch": 0.229102247967957,
      "grad_norm": 4.2388129234313965,
      "learning_rate": 4.618234956421899e-05,
      "loss": 1.0445,
      "step": 4390
    },
    {
      "epoch": 0.2296241209701619,
      "grad_norm": 4.893985748291016,
      "learning_rate": 4.61736513403963e-05,
      "loss": 1.1877,
      "step": 4400
    },
    {
      "epoch": 0.23014599397236682,
      "grad_norm": 4.413876056671143,
      "learning_rate": 4.61649531165736e-05,
      "loss": 1.264,
      "step": 4410
    },
    {
      "epoch": 0.23066786697457173,
      "grad_norm": 3.8731822967529297,
      "learning_rate": 4.61562548927509e-05,
      "loss": 1.0992,
      "step": 4420
    },
    {
      "epoch": 0.23118973997677666,
      "grad_norm": 4.495370388031006,
      "learning_rate": 4.61475566689282e-05,
      "loss": 1.1052,
      "step": 4430
    },
    {
      "epoch": 0.23171161297898157,
      "grad_norm": 3.9866034984588623,
      "learning_rate": 4.613885844510551e-05,
      "loss": 1.1771,
      "step": 4440
    },
    {
      "epoch": 0.23223348598118648,
      "grad_norm": 4.280478477478027,
      "learning_rate": 4.6130160221282815e-05,
      "loss": 1.1254,
      "step": 4450
    },
    {
      "epoch": 0.2327553589833914,
      "grad_norm": 4.306457042694092,
      "learning_rate": 4.612146199746012e-05,
      "loss": 1.1121,
      "step": 4460
    },
    {
      "epoch": 0.2332772319855963,
      "grad_norm": 3.1632235050201416,
      "learning_rate": 4.611276377363742e-05,
      "loss": 1.0354,
      "step": 4470
    },
    {
      "epoch": 0.2337991049878012,
      "grad_norm": 6.241295337677002,
      "learning_rate": 4.6104065549814726e-05,
      "loss": 1.093,
      "step": 4480
    },
    {
      "epoch": 0.23432097799000612,
      "grad_norm": 5.256368160247803,
      "learning_rate": 4.6095367325992036e-05,
      "loss": 1.1115,
      "step": 4490
    },
    {
      "epoch": 0.23484285099221106,
      "grad_norm": 4.094829559326172,
      "learning_rate": 4.608666910216934e-05,
      "loss": 1.1202,
      "step": 4500
    },
    {
      "epoch": 0.23536472399441596,
      "grad_norm": 4.936419486999512,
      "learning_rate": 4.6077970878346644e-05,
      "loss": 1.1718,
      "step": 4510
    },
    {
      "epoch": 0.23588659699662087,
      "grad_norm": 5.8722028732299805,
      "learning_rate": 4.606927265452395e-05,
      "loss": 1.1106,
      "step": 4520
    },
    {
      "epoch": 0.23640846999882578,
      "grad_norm": 4.146575927734375,
      "learning_rate": 4.606057443070125e-05,
      "loss": 1.0723,
      "step": 4530
    },
    {
      "epoch": 0.2369303430010307,
      "grad_norm": 4.605004787445068,
      "learning_rate": 4.605187620687856e-05,
      "loss": 1.1585,
      "step": 4540
    },
    {
      "epoch": 0.2374522160032356,
      "grad_norm": 4.703554630279541,
      "learning_rate": 4.6043177983055865e-05,
      "loss": 1.0482,
      "step": 4550
    },
    {
      "epoch": 0.23797408900544054,
      "grad_norm": 3.541060447692871,
      "learning_rate": 4.603447975923317e-05,
      "loss": 1.1431,
      "step": 4560
    },
    {
      "epoch": 0.23849596200764545,
      "grad_norm": 4.552922248840332,
      "learning_rate": 4.602578153541047e-05,
      "loss": 1.0947,
      "step": 4570
    },
    {
      "epoch": 0.23901783500985035,
      "grad_norm": 4.676563739776611,
      "learning_rate": 4.6017083311587775e-05,
      "loss": 1.151,
      "step": 4580
    },
    {
      "epoch": 0.23953970801205526,
      "grad_norm": 4.168123722076416,
      "learning_rate": 4.6008385087765086e-05,
      "loss": 1.0369,
      "step": 4590
    },
    {
      "epoch": 0.24006158101426017,
      "grad_norm": 4.381834983825684,
      "learning_rate": 4.599968686394239e-05,
      "loss": 1.0692,
      "step": 4600
    },
    {
      "epoch": 0.24058345401646508,
      "grad_norm": 4.087157249450684,
      "learning_rate": 4.599098864011969e-05,
      "loss": 1.2098,
      "step": 4610
    },
    {
      "epoch": 0.24110532701867002,
      "grad_norm": 6.584364891052246,
      "learning_rate": 4.5982290416296996e-05,
      "loss": 1.1906,
      "step": 4620
    },
    {
      "epoch": 0.24162720002087493,
      "grad_norm": 5.848927974700928,
      "learning_rate": 4.597359219247429e-05,
      "loss": 1.055,
      "step": 4630
    },
    {
      "epoch": 0.24214907302307984,
      "grad_norm": 4.5716447830200195,
      "learning_rate": 4.59648939686516e-05,
      "loss": 1.163,
      "step": 4640
    },
    {
      "epoch": 0.24267094602528475,
      "grad_norm": 3.5135319232940674,
      "learning_rate": 4.595619574482891e-05,
      "loss": 0.9776,
      "step": 4650
    },
    {
      "epoch": 0.24319281902748965,
      "grad_norm": 4.445549964904785,
      "learning_rate": 4.594749752100621e-05,
      "loss": 1.0175,
      "step": 4660
    },
    {
      "epoch": 0.24371469202969456,
      "grad_norm": 5.7047038078308105,
      "learning_rate": 4.5938799297183514e-05,
      "loss": 1.1628,
      "step": 4670
    },
    {
      "epoch": 0.2442365650318995,
      "grad_norm": 5.286273002624512,
      "learning_rate": 4.593010107336082e-05,
      "loss": 1.0918,
      "step": 4680
    },
    {
      "epoch": 0.2447584380341044,
      "grad_norm": 4.781688690185547,
      "learning_rate": 4.592140284953813e-05,
      "loss": 0.9677,
      "step": 4690
    },
    {
      "epoch": 0.24528031103630932,
      "grad_norm": 5.271426677703857,
      "learning_rate": 4.591270462571543e-05,
      "loss": 1.0932,
      "step": 4700
    },
    {
      "epoch": 0.24580218403851423,
      "grad_norm": 4.522574424743652,
      "learning_rate": 4.5904006401892735e-05,
      "loss": 1.0247,
      "step": 4710
    },
    {
      "epoch": 0.24632405704071914,
      "grad_norm": 4.822240352630615,
      "learning_rate": 4.589530817807004e-05,
      "loss": 1.0487,
      "step": 4720
    },
    {
      "epoch": 0.24684593004292404,
      "grad_norm": 4.546276569366455,
      "learning_rate": 4.588660995424734e-05,
      "loss": 1.1249,
      "step": 4730
    },
    {
      "epoch": 0.24736780304512898,
      "grad_norm": 4.315184593200684,
      "learning_rate": 4.587791173042465e-05,
      "loss": 1.0191,
      "step": 4740
    },
    {
      "epoch": 0.2478896760473339,
      "grad_norm": 5.513609886169434,
      "learning_rate": 4.5869213506601956e-05,
      "loss": 1.0831,
      "step": 4750
    },
    {
      "epoch": 0.2484115490495388,
      "grad_norm": 4.621212005615234,
      "learning_rate": 4.586051528277926e-05,
      "loss": 1.0785,
      "step": 4760
    },
    {
      "epoch": 0.2489334220517437,
      "grad_norm": 4.45919942855835,
      "learning_rate": 4.585181705895656e-05,
      "loss": 0.95,
      "step": 4770
    },
    {
      "epoch": 0.24945529505394862,
      "grad_norm": 4.65622091293335,
      "learning_rate": 4.5843118835133866e-05,
      "loss": 1.1383,
      "step": 4780
    },
    {
      "epoch": 0.24997716805615353,
      "grad_norm": 4.17941951751709,
      "learning_rate": 4.583442061131118e-05,
      "loss": 1.0985,
      "step": 4790
    },
    {
      "epoch": 0.25049904105835846,
      "grad_norm": 5.627443790435791,
      "learning_rate": 4.582572238748848e-05,
      "loss": 1.1144,
      "step": 4800
    },
    {
      "epoch": 0.25102091406056337,
      "grad_norm": 5.381149768829346,
      "learning_rate": 4.5817024163665784e-05,
      "loss": 1.055,
      "step": 4810
    },
    {
      "epoch": 0.2515427870627683,
      "grad_norm": 4.257904529571533,
      "learning_rate": 4.580832593984309e-05,
      "loss": 1.1491,
      "step": 4820
    },
    {
      "epoch": 0.2520646600649732,
      "grad_norm": 4.796273708343506,
      "learning_rate": 4.579962771602039e-05,
      "loss": 1.13,
      "step": 4830
    },
    {
      "epoch": 0.2525865330671781,
      "grad_norm": 4.20608377456665,
      "learning_rate": 4.5790929492197695e-05,
      "loss": 1.0092,
      "step": 4840
    },
    {
      "epoch": 0.253108406069383,
      "grad_norm": 5.730111598968506,
      "learning_rate": 4.5782231268375e-05,
      "loss": 1.2441,
      "step": 4850
    },
    {
      "epoch": 0.2536302790715879,
      "grad_norm": 4.205788612365723,
      "learning_rate": 4.57735330445523e-05,
      "loss": 1.1734,
      "step": 4860
    },
    {
      "epoch": 0.2541521520737928,
      "grad_norm": 6.077715873718262,
      "learning_rate": 4.5764834820729605e-05,
      "loss": 1.0972,
      "step": 4870
    },
    {
      "epoch": 0.25467402507599773,
      "grad_norm": 4.141361713409424,
      "learning_rate": 4.5756136596906916e-05,
      "loss": 1.0608,
      "step": 4880
    },
    {
      "epoch": 0.25519589807820264,
      "grad_norm": 4.731325149536133,
      "learning_rate": 4.574743837308422e-05,
      "loss": 1.1971,
      "step": 4890
    },
    {
      "epoch": 0.2557177710804076,
      "grad_norm": 5.393826961517334,
      "learning_rate": 4.573874014926152e-05,
      "loss": 1.041,
      "step": 4900
    },
    {
      "epoch": 0.2562396440826125,
      "grad_norm": 4.219085216522217,
      "learning_rate": 4.5730041925438826e-05,
      "loss": 1.0986,
      "step": 4910
    },
    {
      "epoch": 0.2567615170848174,
      "grad_norm": 4.175786018371582,
      "learning_rate": 4.572134370161613e-05,
      "loss": 0.9993,
      "step": 4920
    },
    {
      "epoch": 0.25728339008702233,
      "grad_norm": 4.8153276443481445,
      "learning_rate": 4.571264547779343e-05,
      "loss": 0.9755,
      "step": 4930
    },
    {
      "epoch": 0.25780526308922724,
      "grad_norm": 4.8997697830200195,
      "learning_rate": 4.5703947253970744e-05,
      "loss": 1.1175,
      "step": 4940
    },
    {
      "epoch": 0.25832713609143215,
      "grad_norm": 5.559735298156738,
      "learning_rate": 4.569524903014805e-05,
      "loss": 1.1626,
      "step": 4950
    },
    {
      "epoch": 0.25884900909363706,
      "grad_norm": 5.398789882659912,
      "learning_rate": 4.568655080632535e-05,
      "loss": 1.144,
      "step": 4960
    },
    {
      "epoch": 0.25937088209584197,
      "grad_norm": 4.809335231781006,
      "learning_rate": 4.5677852582502654e-05,
      "loss": 1.129,
      "step": 4970
    },
    {
      "epoch": 0.2598927550980469,
      "grad_norm": 3.7560229301452637,
      "learning_rate": 4.566915435867996e-05,
      "loss": 1.0076,
      "step": 4980
    },
    {
      "epoch": 0.2604146281002518,
      "grad_norm": 5.148644924163818,
      "learning_rate": 4.566045613485727e-05,
      "loss": 1.1188,
      "step": 4990
    },
    {
      "epoch": 0.2609365011024567,
      "grad_norm": 4.618556976318359,
      "learning_rate": 4.565175791103457e-05,
      "loss": 1.0232,
      "step": 5000
    },
    {
      "epoch": 0.2614583741046616,
      "grad_norm": 3.9291675090789795,
      "learning_rate": 4.5643059687211875e-05,
      "loss": 1.1155,
      "step": 5010
    },
    {
      "epoch": 0.26198024710686657,
      "grad_norm": 3.9904537200927734,
      "learning_rate": 4.563436146338918e-05,
      "loss": 1.0947,
      "step": 5020
    },
    {
      "epoch": 0.2625021201090715,
      "grad_norm": 4.280638217926025,
      "learning_rate": 4.562566323956648e-05,
      "loss": 1.0365,
      "step": 5030
    },
    {
      "epoch": 0.2630239931112764,
      "grad_norm": 4.469633102416992,
      "learning_rate": 4.561696501574379e-05,
      "loss": 1.0008,
      "step": 5040
    },
    {
      "epoch": 0.2635458661134813,
      "grad_norm": 4.121488571166992,
      "learning_rate": 4.560826679192109e-05,
      "loss": 1.0665,
      "step": 5050
    },
    {
      "epoch": 0.2640677391156862,
      "grad_norm": 4.912363529205322,
      "learning_rate": 4.559956856809839e-05,
      "loss": 1.0018,
      "step": 5060
    },
    {
      "epoch": 0.2645896121178911,
      "grad_norm": 5.2009501457214355,
      "learning_rate": 4.5590870344275697e-05,
      "loss": 1.0885,
      "step": 5070
    },
    {
      "epoch": 0.265111485120096,
      "grad_norm": 4.445443153381348,
      "learning_rate": 4.558217212045301e-05,
      "loss": 1.0469,
      "step": 5080
    },
    {
      "epoch": 0.26563335812230093,
      "grad_norm": 4.362040042877197,
      "learning_rate": 4.557347389663031e-05,
      "loss": 1.0988,
      "step": 5090
    },
    {
      "epoch": 0.26615523112450584,
      "grad_norm": 4.02547550201416,
      "learning_rate": 4.5564775672807614e-05,
      "loss": 1.1199,
      "step": 5100
    },
    {
      "epoch": 0.26667710412671075,
      "grad_norm": 4.869834899902344,
      "learning_rate": 4.555607744898492e-05,
      "loss": 1.0812,
      "step": 5110
    },
    {
      "epoch": 0.26719897712891566,
      "grad_norm": 5.107236385345459,
      "learning_rate": 4.554737922516222e-05,
      "loss": 1.1724,
      "step": 5120
    },
    {
      "epoch": 0.26772085013112057,
      "grad_norm": 4.538249969482422,
      "learning_rate": 4.553868100133953e-05,
      "loss": 1.0853,
      "step": 5130
    },
    {
      "epoch": 0.2682427231333255,
      "grad_norm": 5.282759666442871,
      "learning_rate": 4.5529982777516835e-05,
      "loss": 0.9994,
      "step": 5140
    },
    {
      "epoch": 0.26876459613553044,
      "grad_norm": 4.279083251953125,
      "learning_rate": 4.552128455369414e-05,
      "loss": 0.981,
      "step": 5150
    },
    {
      "epoch": 0.26928646913773535,
      "grad_norm": 5.044832229614258,
      "learning_rate": 4.551258632987144e-05,
      "loss": 1.0521,
      "step": 5160
    },
    {
      "epoch": 0.26980834213994026,
      "grad_norm": 4.190216541290283,
      "learning_rate": 4.5503888106048746e-05,
      "loss": 1.0113,
      "step": 5170
    },
    {
      "epoch": 0.27033021514214517,
      "grad_norm": 4.054166316986084,
      "learning_rate": 4.549518988222605e-05,
      "loss": 1.1461,
      "step": 5180
    },
    {
      "epoch": 0.2708520881443501,
      "grad_norm": 3.782968044281006,
      "learning_rate": 4.548649165840336e-05,
      "loss": 1.0103,
      "step": 5190
    },
    {
      "epoch": 0.271373961146555,
      "grad_norm": 4.02012300491333,
      "learning_rate": 4.547779343458066e-05,
      "loss": 1.0335,
      "step": 5200
    },
    {
      "epoch": 0.2718958341487599,
      "grad_norm": 4.972684383392334,
      "learning_rate": 4.5469095210757967e-05,
      "loss": 1.005,
      "step": 5210
    },
    {
      "epoch": 0.2724177071509648,
      "grad_norm": 4.794705867767334,
      "learning_rate": 4.546039698693527e-05,
      "loss": 1.1322,
      "step": 5220
    },
    {
      "epoch": 0.2729395801531697,
      "grad_norm": 4.861769199371338,
      "learning_rate": 4.5451698763112574e-05,
      "loss": 1.1285,
      "step": 5230
    },
    {
      "epoch": 0.2734614531553746,
      "grad_norm": 5.131702899932861,
      "learning_rate": 4.5443000539289884e-05,
      "loss": 1.1682,
      "step": 5240
    },
    {
      "epoch": 0.27398332615757953,
      "grad_norm": 4.765778064727783,
      "learning_rate": 4.543430231546719e-05,
      "loss": 1.1572,
      "step": 5250
    },
    {
      "epoch": 0.27450519915978444,
      "grad_norm": 4.07348108291626,
      "learning_rate": 4.5425604091644484e-05,
      "loss": 1.0009,
      "step": 5260
    },
    {
      "epoch": 0.2750270721619894,
      "grad_norm": 5.001435279846191,
      "learning_rate": 4.541690586782179e-05,
      "loss": 1.0454,
      "step": 5270
    },
    {
      "epoch": 0.2755489451641943,
      "grad_norm": 4.987857341766357,
      "learning_rate": 4.54082076439991e-05,
      "loss": 1.0167,
      "step": 5280
    },
    {
      "epoch": 0.2760708181663992,
      "grad_norm": 3.9320478439331055,
      "learning_rate": 4.53995094201764e-05,
      "loss": 1.0585,
      "step": 5290
    },
    {
      "epoch": 0.27659269116860413,
      "grad_norm": 4.495091438293457,
      "learning_rate": 4.5390811196353705e-05,
      "loss": 1.0626,
      "step": 5300
    },
    {
      "epoch": 0.27711456417080904,
      "grad_norm": 3.835960865020752,
      "learning_rate": 4.538211297253101e-05,
      "loss": 1.0537,
      "step": 5310
    },
    {
      "epoch": 0.27763643717301395,
      "grad_norm": 4.584191799163818,
      "learning_rate": 4.537341474870831e-05,
      "loss": 1.0904,
      "step": 5320
    },
    {
      "epoch": 0.27815831017521886,
      "grad_norm": 4.285675525665283,
      "learning_rate": 4.536471652488562e-05,
      "loss": 1.0839,
      "step": 5330
    },
    {
      "epoch": 0.27868018317742377,
      "grad_norm": 4.895606517791748,
      "learning_rate": 4.5356018301062926e-05,
      "loss": 1.0563,
      "step": 5340
    },
    {
      "epoch": 0.2792020561796287,
      "grad_norm": 3.995026111602783,
      "learning_rate": 4.534732007724023e-05,
      "loss": 1.2002,
      "step": 5350
    },
    {
      "epoch": 0.2797239291818336,
      "grad_norm": 4.489776134490967,
      "learning_rate": 4.533862185341753e-05,
      "loss": 1.0011,
      "step": 5360
    },
    {
      "epoch": 0.2802458021840385,
      "grad_norm": 4.6968817710876465,
      "learning_rate": 4.532992362959484e-05,
      "loss": 0.9864,
      "step": 5370
    },
    {
      "epoch": 0.2807676751862434,
      "grad_norm": 3.966326951980591,
      "learning_rate": 4.532122540577215e-05,
      "loss": 1.0687,
      "step": 5380
    },
    {
      "epoch": 0.28128954818844837,
      "grad_norm": 4.413974761962891,
      "learning_rate": 4.531252718194945e-05,
      "loss": 1.0561,
      "step": 5390
    },
    {
      "epoch": 0.2818114211906533,
      "grad_norm": 4.47479248046875,
      "learning_rate": 4.5303828958126754e-05,
      "loss": 1.0974,
      "step": 5400
    },
    {
      "epoch": 0.2823332941928582,
      "grad_norm": 4.475172519683838,
      "learning_rate": 4.529513073430406e-05,
      "loss": 1.1607,
      "step": 5410
    },
    {
      "epoch": 0.2828551671950631,
      "grad_norm": 5.720571994781494,
      "learning_rate": 4.528643251048136e-05,
      "loss": 1.0314,
      "step": 5420
    },
    {
      "epoch": 0.283377040197268,
      "grad_norm": 4.387081623077393,
      "learning_rate": 4.527773428665867e-05,
      "loss": 1.007,
      "step": 5430
    },
    {
      "epoch": 0.2838989131994729,
      "grad_norm": 4.819616794586182,
      "learning_rate": 4.5269036062835975e-05,
      "loss": 1.0544,
      "step": 5440
    },
    {
      "epoch": 0.2844207862016778,
      "grad_norm": 4.7274627685546875,
      "learning_rate": 4.526033783901328e-05,
      "loss": 1.0586,
      "step": 5450
    },
    {
      "epoch": 0.28494265920388273,
      "grad_norm": 4.755660533905029,
      "learning_rate": 4.5251639615190576e-05,
      "loss": 1.1228,
      "step": 5460
    },
    {
      "epoch": 0.28546453220608764,
      "grad_norm": 5.418482303619385,
      "learning_rate": 4.524294139136788e-05,
      "loss": 1.1637,
      "step": 5470
    },
    {
      "epoch": 0.28598640520829255,
      "grad_norm": 5.423502445220947,
      "learning_rate": 4.523424316754519e-05,
      "loss": 1.1076,
      "step": 5480
    },
    {
      "epoch": 0.28650827821049746,
      "grad_norm": 4.7290873527526855,
      "learning_rate": 4.522554494372249e-05,
      "loss": 1.0935,
      "step": 5490
    },
    {
      "epoch": 0.28703015121270237,
      "grad_norm": 4.305572032928467,
      "learning_rate": 4.5216846719899797e-05,
      "loss": 0.9838,
      "step": 5500
    },
    {
      "epoch": 0.2875520242149073,
      "grad_norm": 4.429622650146484,
      "learning_rate": 4.52081484960771e-05,
      "loss": 1.0244,
      "step": 5510
    },
    {
      "epoch": 0.28807389721711224,
      "grad_norm": 5.84445333480835,
      "learning_rate": 4.5199450272254404e-05,
      "loss": 1.1333,
      "step": 5520
    },
    {
      "epoch": 0.28859577021931715,
      "grad_norm": 4.607588291168213,
      "learning_rate": 4.5190752048431714e-05,
      "loss": 1.0583,
      "step": 5530
    },
    {
      "epoch": 0.28911764322152206,
      "grad_norm": 4.748863697052002,
      "learning_rate": 4.518205382460902e-05,
      "loss": 0.9781,
      "step": 5540
    },
    {
      "epoch": 0.28963951622372697,
      "grad_norm": 5.040297508239746,
      "learning_rate": 4.517335560078632e-05,
      "loss": 1.1364,
      "step": 5550
    },
    {
      "epoch": 0.2901613892259319,
      "grad_norm": 4.263574600219727,
      "learning_rate": 4.5164657376963625e-05,
      "loss": 1.0549,
      "step": 5560
    },
    {
      "epoch": 0.2906832622281368,
      "grad_norm": 3.972381830215454,
      "learning_rate": 4.515595915314093e-05,
      "loss": 1.0322,
      "step": 5570
    },
    {
      "epoch": 0.2912051352303417,
      "grad_norm": 4.327001571655273,
      "learning_rate": 4.514726092931824e-05,
      "loss": 1.0658,
      "step": 5580
    },
    {
      "epoch": 0.2917270082325466,
      "grad_norm": 5.115451812744141,
      "learning_rate": 4.513856270549554e-05,
      "loss": 1.0681,
      "step": 5590
    },
    {
      "epoch": 0.2922488812347515,
      "grad_norm": 4.3756422996521,
      "learning_rate": 4.5129864481672846e-05,
      "loss": 1.1783,
      "step": 5600
    },
    {
      "epoch": 0.2927707542369564,
      "grad_norm": 4.970703601837158,
      "learning_rate": 4.512116625785015e-05,
      "loss": 0.8943,
      "step": 5610
    },
    {
      "epoch": 0.29329262723916133,
      "grad_norm": 4.852116107940674,
      "learning_rate": 4.511246803402745e-05,
      "loss": 1.0153,
      "step": 5620
    },
    {
      "epoch": 0.29381450024136624,
      "grad_norm": 5.774364948272705,
      "learning_rate": 4.510376981020476e-05,
      "loss": 1.0466,
      "step": 5630
    },
    {
      "epoch": 0.2943363732435712,
      "grad_norm": 3.891812324523926,
      "learning_rate": 4.5095071586382067e-05,
      "loss": 1.022,
      "step": 5640
    },
    {
      "epoch": 0.2948582462457761,
      "grad_norm": 4.191998481750488,
      "learning_rate": 4.508637336255937e-05,
      "loss": 0.9875,
      "step": 5650
    },
    {
      "epoch": 0.295380119247981,
      "grad_norm": 3.9002411365509033,
      "learning_rate": 4.5077675138736674e-05,
      "loss": 1.1451,
      "step": 5660
    },
    {
      "epoch": 0.29590199225018593,
      "grad_norm": 5.027022361755371,
      "learning_rate": 4.506897691491398e-05,
      "loss": 1.187,
      "step": 5670
    },
    {
      "epoch": 0.29642386525239084,
      "grad_norm": 4.182656288146973,
      "learning_rate": 4.506027869109128e-05,
      "loss": 1.0777,
      "step": 5680
    },
    {
      "epoch": 0.29694573825459575,
      "grad_norm": 5.4126763343811035,
      "learning_rate": 4.5051580467268584e-05,
      "loss": 1.1779,
      "step": 5690
    },
    {
      "epoch": 0.29746761125680066,
      "grad_norm": 4.3057861328125,
      "learning_rate": 4.504288224344589e-05,
      "loss": 1.0375,
      "step": 5700
    },
    {
      "epoch": 0.29798948425900557,
      "grad_norm": 5.357059478759766,
      "learning_rate": 4.503418401962319e-05,
      "loss": 1.0961,
      "step": 5710
    },
    {
      "epoch": 0.2985113572612105,
      "grad_norm": 4.133066654205322,
      "learning_rate": 4.5025485795800495e-05,
      "loss": 1.0436,
      "step": 5720
    },
    {
      "epoch": 0.2990332302634154,
      "grad_norm": 4.960912704467773,
      "learning_rate": 4.5016787571977805e-05,
      "loss": 1.0748,
      "step": 5730
    },
    {
      "epoch": 0.2995551032656203,
      "grad_norm": 4.244312763214111,
      "learning_rate": 4.500808934815511e-05,
      "loss": 1.0337,
      "step": 5740
    },
    {
      "epoch": 0.3000769762678252,
      "grad_norm": 3.802963972091675,
      "learning_rate": 4.499939112433241e-05,
      "loss": 1.0603,
      "step": 5750
    },
    {
      "epoch": 0.3005988492700301,
      "grad_norm": 4.9351887702941895,
      "learning_rate": 4.4990692900509716e-05,
      "loss": 1.1561,
      "step": 5760
    },
    {
      "epoch": 0.3011207222722351,
      "grad_norm": 4.5997819900512695,
      "learning_rate": 4.498199467668702e-05,
      "loss": 1.0212,
      "step": 5770
    },
    {
      "epoch": 0.30164259527444,
      "grad_norm": 4.74406623840332,
      "learning_rate": 4.497329645286433e-05,
      "loss": 1.084,
      "step": 5780
    },
    {
      "epoch": 0.3021644682766449,
      "grad_norm": 4.794493198394775,
      "learning_rate": 4.496459822904163e-05,
      "loss": 1.1136,
      "step": 5790
    },
    {
      "epoch": 0.3026863412788498,
      "grad_norm": 4.272096157073975,
      "learning_rate": 4.495590000521894e-05,
      "loss": 0.9178,
      "step": 5800
    },
    {
      "epoch": 0.3032082142810547,
      "grad_norm": 4.880882740020752,
      "learning_rate": 4.494720178139624e-05,
      "loss": 0.9977,
      "step": 5810
    },
    {
      "epoch": 0.3037300872832596,
      "grad_norm": 5.042996883392334,
      "learning_rate": 4.4938503557573544e-05,
      "loss": 1.0499,
      "step": 5820
    },
    {
      "epoch": 0.30425196028546453,
      "grad_norm": 4.644847869873047,
      "learning_rate": 4.4929805333750854e-05,
      "loss": 1.1606,
      "step": 5830
    },
    {
      "epoch": 0.30477383328766944,
      "grad_norm": 6.530862331390381,
      "learning_rate": 4.492110710992816e-05,
      "loss": 1.0683,
      "step": 5840
    },
    {
      "epoch": 0.30529570628987435,
      "grad_norm": 4.774514198303223,
      "learning_rate": 4.491240888610546e-05,
      "loss": 1.0798,
      "step": 5850
    },
    {
      "epoch": 0.30581757929207926,
      "grad_norm": 4.752744674682617,
      "learning_rate": 4.4903710662282765e-05,
      "loss": 1.0175,
      "step": 5860
    },
    {
      "epoch": 0.30633945229428416,
      "grad_norm": 4.872307300567627,
      "learning_rate": 4.489501243846007e-05,
      "loss": 1.1392,
      "step": 5870
    },
    {
      "epoch": 0.3068613252964891,
      "grad_norm": 4.610931873321533,
      "learning_rate": 4.488631421463737e-05,
      "loss": 1.0923,
      "step": 5880
    },
    {
      "epoch": 0.30738319829869404,
      "grad_norm": 4.289731502532959,
      "learning_rate": 4.4877615990814676e-05,
      "loss": 1.1084,
      "step": 5890
    },
    {
      "epoch": 0.30790507130089895,
      "grad_norm": 4.857282638549805,
      "learning_rate": 4.486891776699198e-05,
      "loss": 1.058,
      "step": 5900
    },
    {
      "epoch": 0.30842694430310386,
      "grad_norm": 4.723478317260742,
      "learning_rate": 4.486021954316928e-05,
      "loss": 1.1084,
      "step": 5910
    },
    {
      "epoch": 0.30894881730530876,
      "grad_norm": 4.947139739990234,
      "learning_rate": 4.485152131934659e-05,
      "loss": 1.0078,
      "step": 5920
    },
    {
      "epoch": 0.3094706903075137,
      "grad_norm": 3.9306888580322266,
      "learning_rate": 4.4842823095523897e-05,
      "loss": 1.084,
      "step": 5930
    },
    {
      "epoch": 0.3099925633097186,
      "grad_norm": 5.020507335662842,
      "learning_rate": 4.48341248717012e-05,
      "loss": 1.0834,
      "step": 5940
    },
    {
      "epoch": 0.3105144363119235,
      "grad_norm": 3.2416770458221436,
      "learning_rate": 4.4825426647878504e-05,
      "loss": 1.0855,
      "step": 5950
    },
    {
      "epoch": 0.3110363093141284,
      "grad_norm": 3.893425226211548,
      "learning_rate": 4.481672842405581e-05,
      "loss": 1.1092,
      "step": 5960
    },
    {
      "epoch": 0.3115581823163333,
      "grad_norm": 4.505772590637207,
      "learning_rate": 4.480803020023312e-05,
      "loss": 1.0877,
      "step": 5970
    },
    {
      "epoch": 0.3120800553185382,
      "grad_norm": 3.9794199466705322,
      "learning_rate": 4.479933197641042e-05,
      "loss": 1.0134,
      "step": 5980
    },
    {
      "epoch": 0.3126019283207431,
      "grad_norm": 4.095259666442871,
      "learning_rate": 4.4790633752587725e-05,
      "loss": 1.0662,
      "step": 5990
    },
    {
      "epoch": 0.31312380132294804,
      "grad_norm": 5.4345703125,
      "learning_rate": 4.478193552876503e-05,
      "loss": 1.0683,
      "step": 6000
    },
    {
      "epoch": 0.313645674325153,
      "grad_norm": 3.8328616619110107,
      "learning_rate": 4.477323730494233e-05,
      "loss": 1.1132,
      "step": 6010
    },
    {
      "epoch": 0.3141675473273579,
      "grad_norm": 4.3062872886657715,
      "learning_rate": 4.4764539081119635e-05,
      "loss": 1.0986,
      "step": 6020
    },
    {
      "epoch": 0.3146894203295628,
      "grad_norm": 4.762863636016846,
      "learning_rate": 4.4755840857296946e-05,
      "loss": 1.0811,
      "step": 6030
    },
    {
      "epoch": 0.31521129333176773,
      "grad_norm": 4.706498146057129,
      "learning_rate": 4.474714263347425e-05,
      "loss": 0.9907,
      "step": 6040
    },
    {
      "epoch": 0.31573316633397264,
      "grad_norm": 4.263026714324951,
      "learning_rate": 4.473844440965155e-05,
      "loss": 0.9188,
      "step": 6050
    },
    {
      "epoch": 0.31625503933617755,
      "grad_norm": 4.177793025970459,
      "learning_rate": 4.4729746185828856e-05,
      "loss": 1.0365,
      "step": 6060
    },
    {
      "epoch": 0.31677691233838245,
      "grad_norm": 4.145765781402588,
      "learning_rate": 4.472104796200616e-05,
      "loss": 1.033,
      "step": 6070
    },
    {
      "epoch": 0.31729878534058736,
      "grad_norm": 4.731429576873779,
      "learning_rate": 4.471234973818347e-05,
      "loss": 1.1303,
      "step": 6080
    },
    {
      "epoch": 0.3178206583427923,
      "grad_norm": 5.504493236541748,
      "learning_rate": 4.470365151436077e-05,
      "loss": 1.0785,
      "step": 6090
    },
    {
      "epoch": 0.3183425313449972,
      "grad_norm": 5.481440544128418,
      "learning_rate": 4.469495329053807e-05,
      "loss": 1.115,
      "step": 6100
    },
    {
      "epoch": 0.3188644043472021,
      "grad_norm": 4.739593029022217,
      "learning_rate": 4.4686255066715374e-05,
      "loss": 1.1026,
      "step": 6110
    },
    {
      "epoch": 0.319386277349407,
      "grad_norm": 3.5920393466949463,
      "learning_rate": 4.4677556842892684e-05,
      "loss": 1.1194,
      "step": 6120
    },
    {
      "epoch": 0.3199081503516119,
      "grad_norm": 4.493908405303955,
      "learning_rate": 4.466885861906999e-05,
      "loss": 1.0442,
      "step": 6130
    },
    {
      "epoch": 0.3204300233538169,
      "grad_norm": 5.198693752288818,
      "learning_rate": 4.466016039524729e-05,
      "loss": 1.1137,
      "step": 6140
    },
    {
      "epoch": 0.3209518963560218,
      "grad_norm": 3.9954535961151123,
      "learning_rate": 4.4651462171424595e-05,
      "loss": 0.9966,
      "step": 6150
    },
    {
      "epoch": 0.3214737693582267,
      "grad_norm": 4.9329094886779785,
      "learning_rate": 4.46427639476019e-05,
      "loss": 0.9878,
      "step": 6160
    },
    {
      "epoch": 0.3219956423604316,
      "grad_norm": 5.294462203979492,
      "learning_rate": 4.463406572377921e-05,
      "loss": 1.0548,
      "step": 6170
    },
    {
      "epoch": 0.3225175153626365,
      "grad_norm": 4.678293228149414,
      "learning_rate": 4.462536749995651e-05,
      "loss": 1.1187,
      "step": 6180
    },
    {
      "epoch": 0.3230393883648414,
      "grad_norm": 4.152077674865723,
      "learning_rate": 4.461753909851609e-05,
      "loss": 1.1194,
      "step": 6190
    },
    {
      "epoch": 0.3235612613670463,
      "grad_norm": 4.6081109046936035,
      "learning_rate": 4.460884087469339e-05,
      "loss": 1.0057,
      "step": 6200
    },
    {
      "epoch": 0.32408313436925124,
      "grad_norm": 5.087030410766602,
      "learning_rate": 4.4600142650870696e-05,
      "loss": 1.0484,
      "step": 6210
    },
    {
      "epoch": 0.32460500737145614,
      "grad_norm": 4.936200141906738,
      "learning_rate": 4.4591444427048e-05,
      "loss": 1.1134,
      "step": 6220
    },
    {
      "epoch": 0.32512688037366105,
      "grad_norm": 4.526604175567627,
      "learning_rate": 4.45827462032253e-05,
      "loss": 1.1511,
      "step": 6230
    },
    {
      "epoch": 0.32564875337586596,
      "grad_norm": 4.475792407989502,
      "learning_rate": 4.457404797940261e-05,
      "loss": 1.0678,
      "step": 6240
    },
    {
      "epoch": 0.32617062637807087,
      "grad_norm": 3.630089521408081,
      "learning_rate": 4.456534975557991e-05,
      "loss": 0.9517,
      "step": 6250
    },
    {
      "epoch": 0.32669249938027584,
      "grad_norm": 5.119325637817383,
      "learning_rate": 4.4556651531757214e-05,
      "loss": 1.0242,
      "step": 6260
    },
    {
      "epoch": 0.32721437238248074,
      "grad_norm": 4.025352954864502,
      "learning_rate": 4.454795330793452e-05,
      "loss": 1.0596,
      "step": 6270
    },
    {
      "epoch": 0.32773624538468565,
      "grad_norm": 4.3137288093566895,
      "learning_rate": 4.453925508411183e-05,
      "loss": 0.9377,
      "step": 6280
    },
    {
      "epoch": 0.32825811838689056,
      "grad_norm": 4.8754401206970215,
      "learning_rate": 4.453055686028913e-05,
      "loss": 1.0914,
      "step": 6290
    },
    {
      "epoch": 0.32877999138909547,
      "grad_norm": 4.829043388366699,
      "learning_rate": 4.4521858636466435e-05,
      "loss": 1.0742,
      "step": 6300
    },
    {
      "epoch": 0.3293018643913004,
      "grad_norm": 4.831640243530273,
      "learning_rate": 4.451316041264374e-05,
      "loss": 1.0896,
      "step": 6310
    },
    {
      "epoch": 0.3298237373935053,
      "grad_norm": 4.100015163421631,
      "learning_rate": 4.450446218882104e-05,
      "loss": 1.121,
      "step": 6320
    },
    {
      "epoch": 0.3303456103957102,
      "grad_norm": 4.688483238220215,
      "learning_rate": 4.449576396499835e-05,
      "loss": 1.2082,
      "step": 6330
    },
    {
      "epoch": 0.3308674833979151,
      "grad_norm": 4.000520706176758,
      "learning_rate": 4.4487065741175656e-05,
      "loss": 1.1159,
      "step": 6340
    },
    {
      "epoch": 0.33138935640012,
      "grad_norm": 4.182018756866455,
      "learning_rate": 4.447836751735296e-05,
      "loss": 1.0943,
      "step": 6350
    },
    {
      "epoch": 0.3319112294023249,
      "grad_norm": 4.004086494445801,
      "learning_rate": 4.446966929353026e-05,
      "loss": 1.0434,
      "step": 6360
    },
    {
      "epoch": 0.33243310240452983,
      "grad_norm": 4.1646246910095215,
      "learning_rate": 4.4460971069707566e-05,
      "loss": 1.0569,
      "step": 6370
    },
    {
      "epoch": 0.33295497540673474,
      "grad_norm": 4.278195858001709,
      "learning_rate": 4.445227284588488e-05,
      "loss": 1.0542,
      "step": 6380
    },
    {
      "epoch": 0.3334768484089397,
      "grad_norm": 5.012117862701416,
      "learning_rate": 4.444357462206218e-05,
      "loss": 1.1744,
      "step": 6390
    },
    {
      "epoch": 0.3339987214111446,
      "grad_norm": 4.133551120758057,
      "learning_rate": 4.4434876398239484e-05,
      "loss": 1.1109,
      "step": 6400
    },
    {
      "epoch": 0.3345205944133495,
      "grad_norm": 3.8837175369262695,
      "learning_rate": 4.442617817441679e-05,
      "loss": 1.0355,
      "step": 6410
    },
    {
      "epoch": 0.33504246741555443,
      "grad_norm": 5.181999683380127,
      "learning_rate": 4.441747995059409e-05,
      "loss": 1.0065,
      "step": 6420
    },
    {
      "epoch": 0.33556434041775934,
      "grad_norm": 4.789642333984375,
      "learning_rate": 4.44087817267714e-05,
      "loss": 0.9923,
      "step": 6430
    },
    {
      "epoch": 0.33608621341996425,
      "grad_norm": 3.9839913845062256,
      "learning_rate": 4.44000835029487e-05,
      "loss": 1.1055,
      "step": 6440
    },
    {
      "epoch": 0.33660808642216916,
      "grad_norm": 3.950631856918335,
      "learning_rate": 4.4391385279126e-05,
      "loss": 0.9351,
      "step": 6450
    },
    {
      "epoch": 0.33712995942437407,
      "grad_norm": 4.006711006164551,
      "learning_rate": 4.4382687055303305e-05,
      "loss": 0.9507,
      "step": 6460
    },
    {
      "epoch": 0.337651832426579,
      "grad_norm": 5.543549060821533,
      "learning_rate": 4.437398883148061e-05,
      "loss": 1.1314,
      "step": 6470
    },
    {
      "epoch": 0.3381737054287839,
      "grad_norm": 4.73128604888916,
      "learning_rate": 4.436529060765792e-05,
      "loss": 0.9996,
      "step": 6480
    },
    {
      "epoch": 0.3386955784309888,
      "grad_norm": 6.027141094207764,
      "learning_rate": 4.435659238383522e-05,
      "loss": 1.1026,
      "step": 6490
    },
    {
      "epoch": 0.3392174514331937,
      "grad_norm": 5.022853374481201,
      "learning_rate": 4.4347894160012526e-05,
      "loss": 1.0856,
      "step": 6500
    },
    {
      "epoch": 0.33973932443539867,
      "grad_norm": 4.969917297363281,
      "learning_rate": 4.433919593618983e-05,
      "loss": 1.0333,
      "step": 6510
    },
    {
      "epoch": 0.3402611974376036,
      "grad_norm": 4.87515115737915,
      "learning_rate": 4.433049771236713e-05,
      "loss": 0.9217,
      "step": 6520
    },
    {
      "epoch": 0.3407830704398085,
      "grad_norm": 5.086510181427002,
      "learning_rate": 4.4321799488544444e-05,
      "loss": 1.1823,
      "step": 6530
    },
    {
      "epoch": 0.3413049434420134,
      "grad_norm": 4.4156999588012695,
      "learning_rate": 4.431310126472175e-05,
      "loss": 1.0755,
      "step": 6540
    },
    {
      "epoch": 0.3418268164442183,
      "grad_norm": 4.727639198303223,
      "learning_rate": 4.430440304089905e-05,
      "loss": 1.1053,
      "step": 6550
    },
    {
      "epoch": 0.3423486894464232,
      "grad_norm": 5.361532688140869,
      "learning_rate": 4.4295704817076354e-05,
      "loss": 1.161,
      "step": 6560
    },
    {
      "epoch": 0.3428705624486281,
      "grad_norm": 5.324329853057861,
      "learning_rate": 4.428700659325366e-05,
      "loss": 1.0658,
      "step": 6570
    },
    {
      "epoch": 0.34339243545083303,
      "grad_norm": 4.044068813323975,
      "learning_rate": 4.427830836943097e-05,
      "loss": 1.1932,
      "step": 6580
    },
    {
      "epoch": 0.34391430845303794,
      "grad_norm": 4.291125774383545,
      "learning_rate": 4.426961014560827e-05,
      "loss": 1.0441,
      "step": 6590
    },
    {
      "epoch": 0.34443618145524285,
      "grad_norm": 5.618902206420898,
      "learning_rate": 4.4260911921785575e-05,
      "loss": 1.0762,
      "step": 6600
    },
    {
      "epoch": 0.34495805445744776,
      "grad_norm": 4.633714199066162,
      "learning_rate": 4.425221369796288e-05,
      "loss": 1.0455,
      "step": 6610
    },
    {
      "epoch": 0.34547992745965267,
      "grad_norm": 3.9497525691986084,
      "learning_rate": 4.424351547414018e-05,
      "loss": 1.0627,
      "step": 6620
    },
    {
      "epoch": 0.34600180046185763,
      "grad_norm": 4.545682430267334,
      "learning_rate": 4.423481725031749e-05,
      "loss": 0.9736,
      "step": 6630
    },
    {
      "epoch": 0.34652367346406254,
      "grad_norm": 4.926306247711182,
      "learning_rate": 4.4226119026494796e-05,
      "loss": 1.0519,
      "step": 6640
    },
    {
      "epoch": 0.34704554646626745,
      "grad_norm": 4.439229965209961,
      "learning_rate": 4.421742080267209e-05,
      "loss": 1.0873,
      "step": 6650
    },
    {
      "epoch": 0.34756741946847236,
      "grad_norm": 3.4057512283325195,
      "learning_rate": 4.4208722578849396e-05,
      "loss": 0.9557,
      "step": 6660
    },
    {
      "epoch": 0.34808929247067727,
      "grad_norm": 5.5664544105529785,
      "learning_rate": 4.420002435502671e-05,
      "loss": 1.1827,
      "step": 6670
    },
    {
      "epoch": 0.3486111654728822,
      "grad_norm": 4.227226734161377,
      "learning_rate": 4.419132613120401e-05,
      "loss": 1.0514,
      "step": 6680
    },
    {
      "epoch": 0.3491330384750871,
      "grad_norm": 5.137386798858643,
      "learning_rate": 4.4182627907381314e-05,
      "loss": 1.1049,
      "step": 6690
    },
    {
      "epoch": 0.349654911477292,
      "grad_norm": 4.064234733581543,
      "learning_rate": 4.417392968355862e-05,
      "loss": 1.0072,
      "step": 6700
    },
    {
      "epoch": 0.3501767844794969,
      "grad_norm": 4.648256301879883,
      "learning_rate": 4.416523145973592e-05,
      "loss": 1.0583,
      "step": 6710
    },
    {
      "epoch": 0.3506986574817018,
      "grad_norm": 5.022603511810303,
      "learning_rate": 4.4156533235913225e-05,
      "loss": 1.0544,
      "step": 6720
    },
    {
      "epoch": 0.3512205304839067,
      "grad_norm": 4.650893211364746,
      "learning_rate": 4.4147835012090535e-05,
      "loss": 1.0296,
      "step": 6730
    },
    {
      "epoch": 0.35174240348611163,
      "grad_norm": 4.482600212097168,
      "learning_rate": 4.413913678826784e-05,
      "loss": 0.9778,
      "step": 6740
    },
    {
      "epoch": 0.35226427648831654,
      "grad_norm": 4.907472133636475,
      "learning_rate": 4.413043856444514e-05,
      "loss": 1.0379,
      "step": 6750
    },
    {
      "epoch": 0.3527861494905215,
      "grad_norm": 4.546114444732666,
      "learning_rate": 4.4121740340622446e-05,
      "loss": 1.0726,
      "step": 6760
    },
    {
      "epoch": 0.3533080224927264,
      "grad_norm": 4.5844340324401855,
      "learning_rate": 4.411304211679975e-05,
      "loss": 1.0947,
      "step": 6770
    },
    {
      "epoch": 0.3538298954949313,
      "grad_norm": 4.4080328941345215,
      "learning_rate": 4.410434389297706e-05,
      "loss": 0.9775,
      "step": 6780
    },
    {
      "epoch": 0.35435176849713623,
      "grad_norm": 3.6549203395843506,
      "learning_rate": 4.409564566915436e-05,
      "loss": 1.0497,
      "step": 6790
    },
    {
      "epoch": 0.35487364149934114,
      "grad_norm": 4.390869140625,
      "learning_rate": 4.4086947445331666e-05,
      "loss": 1.0917,
      "step": 6800
    },
    {
      "epoch": 0.35539551450154605,
      "grad_norm": 5.318845748901367,
      "learning_rate": 4.407824922150897e-05,
      "loss": 0.9843,
      "step": 6810
    },
    {
      "epoch": 0.35591738750375096,
      "grad_norm": 4.982092380523682,
      "learning_rate": 4.4069550997686274e-05,
      "loss": 0.9889,
      "step": 6820
    },
    {
      "epoch": 0.35643926050595587,
      "grad_norm": 3.712482213973999,
      "learning_rate": 4.4060852773863584e-05,
      "loss": 0.9793,
      "step": 6830
    },
    {
      "epoch": 0.3569611335081608,
      "grad_norm": 5.375080108642578,
      "learning_rate": 4.405215455004089e-05,
      "loss": 1.0769,
      "step": 6840
    },
    {
      "epoch": 0.3574830065103657,
      "grad_norm": 4.6998419761657715,
      "learning_rate": 4.404345632621819e-05,
      "loss": 1.1285,
      "step": 6850
    },
    {
      "epoch": 0.3580048795125706,
      "grad_norm": 5.123586654663086,
      "learning_rate": 4.403475810239549e-05,
      "loss": 1.0342,
      "step": 6860
    },
    {
      "epoch": 0.3585267525147755,
      "grad_norm": 4.433322429656982,
      "learning_rate": 4.40260598785728e-05,
      "loss": 1.0275,
      "step": 6870
    },
    {
      "epoch": 0.35904862551698047,
      "grad_norm": 4.608012676239014,
      "learning_rate": 4.40173616547501e-05,
      "loss": 0.9195,
      "step": 6880
    },
    {
      "epoch": 0.3595704985191854,
      "grad_norm": 4.426945686340332,
      "learning_rate": 4.4008663430927405e-05,
      "loss": 1.1621,
      "step": 6890
    },
    {
      "epoch": 0.3600923715213903,
      "grad_norm": 4.417391300201416,
      "learning_rate": 4.399996520710471e-05,
      "loss": 0.9486,
      "step": 6900
    },
    {
      "epoch": 0.3606142445235952,
      "grad_norm": 3.868237257003784,
      "learning_rate": 4.399126698328201e-05,
      "loss": 1.0731,
      "step": 6910
    },
    {
      "epoch": 0.3611361175258001,
      "grad_norm": 4.348087787628174,
      "learning_rate": 4.398256875945932e-05,
      "loss": 1.0219,
      "step": 6920
    },
    {
      "epoch": 0.361657990528005,
      "grad_norm": 5.146921634674072,
      "learning_rate": 4.3973870535636626e-05,
      "loss": 1.0515,
      "step": 6930
    },
    {
      "epoch": 0.3621798635302099,
      "grad_norm": 4.945202827453613,
      "learning_rate": 4.396517231181393e-05,
      "loss": 0.9513,
      "step": 6940
    },
    {
      "epoch": 0.36270173653241483,
      "grad_norm": 3.837966203689575,
      "learning_rate": 4.395647408799123e-05,
      "loss": 0.9427,
      "step": 6950
    },
    {
      "epoch": 0.36322360953461974,
      "grad_norm": 5.998059272766113,
      "learning_rate": 4.394777586416854e-05,
      "loss": 0.9524,
      "step": 6960
    },
    {
      "epoch": 0.36374548253682465,
      "grad_norm": 4.234387397766113,
      "learning_rate": 4.393907764034584e-05,
      "loss": 0.9285,
      "step": 6970
    },
    {
      "epoch": 0.36426735553902956,
      "grad_norm": 3.8735787868499756,
      "learning_rate": 4.393037941652315e-05,
      "loss": 1.0913,
      "step": 6980
    },
    {
      "epoch": 0.36478922854123447,
      "grad_norm": 4.299156188964844,
      "learning_rate": 4.3921681192700454e-05,
      "loss": 1.085,
      "step": 6990
    },
    {
      "epoch": 0.36531110154343943,
      "grad_norm": 4.979771137237549,
      "learning_rate": 4.391298296887776e-05,
      "loss": 0.9502,
      "step": 7000
    },
    {
      "epoch": 0.36583297454564434,
      "grad_norm": 4.4252190589904785,
      "learning_rate": 4.390428474505506e-05,
      "loss": 0.9879,
      "step": 7010
    },
    {
      "epoch": 0.36635484754784925,
      "grad_norm": 3.695539712905884,
      "learning_rate": 4.3895586521232365e-05,
      "loss": 0.9936,
      "step": 7020
    },
    {
      "epoch": 0.36687672055005416,
      "grad_norm": 3.722766637802124,
      "learning_rate": 4.3886888297409675e-05,
      "loss": 0.972,
      "step": 7030
    },
    {
      "epoch": 0.36739859355225907,
      "grad_norm": 4.4064764976501465,
      "learning_rate": 4.387819007358698e-05,
      "loss": 1.0538,
      "step": 7040
    },
    {
      "epoch": 0.367920466554464,
      "grad_norm": 3.7357871532440186,
      "learning_rate": 4.386949184976428e-05,
      "loss": 0.9999,
      "step": 7050
    },
    {
      "epoch": 0.3684423395566689,
      "grad_norm": 5.639990329742432,
      "learning_rate": 4.3860793625941586e-05,
      "loss": 0.9765,
      "step": 7060
    },
    {
      "epoch": 0.3689642125588738,
      "grad_norm": 4.217538356781006,
      "learning_rate": 4.385209540211889e-05,
      "loss": 1.0087,
      "step": 7070
    },
    {
      "epoch": 0.3694860855610787,
      "grad_norm": 4.877538204193115,
      "learning_rate": 4.384339717829619e-05,
      "loss": 1.0341,
      "step": 7080
    },
    {
      "epoch": 0.3700079585632836,
      "grad_norm": 4.957415580749512,
      "learning_rate": 4.3834698954473497e-05,
      "loss": 1.084,
      "step": 7090
    },
    {
      "epoch": 0.3705298315654885,
      "grad_norm": 4.544590950012207,
      "learning_rate": 4.38260007306508e-05,
      "loss": 1.0737,
      "step": 7100
    },
    {
      "epoch": 0.37105170456769343,
      "grad_norm": 4.135619163513184,
      "learning_rate": 4.3817302506828104e-05,
      "loss": 0.9443,
      "step": 7110
    },
    {
      "epoch": 0.37157357756989834,
      "grad_norm": 4.235060214996338,
      "learning_rate": 4.3808604283005414e-05,
      "loss": 0.9703,
      "step": 7120
    },
    {
      "epoch": 0.3720954505721033,
      "grad_norm": 5.171472549438477,
      "learning_rate": 4.379990605918272e-05,
      "loss": 1.0132,
      "step": 7130
    },
    {
      "epoch": 0.3726173235743082,
      "grad_norm": 4.475857734680176,
      "learning_rate": 4.379120783536002e-05,
      "loss": 1.114,
      "step": 7140
    },
    {
      "epoch": 0.3731391965765131,
      "grad_norm": 4.222967147827148,
      "learning_rate": 4.3782509611537325e-05,
      "loss": 0.9825,
      "step": 7150
    },
    {
      "epoch": 0.37366106957871803,
      "grad_norm": 4.301926612854004,
      "learning_rate": 4.377381138771463e-05,
      "loss": 1.0748,
      "step": 7160
    },
    {
      "epoch": 0.37418294258092294,
      "grad_norm": 5.323007583618164,
      "learning_rate": 4.376511316389194e-05,
      "loss": 0.9782,
      "step": 7170
    },
    {
      "epoch": 0.37470481558312785,
      "grad_norm": 3.915724992752075,
      "learning_rate": 4.375641494006924e-05,
      "loss": 1.0042,
      "step": 7180
    },
    {
      "epoch": 0.37522668858533276,
      "grad_norm": 3.9845054149627686,
      "learning_rate": 4.3747716716246546e-05,
      "loss": 1.0213,
      "step": 7190
    },
    {
      "epoch": 0.37574856158753767,
      "grad_norm": 5.113161087036133,
      "learning_rate": 4.373901849242385e-05,
      "loss": 1.0605,
      "step": 7200
    },
    {
      "epoch": 0.3762704345897426,
      "grad_norm": 4.562958717346191,
      "learning_rate": 4.373032026860115e-05,
      "loss": 1.1089,
      "step": 7210
    },
    {
      "epoch": 0.3767923075919475,
      "grad_norm": 4.3517231941223145,
      "learning_rate": 4.372162204477846e-05,
      "loss": 1.008,
      "step": 7220
    },
    {
      "epoch": 0.3773141805941524,
      "grad_norm": 4.079843997955322,
      "learning_rate": 4.3712923820955767e-05,
      "loss": 1.1658,
      "step": 7230
    },
    {
      "epoch": 0.3778360535963573,
      "grad_norm": 3.683809995651245,
      "learning_rate": 4.370422559713307e-05,
      "loss": 1.0661,
      "step": 7240
    },
    {
      "epoch": 0.37835792659856227,
      "grad_norm": 4.927857398986816,
      "learning_rate": 4.3695527373310374e-05,
      "loss": 1.0377,
      "step": 7250
    },
    {
      "epoch": 0.3788797996007672,
      "grad_norm": 4.5494914054870605,
      "learning_rate": 4.368682914948768e-05,
      "loss": 1.0439,
      "step": 7260
    },
    {
      "epoch": 0.3794016726029721,
      "grad_norm": 4.975752353668213,
      "learning_rate": 4.367813092566498e-05,
      "loss": 1.0661,
      "step": 7270
    },
    {
      "epoch": 0.379923545605177,
      "grad_norm": 3.9163568019866943,
      "learning_rate": 4.3669432701842284e-05,
      "loss": 1.0087,
      "step": 7280
    },
    {
      "epoch": 0.3804454186073819,
      "grad_norm": 4.775856018066406,
      "learning_rate": 4.366073447801959e-05,
      "loss": 1.025,
      "step": 7290
    },
    {
      "epoch": 0.3809672916095868,
      "grad_norm": 5.128183841705322,
      "learning_rate": 4.365203625419689e-05,
      "loss": 1.1762,
      "step": 7300
    },
    {
      "epoch": 0.3814891646117917,
      "grad_norm": 5.43035364151001,
      "learning_rate": 4.3643338030374195e-05,
      "loss": 1.0119,
      "step": 7310
    },
    {
      "epoch": 0.38201103761399663,
      "grad_norm": 4.2503981590271,
      "learning_rate": 4.3634639806551505e-05,
      "loss": 1.0032,
      "step": 7320
    },
    {
      "epoch": 0.38253291061620154,
      "grad_norm": 3.4780445098876953,
      "learning_rate": 4.362594158272881e-05,
      "loss": 1.0361,
      "step": 7330
    },
    {
      "epoch": 0.38305478361840645,
      "grad_norm": 5.622657299041748,
      "learning_rate": 4.361724335890611e-05,
      "loss": 1.0471,
      "step": 7340
    },
    {
      "epoch": 0.38357665662061136,
      "grad_norm": 4.590921878814697,
      "learning_rate": 4.3608545135083416e-05,
      "loss": 1.0237,
      "step": 7350
    },
    {
      "epoch": 0.38409852962281626,
      "grad_norm": 4.549516677856445,
      "learning_rate": 4.359984691126072e-05,
      "loss": 1.0697,
      "step": 7360
    },
    {
      "epoch": 0.3846204026250212,
      "grad_norm": 4.978394508361816,
      "learning_rate": 4.359114868743803e-05,
      "loss": 1.0757,
      "step": 7370
    },
    {
      "epoch": 0.38514227562722614,
      "grad_norm": 4.970341682434082,
      "learning_rate": 4.358245046361533e-05,
      "loss": 0.9565,
      "step": 7380
    },
    {
      "epoch": 0.38566414862943105,
      "grad_norm": 4.229367733001709,
      "learning_rate": 4.357375223979264e-05,
      "loss": 1.0562,
      "step": 7390
    },
    {
      "epoch": 0.38618602163163596,
      "grad_norm": 4.653273582458496,
      "learning_rate": 4.356505401596994e-05,
      "loss": 1.0795,
      "step": 7400
    },
    {
      "epoch": 0.38670789463384087,
      "grad_norm": 3.6253085136413574,
      "learning_rate": 4.3556355792147244e-05,
      "loss": 1.0492,
      "step": 7410
    },
    {
      "epoch": 0.3872297676360458,
      "grad_norm": 5.148699760437012,
      "learning_rate": 4.3547657568324554e-05,
      "loss": 1.0721,
      "step": 7420
    },
    {
      "epoch": 0.3877516406382507,
      "grad_norm": 5.570871353149414,
      "learning_rate": 4.353895934450186e-05,
      "loss": 1.0128,
      "step": 7430
    },
    {
      "epoch": 0.3882735136404556,
      "grad_norm": 4.926342487335205,
      "learning_rate": 4.353026112067916e-05,
      "loss": 1.0574,
      "step": 7440
    },
    {
      "epoch": 0.3887953866426605,
      "grad_norm": 3.637226104736328,
      "learning_rate": 4.3521562896856465e-05,
      "loss": 0.9667,
      "step": 7450
    },
    {
      "epoch": 0.3893172596448654,
      "grad_norm": 4.430840492248535,
      "learning_rate": 4.351286467303377e-05,
      "loss": 1.0655,
      "step": 7460
    },
    {
      "epoch": 0.3898391326470703,
      "grad_norm": 4.110293865203857,
      "learning_rate": 4.350416644921108e-05,
      "loss": 1.0456,
      "step": 7470
    },
    {
      "epoch": 0.39036100564927523,
      "grad_norm": 4.382238388061523,
      "learning_rate": 4.349546822538838e-05,
      "loss": 1.0928,
      "step": 7480
    },
    {
      "epoch": 0.39088287865148014,
      "grad_norm": 4.419046878814697,
      "learning_rate": 4.348677000156568e-05,
      "loss": 1.0115,
      "step": 7490
    },
    {
      "epoch": 0.3914047516536851,
      "grad_norm": 3.6266238689422607,
      "learning_rate": 4.347807177774298e-05,
      "loss": 0.9633,
      "step": 7500
    },
    {
      "epoch": 0.39192662465589,
      "grad_norm": 4.930685997009277,
      "learning_rate": 4.3469373553920286e-05,
      "loss": 1.0352,
      "step": 7510
    },
    {
      "epoch": 0.3924484976580949,
      "grad_norm": 4.6938090324401855,
      "learning_rate": 4.3460675330097597e-05,
      "loss": 1.0642,
      "step": 7520
    },
    {
      "epoch": 0.39297037066029983,
      "grad_norm": 5.166369438171387,
      "learning_rate": 4.34519771062749e-05,
      "loss": 1.0523,
      "step": 7530
    },
    {
      "epoch": 0.39349224366250474,
      "grad_norm": 3.4422950744628906,
      "learning_rate": 4.3443278882452204e-05,
      "loss": 0.9604,
      "step": 7540
    },
    {
      "epoch": 0.39401411666470965,
      "grad_norm": 4.099920272827148,
      "learning_rate": 4.343458065862951e-05,
      "loss": 1.0553,
      "step": 7550
    },
    {
      "epoch": 0.39453598966691455,
      "grad_norm": 5.542716026306152,
      "learning_rate": 4.342588243480681e-05,
      "loss": 0.9964,
      "step": 7560
    },
    {
      "epoch": 0.39505786266911946,
      "grad_norm": 4.296926498413086,
      "learning_rate": 4.341718421098412e-05,
      "loss": 0.9994,
      "step": 7570
    },
    {
      "epoch": 0.3955797356713244,
      "grad_norm": 3.9640300273895264,
      "learning_rate": 4.3408485987161425e-05,
      "loss": 1.0559,
      "step": 7580
    },
    {
      "epoch": 0.3961016086735293,
      "grad_norm": 4.4527459144592285,
      "learning_rate": 4.339978776333873e-05,
      "loss": 0.9874,
      "step": 7590
    },
    {
      "epoch": 0.3966234816757342,
      "grad_norm": 4.7544121742248535,
      "learning_rate": 4.339108953951603e-05,
      "loss": 1.0022,
      "step": 7600
    },
    {
      "epoch": 0.3971453546779391,
      "grad_norm": 4.583972454071045,
      "learning_rate": 4.3382391315693335e-05,
      "loss": 0.9864,
      "step": 7610
    },
    {
      "epoch": 0.39766722768014406,
      "grad_norm": 4.894162654876709,
      "learning_rate": 4.3373693091870646e-05,
      "loss": 1.1106,
      "step": 7620
    },
    {
      "epoch": 0.398189100682349,
      "grad_norm": 5.14448881149292,
      "learning_rate": 4.336499486804795e-05,
      "loss": 1.1244,
      "step": 7630
    },
    {
      "epoch": 0.3987109736845539,
      "grad_norm": 6.011274337768555,
      "learning_rate": 4.335629664422525e-05,
      "loss": 1.0019,
      "step": 7640
    },
    {
      "epoch": 0.3992328466867588,
      "grad_norm": 4.685923099517822,
      "learning_rate": 4.3347598420402556e-05,
      "loss": 1.0275,
      "step": 7650
    },
    {
      "epoch": 0.3997547196889637,
      "grad_norm": 4.276398658752441,
      "learning_rate": 4.333890019657986e-05,
      "loss": 1.0472,
      "step": 7660
    },
    {
      "epoch": 0.4002765926911686,
      "grad_norm": 4.789527893066406,
      "learning_rate": 4.333020197275717e-05,
      "loss": 1.0799,
      "step": 7670
    },
    {
      "epoch": 0.4007984656933735,
      "grad_norm": 3.9949164390563965,
      "learning_rate": 4.3321503748934474e-05,
      "loss": 1.0395,
      "step": 7680
    },
    {
      "epoch": 0.4013203386955784,
      "grad_norm": 4.886964797973633,
      "learning_rate": 4.331280552511178e-05,
      "loss": 1.0792,
      "step": 7690
    },
    {
      "epoch": 0.40184221169778334,
      "grad_norm": 4.603981971740723,
      "learning_rate": 4.3304107301289074e-05,
      "loss": 0.9946,
      "step": 7700
    },
    {
      "epoch": 0.40236408469998824,
      "grad_norm": 4.0721659660339355,
      "learning_rate": 4.3295409077466384e-05,
      "loss": 0.9411,
      "step": 7710
    },
    {
      "epoch": 0.40288595770219315,
      "grad_norm": 4.7237162590026855,
      "learning_rate": 4.328671085364369e-05,
      "loss": 0.9978,
      "step": 7720
    },
    {
      "epoch": 0.40340783070439806,
      "grad_norm": 4.832433700561523,
      "learning_rate": 4.327801262982099e-05,
      "loss": 1.0197,
      "step": 7730
    },
    {
      "epoch": 0.40392970370660297,
      "grad_norm": 5.392251968383789,
      "learning_rate": 4.3269314405998295e-05,
      "loss": 1.1486,
      "step": 7740
    },
    {
      "epoch": 0.40445157670880794,
      "grad_norm": 4.377408504486084,
      "learning_rate": 4.32606161821756e-05,
      "loss": 0.9572,
      "step": 7750
    },
    {
      "epoch": 0.40497344971101285,
      "grad_norm": 4.995782852172852,
      "learning_rate": 4.325191795835291e-05,
      "loss": 1.1039,
      "step": 7760
    },
    {
      "epoch": 0.40549532271321775,
      "grad_norm": 4.620489597320557,
      "learning_rate": 4.324321973453021e-05,
      "loss": 0.959,
      "step": 7770
    },
    {
      "epoch": 0.40601719571542266,
      "grad_norm": 5.172963619232178,
      "learning_rate": 4.3234521510707516e-05,
      "loss": 1.064,
      "step": 7780
    },
    {
      "epoch": 0.40653906871762757,
      "grad_norm": 4.075128078460693,
      "learning_rate": 4.322582328688482e-05,
      "loss": 1.0673,
      "step": 7790
    },
    {
      "epoch": 0.4070609417198325,
      "grad_norm": 4.880765438079834,
      "learning_rate": 4.321712506306212e-05,
      "loss": 0.9595,
      "step": 7800
    },
    {
      "epoch": 0.4075828147220374,
      "grad_norm": 4.112283229827881,
      "learning_rate": 4.3208426839239427e-05,
      "loss": 1.0373,
      "step": 7810
    },
    {
      "epoch": 0.4081046877242423,
      "grad_norm": 5.222761631011963,
      "learning_rate": 4.319972861541674e-05,
      "loss": 1.1069,
      "step": 7820
    },
    {
      "epoch": 0.4086265607264472,
      "grad_norm": 5.087874412536621,
      "learning_rate": 4.319103039159404e-05,
      "loss": 1.0426,
      "step": 7830
    },
    {
      "epoch": 0.4091484337286521,
      "grad_norm": 3.8454275131225586,
      "learning_rate": 4.3182332167771344e-05,
      "loss": 1.162,
      "step": 7840
    },
    {
      "epoch": 0.409670306730857,
      "grad_norm": 4.351775646209717,
      "learning_rate": 4.317363394394865e-05,
      "loss": 0.9335,
      "step": 7850
    },
    {
      "epoch": 0.41019217973306193,
      "grad_norm": 4.422412872314453,
      "learning_rate": 4.316493572012595e-05,
      "loss": 0.9605,
      "step": 7860
    },
    {
      "epoch": 0.4107140527352669,
      "grad_norm": 3.789954900741577,
      "learning_rate": 4.315623749630326e-05,
      "loss": 1.0204,
      "step": 7870
    },
    {
      "epoch": 0.4112359257374718,
      "grad_norm": 5.1165056228637695,
      "learning_rate": 4.3147539272480565e-05,
      "loss": 1.1231,
      "step": 7880
    },
    {
      "epoch": 0.4117577987396767,
      "grad_norm": 3.8343896865844727,
      "learning_rate": 4.313884104865787e-05,
      "loss": 0.9911,
      "step": 7890
    },
    {
      "epoch": 0.4122796717418816,
      "grad_norm": 4.858645439147949,
      "learning_rate": 4.3130142824835165e-05,
      "loss": 1.1014,
      "step": 7900
    },
    {
      "epoch": 0.41280154474408653,
      "grad_norm": 4.88373327255249,
      "learning_rate": 4.3121444601012476e-05,
      "loss": 1.0919,
      "step": 7910
    },
    {
      "epoch": 0.41332341774629144,
      "grad_norm": 4.572630882263184,
      "learning_rate": 4.311274637718978e-05,
      "loss": 1.0511,
      "step": 7920
    },
    {
      "epoch": 0.41384529074849635,
      "grad_norm": 3.7628612518310547,
      "learning_rate": 4.310404815336708e-05,
      "loss": 1.0102,
      "step": 7930
    },
    {
      "epoch": 0.41436716375070126,
      "grad_norm": 4.498389720916748,
      "learning_rate": 4.3095349929544386e-05,
      "loss": 0.9326,
      "step": 7940
    },
    {
      "epoch": 0.41488903675290617,
      "grad_norm": 4.379709243774414,
      "learning_rate": 4.308665170572169e-05,
      "loss": 1.0177,
      "step": 7950
    },
    {
      "epoch": 0.4154109097551111,
      "grad_norm": 5.148500919342041,
      "learning_rate": 4.3077953481899e-05,
      "loss": 1.0616,
      "step": 7960
    },
    {
      "epoch": 0.415932782757316,
      "grad_norm": 4.057572841644287,
      "learning_rate": 4.3069255258076304e-05,
      "loss": 1.0634,
      "step": 7970
    },
    {
      "epoch": 0.4164546557595209,
      "grad_norm": 3.7738139629364014,
      "learning_rate": 4.306055703425361e-05,
      "loss": 0.9769,
      "step": 7980
    },
    {
      "epoch": 0.4169765287617258,
      "grad_norm": 4.404915809631348,
      "learning_rate": 4.305185881043091e-05,
      "loss": 1.0136,
      "step": 7990
    },
    {
      "epoch": 0.41749840176393077,
      "grad_norm": 4.693536758422852,
      "learning_rate": 4.3043160586608214e-05,
      "loss": 1.1145,
      "step": 8000
    },
    {
      "epoch": 0.4180202747661357,
      "grad_norm": 3.888669490814209,
      "learning_rate": 4.3034462362785525e-05,
      "loss": 1.0477,
      "step": 8010
    },
    {
      "epoch": 0.4185421477683406,
      "grad_norm": 4.804932117462158,
      "learning_rate": 4.302576413896283e-05,
      "loss": 0.9646,
      "step": 8020
    },
    {
      "epoch": 0.4190640207705455,
      "grad_norm": 3.640007972717285,
      "learning_rate": 4.301706591514013e-05,
      "loss": 0.9094,
      "step": 8030
    },
    {
      "epoch": 0.4195858937727504,
      "grad_norm": 4.492380619049072,
      "learning_rate": 4.3008367691317435e-05,
      "loss": 1.032,
      "step": 8040
    },
    {
      "epoch": 0.4201077667749553,
      "grad_norm": 3.8937716484069824,
      "learning_rate": 4.299966946749474e-05,
      "loss": 1.0296,
      "step": 8050
    },
    {
      "epoch": 0.4206296397771602,
      "grad_norm": 4.331437587738037,
      "learning_rate": 4.299097124367204e-05,
      "loss": 1.0354,
      "step": 8060
    },
    {
      "epoch": 0.42115151277936513,
      "grad_norm": 3.22847843170166,
      "learning_rate": 4.298227301984935e-05,
      "loss": 0.976,
      "step": 8070
    },
    {
      "epoch": 0.42167338578157004,
      "grad_norm": 3.8365695476531982,
      "learning_rate": 4.2973574796026656e-05,
      "loss": 1.0352,
      "step": 8080
    },
    {
      "epoch": 0.42219525878377495,
      "grad_norm": 4.79344367980957,
      "learning_rate": 4.296487657220396e-05,
      "loss": 1.0096,
      "step": 8090
    },
    {
      "epoch": 0.42271713178597986,
      "grad_norm": 3.8719475269317627,
      "learning_rate": 4.295617834838126e-05,
      "loss": 1.0736,
      "step": 8100
    },
    {
      "epoch": 0.42323900478818477,
      "grad_norm": 3.5857837200164795,
      "learning_rate": 4.294748012455857e-05,
      "loss": 0.9126,
      "step": 8110
    },
    {
      "epoch": 0.42376087779038973,
      "grad_norm": 4.445705413818359,
      "learning_rate": 4.293878190073587e-05,
      "loss": 1.0272,
      "step": 8120
    },
    {
      "epoch": 0.42428275079259464,
      "grad_norm": 4.811711311340332,
      "learning_rate": 4.2930083676913174e-05,
      "loss": 1.05,
      "step": 8130
    },
    {
      "epoch": 0.42480462379479955,
      "grad_norm": 5.430438995361328,
      "learning_rate": 4.292138545309048e-05,
      "loss": 1.0384,
      "step": 8140
    },
    {
      "epoch": 0.42532649679700446,
      "grad_norm": 3.5492327213287354,
      "learning_rate": 4.291268722926778e-05,
      "loss": 0.9857,
      "step": 8150
    },
    {
      "epoch": 0.42584836979920937,
      "grad_norm": 4.655429363250732,
      "learning_rate": 4.290398900544509e-05,
      "loss": 1.0591,
      "step": 8160
    },
    {
      "epoch": 0.4263702428014143,
      "grad_norm": 4.602718353271484,
      "learning_rate": 4.2895290781622395e-05,
      "loss": 1.0651,
      "step": 8170
    },
    {
      "epoch": 0.4268921158036192,
      "grad_norm": 3.5831475257873535,
      "learning_rate": 4.28865925577997e-05,
      "loss": 0.924,
      "step": 8180
    },
    {
      "epoch": 0.4274139888058241,
      "grad_norm": 4.6494951248168945,
      "learning_rate": 4.2877894333977e-05,
      "loss": 1.0531,
      "step": 8190
    },
    {
      "epoch": 0.427935861808029,
      "grad_norm": 4.911086559295654,
      "learning_rate": 4.2869196110154306e-05,
      "loss": 1.0091,
      "step": 8200
    },
    {
      "epoch": 0.4284577348102339,
      "grad_norm": 3.8144562244415283,
      "learning_rate": 4.2860497886331616e-05,
      "loss": 1.0535,
      "step": 8210
    },
    {
      "epoch": 0.4289796078124388,
      "grad_norm": 4.5601487159729,
      "learning_rate": 4.285179966250892e-05,
      "loss": 1.0632,
      "step": 8220
    },
    {
      "epoch": 0.42950148081464373,
      "grad_norm": 5.790211200714111,
      "learning_rate": 4.284310143868622e-05,
      "loss": 1.2023,
      "step": 8230
    },
    {
      "epoch": 0.4300233538168487,
      "grad_norm": 4.318907260894775,
      "learning_rate": 4.2834403214863527e-05,
      "loss": 1.0927,
      "step": 8240
    },
    {
      "epoch": 0.4305452268190536,
      "grad_norm": 5.3424201011657715,
      "learning_rate": 4.282570499104083e-05,
      "loss": 1.0146,
      "step": 8250
    },
    {
      "epoch": 0.4310670998212585,
      "grad_norm": 4.319552421569824,
      "learning_rate": 4.281700676721814e-05,
      "loss": 1.0367,
      "step": 8260
    },
    {
      "epoch": 0.4315889728234634,
      "grad_norm": 4.5392279624938965,
      "learning_rate": 4.2808308543395444e-05,
      "loss": 1.0586,
      "step": 8270
    },
    {
      "epoch": 0.43211084582566833,
      "grad_norm": 4.849401950836182,
      "learning_rate": 4.279961031957275e-05,
      "loss": 1.1506,
      "step": 8280
    },
    {
      "epoch": 0.43263271882787324,
      "grad_norm": 4.7558465003967285,
      "learning_rate": 4.279091209575005e-05,
      "loss": 1.0119,
      "step": 8290
    },
    {
      "epoch": 0.43315459183007815,
      "grad_norm": 4.968790531158447,
      "learning_rate": 4.2782213871927355e-05,
      "loss": 1.0305,
      "step": 8300
    },
    {
      "epoch": 0.43367646483228306,
      "grad_norm": 4.843050956726074,
      "learning_rate": 4.2773515648104665e-05,
      "loss": 1.0667,
      "step": 8310
    },
    {
      "epoch": 0.43419833783448797,
      "grad_norm": 4.397551536560059,
      "learning_rate": 4.276481742428196e-05,
      "loss": 0.9685,
      "step": 8320
    },
    {
      "epoch": 0.4347202108366929,
      "grad_norm": 4.13893985748291,
      "learning_rate": 4.2756119200459265e-05,
      "loss": 1.0447,
      "step": 8330
    },
    {
      "epoch": 0.4352420838388978,
      "grad_norm": 4.876458644866943,
      "learning_rate": 4.274742097663657e-05,
      "loss": 1.114,
      "step": 8340
    },
    {
      "epoch": 0.4357639568411027,
      "grad_norm": 4.326708793640137,
      "learning_rate": 4.273872275281387e-05,
      "loss": 1.0419,
      "step": 8350
    },
    {
      "epoch": 0.4362858298433076,
      "grad_norm": 4.410521030426025,
      "learning_rate": 4.273002452899118e-05,
      "loss": 0.9488,
      "step": 8360
    },
    {
      "epoch": 0.43680770284551257,
      "grad_norm": 5.215363025665283,
      "learning_rate": 4.2721326305168486e-05,
      "loss": 0.939,
      "step": 8370
    },
    {
      "epoch": 0.4373295758477175,
      "grad_norm": 5.401589393615723,
      "learning_rate": 4.271262808134579e-05,
      "loss": 1.0816,
      "step": 8380
    },
    {
      "epoch": 0.4378514488499224,
      "grad_norm": 4.338310241699219,
      "learning_rate": 4.2703929857523093e-05,
      "loss": 1.0883,
      "step": 8390
    },
    {
      "epoch": 0.4383733218521273,
      "grad_norm": 4.411937713623047,
      "learning_rate": 4.26952316337004e-05,
      "loss": 1.104,
      "step": 8400
    },
    {
      "epoch": 0.4388951948543322,
      "grad_norm": 5.024091720581055,
      "learning_rate": 4.268653340987771e-05,
      "loss": 1.0296,
      "step": 8410
    },
    {
      "epoch": 0.4394170678565371,
      "grad_norm": 4.443806171417236,
      "learning_rate": 4.267783518605501e-05,
      "loss": 1.0614,
      "step": 8420
    },
    {
      "epoch": 0.439938940858742,
      "grad_norm": 4.358712673187256,
      "learning_rate": 4.2669136962232314e-05,
      "loss": 0.9108,
      "step": 8430
    },
    {
      "epoch": 0.44046081386094693,
      "grad_norm": 5.652008533477783,
      "learning_rate": 4.266043873840962e-05,
      "loss": 1.0168,
      "step": 8440
    },
    {
      "epoch": 0.44098268686315184,
      "grad_norm": 4.072277069091797,
      "learning_rate": 4.265174051458692e-05,
      "loss": 1.0144,
      "step": 8450
    },
    {
      "epoch": 0.44150455986535675,
      "grad_norm": 3.9931540489196777,
      "learning_rate": 4.264304229076423e-05,
      "loss": 1.0515,
      "step": 8460
    },
    {
      "epoch": 0.44202643286756166,
      "grad_norm": 4.316378593444824,
      "learning_rate": 4.2634344066941535e-05,
      "loss": 1.0856,
      "step": 8470
    },
    {
      "epoch": 0.44254830586976657,
      "grad_norm": 4.256722927093506,
      "learning_rate": 4.262564584311884e-05,
      "loss": 1.0379,
      "step": 8480
    },
    {
      "epoch": 0.44307017887197153,
      "grad_norm": 4.646162986755371,
      "learning_rate": 4.261694761929614e-05,
      "loss": 1.01,
      "step": 8490
    },
    {
      "epoch": 0.44359205187417644,
      "grad_norm": 4.131104946136475,
      "learning_rate": 4.2608249395473446e-05,
      "loss": 1.0224,
      "step": 8500
    },
    {
      "epoch": 0.44411392487638135,
      "grad_norm": 5.275995254516602,
      "learning_rate": 4.2599551171650756e-05,
      "loss": 1.0531,
      "step": 8510
    },
    {
      "epoch": 0.44463579787858626,
      "grad_norm": 5.110236167907715,
      "learning_rate": 4.259085294782806e-05,
      "loss": 0.9595,
      "step": 8520
    },
    {
      "epoch": 0.44515767088079117,
      "grad_norm": 4.392431259155273,
      "learning_rate": 4.258215472400536e-05,
      "loss": 1.1027,
      "step": 8530
    },
    {
      "epoch": 0.4456795438829961,
      "grad_norm": 4.543727874755859,
      "learning_rate": 4.257345650018266e-05,
      "loss": 1.021,
      "step": 8540
    },
    {
      "epoch": 0.446201416885201,
      "grad_norm": 4.947291851043701,
      "learning_rate": 4.256475827635997e-05,
      "loss": 1.0083,
      "step": 8550
    },
    {
      "epoch": 0.4467232898874059,
      "grad_norm": 3.864161729812622,
      "learning_rate": 4.2556060052537274e-05,
      "loss": 1.0453,
      "step": 8560
    },
    {
      "epoch": 0.4472451628896108,
      "grad_norm": 4.280794620513916,
      "learning_rate": 4.254736182871458e-05,
      "loss": 0.9413,
      "step": 8570
    },
    {
      "epoch": 0.4477670358918157,
      "grad_norm": 4.200742244720459,
      "learning_rate": 4.253866360489188e-05,
      "loss": 0.9216,
      "step": 8580
    },
    {
      "epoch": 0.4482889088940206,
      "grad_norm": 3.6640098094940186,
      "learning_rate": 4.2529965381069185e-05,
      "loss": 0.9231,
      "step": 8590
    },
    {
      "epoch": 0.44881078189622553,
      "grad_norm": 4.592194080352783,
      "learning_rate": 4.252126715724649e-05,
      "loss": 0.8757,
      "step": 8600
    },
    {
      "epoch": 0.4493326548984305,
      "grad_norm": 4.82550573348999,
      "learning_rate": 4.25125689334238e-05,
      "loss": 0.9926,
      "step": 8610
    },
    {
      "epoch": 0.4498545279006354,
      "grad_norm": 3.810624599456787,
      "learning_rate": 4.25038707096011e-05,
      "loss": 1.031,
      "step": 8620
    },
    {
      "epoch": 0.4503764009028403,
      "grad_norm": 4.917344093322754,
      "learning_rate": 4.2495172485778406e-05,
      "loss": 1.0129,
      "step": 8630
    },
    {
      "epoch": 0.4508982739050452,
      "grad_norm": 5.142720699310303,
      "learning_rate": 4.248647426195571e-05,
      "loss": 1.0659,
      "step": 8640
    },
    {
      "epoch": 0.45142014690725013,
      "grad_norm": 4.31190824508667,
      "learning_rate": 4.247777603813301e-05,
      "loss": 1.0234,
      "step": 8650
    },
    {
      "epoch": 0.45194201990945504,
      "grad_norm": 4.833617687225342,
      "learning_rate": 4.246907781431032e-05,
      "loss": 1.0232,
      "step": 8660
    },
    {
      "epoch": 0.45246389291165995,
      "grad_norm": 4.927944183349609,
      "learning_rate": 4.246037959048763e-05,
      "loss": 0.953,
      "step": 8670
    },
    {
      "epoch": 0.45298576591386486,
      "grad_norm": 4.768002510070801,
      "learning_rate": 4.245168136666493e-05,
      "loss": 1.0413,
      "step": 8680
    },
    {
      "epoch": 0.45350763891606977,
      "grad_norm": 5.184591293334961,
      "learning_rate": 4.2442983142842234e-05,
      "loss": 0.9898,
      "step": 8690
    },
    {
      "epoch": 0.4540295119182747,
      "grad_norm": 4.641739368438721,
      "learning_rate": 4.243428491901954e-05,
      "loss": 1.0543,
      "step": 8700
    },
    {
      "epoch": 0.4545513849204796,
      "grad_norm": 4.1149797439575195,
      "learning_rate": 4.242558669519685e-05,
      "loss": 0.9941,
      "step": 8710
    },
    {
      "epoch": 0.4550732579226845,
      "grad_norm": 4.550938606262207,
      "learning_rate": 4.241688847137415e-05,
      "loss": 1.0057,
      "step": 8720
    },
    {
      "epoch": 0.4555951309248894,
      "grad_norm": 4.510898113250732,
      "learning_rate": 4.2408190247551455e-05,
      "loss": 1.0927,
      "step": 8730
    },
    {
      "epoch": 0.45611700392709437,
      "grad_norm": 5.075477123260498,
      "learning_rate": 4.239949202372875e-05,
      "loss": 1.0075,
      "step": 8740
    },
    {
      "epoch": 0.4566388769292993,
      "grad_norm": 4.179607391357422,
      "learning_rate": 4.239079379990606e-05,
      "loss": 1.0722,
      "step": 8750
    },
    {
      "epoch": 0.4571607499315042,
      "grad_norm": 4.851905822753906,
      "learning_rate": 4.2382095576083365e-05,
      "loss": 1.0204,
      "step": 8760
    },
    {
      "epoch": 0.4576826229337091,
      "grad_norm": 6.005990982055664,
      "learning_rate": 4.237339735226067e-05,
      "loss": 0.9685,
      "step": 8770
    },
    {
      "epoch": 0.458204495935914,
      "grad_norm": 3.7699124813079834,
      "learning_rate": 4.236469912843797e-05,
      "loss": 1.0836,
      "step": 8780
    },
    {
      "epoch": 0.4587263689381189,
      "grad_norm": 4.331534385681152,
      "learning_rate": 4.2356000904615276e-05,
      "loss": 0.9609,
      "step": 8790
    },
    {
      "epoch": 0.4592482419403238,
      "grad_norm": 4.530407905578613,
      "learning_rate": 4.2347302680792586e-05,
      "loss": 1.0514,
      "step": 8800
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 4.128003120422363,
      "learning_rate": 4.233860445696989e-05,
      "loss": 1.0107,
      "step": 8810
    },
    {
      "epoch": 0.46029198794473364,
      "grad_norm": 5.004971027374268,
      "learning_rate": 4.2329906233147193e-05,
      "loss": 0.8988,
      "step": 8820
    },
    {
      "epoch": 0.46081386094693855,
      "grad_norm": 4.9743266105651855,
      "learning_rate": 4.23212080093245e-05,
      "loss": 1.002,
      "step": 8830
    },
    {
      "epoch": 0.46133573394914346,
      "grad_norm": 4.529928684234619,
      "learning_rate": 4.23125097855018e-05,
      "loss": 1.0857,
      "step": 8840
    },
    {
      "epoch": 0.46185760695134837,
      "grad_norm": 5.12078857421875,
      "learning_rate": 4.230381156167911e-05,
      "loss": 1.0467,
      "step": 8850
    },
    {
      "epoch": 0.46237947995355333,
      "grad_norm": 4.722482681274414,
      "learning_rate": 4.2295113337856414e-05,
      "loss": 1.0668,
      "step": 8860
    },
    {
      "epoch": 0.46290135295575824,
      "grad_norm": 5.232975959777832,
      "learning_rate": 4.228641511403372e-05,
      "loss": 1.0289,
      "step": 8870
    },
    {
      "epoch": 0.46342322595796315,
      "grad_norm": 4.377335071563721,
      "learning_rate": 4.227771689021102e-05,
      "loss": 1.0649,
      "step": 8880
    },
    {
      "epoch": 0.46394509896016806,
      "grad_norm": 4.16796350479126,
      "learning_rate": 4.2269018666388325e-05,
      "loss": 1.0502,
      "step": 8890
    },
    {
      "epoch": 0.46446697196237297,
      "grad_norm": 4.191132068634033,
      "learning_rate": 4.226032044256563e-05,
      "loss": 1.0811,
      "step": 8900
    },
    {
      "epoch": 0.4649888449645779,
      "grad_norm": 4.372437953948975,
      "learning_rate": 4.225162221874294e-05,
      "loss": 0.9657,
      "step": 8910
    },
    {
      "epoch": 0.4655107179667828,
      "grad_norm": 5.1961283683776855,
      "learning_rate": 4.224292399492024e-05,
      "loss": 1.0585,
      "step": 8920
    },
    {
      "epoch": 0.4660325909689877,
      "grad_norm": 5.052152633666992,
      "learning_rate": 4.2234225771097546e-05,
      "loss": 1.1051,
      "step": 8930
    },
    {
      "epoch": 0.4665544639711926,
      "grad_norm": 4.306354999542236,
      "learning_rate": 4.222552754727485e-05,
      "loss": 0.9963,
      "step": 8940
    },
    {
      "epoch": 0.4670763369733975,
      "grad_norm": 4.778066158294678,
      "learning_rate": 4.221682932345215e-05,
      "loss": 1.0312,
      "step": 8950
    },
    {
      "epoch": 0.4675982099756024,
      "grad_norm": 3.8940906524658203,
      "learning_rate": 4.220813109962946e-05,
      "loss": 0.9008,
      "step": 8960
    },
    {
      "epoch": 0.46812008297780733,
      "grad_norm": 4.454577445983887,
      "learning_rate": 4.219943287580676e-05,
      "loss": 1.0148,
      "step": 8970
    },
    {
      "epoch": 0.46864195598001224,
      "grad_norm": 4.108306407928467,
      "learning_rate": 4.2190734651984064e-05,
      "loss": 1.0089,
      "step": 8980
    },
    {
      "epoch": 0.4691638289822172,
      "grad_norm": 4.555793285369873,
      "learning_rate": 4.218203642816137e-05,
      "loss": 0.894,
      "step": 8990
    },
    {
      "epoch": 0.4696857019844221,
      "grad_norm": 4.424726486206055,
      "learning_rate": 4.217333820433868e-05,
      "loss": 0.9603,
      "step": 9000
    },
    {
      "epoch": 0.470207574986627,
      "grad_norm": 4.0965094566345215,
      "learning_rate": 4.216463998051598e-05,
      "loss": 1.0538,
      "step": 9010
    },
    {
      "epoch": 0.47072944798883193,
      "grad_norm": 4.419618606567383,
      "learning_rate": 4.2155941756693285e-05,
      "loss": 1.0191,
      "step": 9020
    },
    {
      "epoch": 0.47125132099103684,
      "grad_norm": 4.6439127922058105,
      "learning_rate": 4.214724353287059e-05,
      "loss": 1.0267,
      "step": 9030
    },
    {
      "epoch": 0.47177319399324175,
      "grad_norm": 4.03680944442749,
      "learning_rate": 4.213854530904789e-05,
      "loss": 1.0614,
      "step": 9040
    },
    {
      "epoch": 0.47229506699544666,
      "grad_norm": 4.865091323852539,
      "learning_rate": 4.21298470852252e-05,
      "loss": 1.0361,
      "step": 9050
    },
    {
      "epoch": 0.47281693999765156,
      "grad_norm": 4.5657172203063965,
      "learning_rate": 4.2121148861402506e-05,
      "loss": 1.1015,
      "step": 9060
    },
    {
      "epoch": 0.4733388129998565,
      "grad_norm": 4.207398414611816,
      "learning_rate": 4.211245063757981e-05,
      "loss": 1.0702,
      "step": 9070
    },
    {
      "epoch": 0.4738606860020614,
      "grad_norm": 4.393315315246582,
      "learning_rate": 4.210375241375711e-05,
      "loss": 1.0056,
      "step": 9080
    },
    {
      "epoch": 0.4743825590042663,
      "grad_norm": 6.226866245269775,
      "learning_rate": 4.2095054189934416e-05,
      "loss": 1.0157,
      "step": 9090
    },
    {
      "epoch": 0.4749044320064712,
      "grad_norm": 4.777642250061035,
      "learning_rate": 4.208635596611173e-05,
      "loss": 1.0001,
      "step": 9100
    },
    {
      "epoch": 0.47542630500867616,
      "grad_norm": 4.404069900512695,
      "learning_rate": 4.207765774228903e-05,
      "loss": 0.8845,
      "step": 9110
    },
    {
      "epoch": 0.4759481780108811,
      "grad_norm": 4.094026565551758,
      "learning_rate": 4.2068959518466334e-05,
      "loss": 0.9057,
      "step": 9120
    },
    {
      "epoch": 0.476470051013086,
      "grad_norm": 4.825336456298828,
      "learning_rate": 4.206026129464364e-05,
      "loss": 0.9497,
      "step": 9130
    },
    {
      "epoch": 0.4769919240152909,
      "grad_norm": 4.20029878616333,
      "learning_rate": 4.205156307082094e-05,
      "loss": 0.9924,
      "step": 9140
    },
    {
      "epoch": 0.4775137970174958,
      "grad_norm": 4.048728942871094,
      "learning_rate": 4.2042864846998244e-05,
      "loss": 0.9878,
      "step": 9150
    },
    {
      "epoch": 0.4780356700197007,
      "grad_norm": 5.7209296226501465,
      "learning_rate": 4.203416662317555e-05,
      "loss": 1.0267,
      "step": 9160
    },
    {
      "epoch": 0.4785575430219056,
      "grad_norm": 3.6682677268981934,
      "learning_rate": 4.202546839935285e-05,
      "loss": 1.0155,
      "step": 9170
    },
    {
      "epoch": 0.4790794160241105,
      "grad_norm": 2.8143415451049805,
      "learning_rate": 4.2016770175530155e-05,
      "loss": 0.9991,
      "step": 9180
    },
    {
      "epoch": 0.47960128902631544,
      "grad_norm": 4.550994873046875,
      "learning_rate": 4.200807195170746e-05,
      "loss": 1.0457,
      "step": 9190
    },
    {
      "epoch": 0.48012316202852034,
      "grad_norm": 4.289320468902588,
      "learning_rate": 4.199937372788477e-05,
      "loss": 0.9045,
      "step": 9200
    },
    {
      "epoch": 0.48064503503072525,
      "grad_norm": 4.566858768463135,
      "learning_rate": 4.199067550406207e-05,
      "loss": 1.025,
      "step": 9210
    },
    {
      "epoch": 0.48116690803293016,
      "grad_norm": 4.9709014892578125,
      "learning_rate": 4.1981977280239376e-05,
      "loss": 1.0409,
      "step": 9220
    },
    {
      "epoch": 0.4816887810351351,
      "grad_norm": 3.8106167316436768,
      "learning_rate": 4.197327905641668e-05,
      "loss": 0.9918,
      "step": 9230
    },
    {
      "epoch": 0.48221065403734004,
      "grad_norm": 4.385397911071777,
      "learning_rate": 4.196458083259398e-05,
      "loss": 0.948,
      "step": 9240
    },
    {
      "epoch": 0.48273252703954495,
      "grad_norm": 3.615468978881836,
      "learning_rate": 4.1955882608771293e-05,
      "loss": 0.9754,
      "step": 9250
    },
    {
      "epoch": 0.48325440004174985,
      "grad_norm": 4.384305953979492,
      "learning_rate": 4.19471843849486e-05,
      "loss": 1.0844,
      "step": 9260
    },
    {
      "epoch": 0.48377627304395476,
      "grad_norm": 4.714532852172852,
      "learning_rate": 4.19384861611259e-05,
      "loss": 0.9317,
      "step": 9270
    },
    {
      "epoch": 0.48429814604615967,
      "grad_norm": 4.618856906890869,
      "learning_rate": 4.1929787937303204e-05,
      "loss": 1.0317,
      "step": 9280
    },
    {
      "epoch": 0.4848200190483646,
      "grad_norm": 4.988653659820557,
      "learning_rate": 4.192108971348051e-05,
      "loss": 1.0412,
      "step": 9290
    },
    {
      "epoch": 0.4853418920505695,
      "grad_norm": 4.103303909301758,
      "learning_rate": 4.191239148965782e-05,
      "loss": 0.9579,
      "step": 9300
    },
    {
      "epoch": 0.4858637650527744,
      "grad_norm": 4.2069091796875,
      "learning_rate": 4.190369326583512e-05,
      "loss": 1.0576,
      "step": 9310
    },
    {
      "epoch": 0.4863856380549793,
      "grad_norm": 4.540400981903076,
      "learning_rate": 4.1894995042012425e-05,
      "loss": 1.0476,
      "step": 9320
    },
    {
      "epoch": 0.4869075110571842,
      "grad_norm": 5.1507792472839355,
      "learning_rate": 4.188629681818973e-05,
      "loss": 1.0496,
      "step": 9330
    },
    {
      "epoch": 0.4874293840593891,
      "grad_norm": 4.56985330581665,
      "learning_rate": 4.187759859436703e-05,
      "loss": 0.9092,
      "step": 9340
    },
    {
      "epoch": 0.48795125706159403,
      "grad_norm": 3.777498245239258,
      "learning_rate": 4.186890037054434e-05,
      "loss": 1.1063,
      "step": 9350
    },
    {
      "epoch": 0.488473130063799,
      "grad_norm": 4.43195915222168,
      "learning_rate": 4.186020214672164e-05,
      "loss": 0.8976,
      "step": 9360
    },
    {
      "epoch": 0.4889950030660039,
      "grad_norm": 3.9795169830322266,
      "learning_rate": 4.185150392289894e-05,
      "loss": 0.9593,
      "step": 9370
    },
    {
      "epoch": 0.4895168760682088,
      "grad_norm": 3.484452247619629,
      "learning_rate": 4.1842805699076246e-05,
      "loss": 0.9562,
      "step": 9380
    },
    {
      "epoch": 0.4900387490704137,
      "grad_norm": 4.177051067352295,
      "learning_rate": 4.183410747525356e-05,
      "loss": 1.0209,
      "step": 9390
    },
    {
      "epoch": 0.49056062207261864,
      "grad_norm": 3.8696630001068115,
      "learning_rate": 4.182540925143086e-05,
      "loss": 1.0236,
      "step": 9400
    },
    {
      "epoch": 0.49108249507482354,
      "grad_norm": 4.335165500640869,
      "learning_rate": 4.1816711027608164e-05,
      "loss": 1.0399,
      "step": 9410
    },
    {
      "epoch": 0.49160436807702845,
      "grad_norm": 3.7726259231567383,
      "learning_rate": 4.180801280378547e-05,
      "loss": 1.0285,
      "step": 9420
    },
    {
      "epoch": 0.49212624107923336,
      "grad_norm": 4.995293617248535,
      "learning_rate": 4.179931457996277e-05,
      "loss": 1.0284,
      "step": 9430
    },
    {
      "epoch": 0.49264811408143827,
      "grad_norm": 3.920046329498291,
      "learning_rate": 4.1790616356140074e-05,
      "loss": 1.0153,
      "step": 9440
    },
    {
      "epoch": 0.4931699870836432,
      "grad_norm": 5.13870096206665,
      "learning_rate": 4.1781918132317385e-05,
      "loss": 0.9247,
      "step": 9450
    },
    {
      "epoch": 0.4936918600858481,
      "grad_norm": 4.20030403137207,
      "learning_rate": 4.177321990849469e-05,
      "loss": 1.024,
      "step": 9460
    },
    {
      "epoch": 0.494213733088053,
      "grad_norm": 5.216078758239746,
      "learning_rate": 4.176452168467199e-05,
      "loss": 1.1385,
      "step": 9470
    },
    {
      "epoch": 0.49473560609025796,
      "grad_norm": 4.187135219573975,
      "learning_rate": 4.1755823460849295e-05,
      "loss": 1.0087,
      "step": 9480
    },
    {
      "epoch": 0.49525747909246287,
      "grad_norm": 4.753890514373779,
      "learning_rate": 4.17471252370266e-05,
      "loss": 1.101,
      "step": 9490
    },
    {
      "epoch": 0.4957793520946678,
      "grad_norm": 4.563903331756592,
      "learning_rate": 4.173842701320391e-05,
      "loss": 1.0264,
      "step": 9500
    },
    {
      "epoch": 0.4963012250968727,
      "grad_norm": 4.241333961486816,
      "learning_rate": 4.172972878938121e-05,
      "loss": 1.0304,
      "step": 9510
    },
    {
      "epoch": 0.4968230980990776,
      "grad_norm": 3.933440923690796,
      "learning_rate": 4.1721030565558516e-05,
      "loss": 0.9687,
      "step": 9520
    },
    {
      "epoch": 0.4973449711012825,
      "grad_norm": 4.35321569442749,
      "learning_rate": 4.171233234173582e-05,
      "loss": 0.9875,
      "step": 9530
    },
    {
      "epoch": 0.4978668441034874,
      "grad_norm": 4.353476524353027,
      "learning_rate": 4.1703634117913123e-05,
      "loss": 1.0405,
      "step": 9540
    },
    {
      "epoch": 0.4983887171056923,
      "grad_norm": 3.9120662212371826,
      "learning_rate": 4.1694935894090434e-05,
      "loss": 0.9625,
      "step": 9550
    },
    {
      "epoch": 0.49891059010789723,
      "grad_norm": 4.412659168243408,
      "learning_rate": 4.168623767026774e-05,
      "loss": 1.0614,
      "step": 9560
    },
    {
      "epoch": 0.49943246311010214,
      "grad_norm": 4.881520748138428,
      "learning_rate": 4.1677539446445034e-05,
      "loss": 1.038,
      "step": 9570
    },
    {
      "epoch": 0.49995433611230705,
      "grad_norm": 4.7439045906066895,
      "learning_rate": 4.166884122262234e-05,
      "loss": 1.1555,
      "step": 9580
    },
    {
      "epoch": 0.500476209114512,
      "grad_norm": 4.1234588623046875,
      "learning_rate": 4.166014299879965e-05,
      "loss": 0.9339,
      "step": 9590
    },
    {
      "epoch": 0.5009980821167169,
      "grad_norm": 4.494361877441406,
      "learning_rate": 4.165144477497695e-05,
      "loss": 1.0459,
      "step": 9600
    },
    {
      "epoch": 0.5015199551189218,
      "grad_norm": 4.150854587554932,
      "learning_rate": 4.1642746551154255e-05,
      "loss": 0.9612,
      "step": 9610
    },
    {
      "epoch": 0.5020418281211267,
      "grad_norm": 3.898359537124634,
      "learning_rate": 4.163404832733156e-05,
      "loss": 1.0548,
      "step": 9620
    },
    {
      "epoch": 0.5025637011233316,
      "grad_norm": 4.138518333435059,
      "learning_rate": 4.162535010350886e-05,
      "loss": 1.0094,
      "step": 9630
    },
    {
      "epoch": 0.5030855741255366,
      "grad_norm": 4.115327835083008,
      "learning_rate": 4.161665187968617e-05,
      "loss": 0.9645,
      "step": 9640
    },
    {
      "epoch": 0.5036074471277414,
      "grad_norm": 4.063162803649902,
      "learning_rate": 4.1607953655863476e-05,
      "loss": 1.047,
      "step": 9650
    },
    {
      "epoch": 0.5041293201299464,
      "grad_norm": 3.4726736545562744,
      "learning_rate": 4.159925543204078e-05,
      "loss": 0.9204,
      "step": 9660
    },
    {
      "epoch": 0.5046511931321513,
      "grad_norm": 4.773776531219482,
      "learning_rate": 4.159055720821808e-05,
      "loss": 1.0832,
      "step": 9670
    },
    {
      "epoch": 0.5051730661343562,
      "grad_norm": 4.423631191253662,
      "learning_rate": 4.158185898439539e-05,
      "loss": 1.0825,
      "step": 9680
    },
    {
      "epoch": 0.5056949391365612,
      "grad_norm": 4.026866912841797,
      "learning_rate": 4.157316076057269e-05,
      "loss": 1.0061,
      "step": 9690
    },
    {
      "epoch": 0.506216812138766,
      "grad_norm": 5.414808750152588,
      "learning_rate": 4.156446253675e-05,
      "loss": 1.0622,
      "step": 9700
    },
    {
      "epoch": 0.506738685140971,
      "grad_norm": 4.632570266723633,
      "learning_rate": 4.1555764312927304e-05,
      "loss": 0.9779,
      "step": 9710
    },
    {
      "epoch": 0.5072605581431758,
      "grad_norm": 5.04935359954834,
      "learning_rate": 4.154706608910461e-05,
      "loss": 1.1291,
      "step": 9720
    },
    {
      "epoch": 0.5077824311453808,
      "grad_norm": 3.6713106632232666,
      "learning_rate": 4.153836786528191e-05,
      "loss": 0.9525,
      "step": 9730
    },
    {
      "epoch": 0.5083043041475857,
      "grad_norm": 4.776798725128174,
      "learning_rate": 4.1529669641459215e-05,
      "loss": 0.9854,
      "step": 9740
    },
    {
      "epoch": 0.5088261771497906,
      "grad_norm": 4.729885101318359,
      "learning_rate": 4.1520971417636525e-05,
      "loss": 1.0094,
      "step": 9750
    },
    {
      "epoch": 0.5093480501519955,
      "grad_norm": 4.821442127227783,
      "learning_rate": 4.151227319381383e-05,
      "loss": 1.0252,
      "step": 9760
    },
    {
      "epoch": 0.5098699231542004,
      "grad_norm": 4.732519149780273,
      "learning_rate": 4.150357496999113e-05,
      "loss": 0.9361,
      "step": 9770
    },
    {
      "epoch": 0.5103917961564053,
      "grad_norm": 4.312872409820557,
      "learning_rate": 4.149487674616843e-05,
      "loss": 0.9518,
      "step": 9780
    },
    {
      "epoch": 0.5109136691586103,
      "grad_norm": 5.376587867736816,
      "learning_rate": 4.148617852234574e-05,
      "loss": 1.0582,
      "step": 9790
    },
    {
      "epoch": 0.5114355421608152,
      "grad_norm": 4.645711421966553,
      "learning_rate": 4.147748029852304e-05,
      "loss": 1.0307,
      "step": 9800
    },
    {
      "epoch": 0.5119574151630201,
      "grad_norm": 4.967959880828857,
      "learning_rate": 4.1468782074700346e-05,
      "loss": 1.0162,
      "step": 9810
    },
    {
      "epoch": 0.512479288165225,
      "grad_norm": 4.806929588317871,
      "learning_rate": 4.146008385087765e-05,
      "loss": 0.9595,
      "step": 9820
    },
    {
      "epoch": 0.5130011611674299,
      "grad_norm": 4.120990753173828,
      "learning_rate": 4.1451385627054954e-05,
      "loss": 0.995,
      "step": 9830
    },
    {
      "epoch": 0.5135230341696349,
      "grad_norm": 4.439975738525391,
      "learning_rate": 4.1442687403232264e-05,
      "loss": 1.0816,
      "step": 9840
    },
    {
      "epoch": 0.5140449071718397,
      "grad_norm": 4.039195537567139,
      "learning_rate": 4.143398917940957e-05,
      "loss": 0.9912,
      "step": 9850
    },
    {
      "epoch": 0.5145667801740447,
      "grad_norm": 4.748598098754883,
      "learning_rate": 4.142529095558687e-05,
      "loss": 0.9399,
      "step": 9860
    },
    {
      "epoch": 0.5150886531762495,
      "grad_norm": 5.091982364654541,
      "learning_rate": 4.1416592731764174e-05,
      "loss": 1.0257,
      "step": 9870
    },
    {
      "epoch": 0.5156105261784545,
      "grad_norm": 5.000667095184326,
      "learning_rate": 4.140789450794148e-05,
      "loss": 1.0389,
      "step": 9880
    },
    {
      "epoch": 0.5161323991806593,
      "grad_norm": 4.5665788650512695,
      "learning_rate": 4.139919628411879e-05,
      "loss": 0.9486,
      "step": 9890
    },
    {
      "epoch": 0.5166542721828643,
      "grad_norm": 4.060916900634766,
      "learning_rate": 4.139049806029609e-05,
      "loss": 0.9924,
      "step": 9900
    },
    {
      "epoch": 0.5171761451850692,
      "grad_norm": 4.547183990478516,
      "learning_rate": 4.1381799836473395e-05,
      "loss": 1.0194,
      "step": 9910
    },
    {
      "epoch": 0.5176980181872741,
      "grad_norm": 4.825728416442871,
      "learning_rate": 4.13731016126507e-05,
      "loss": 1.002,
      "step": 9920
    },
    {
      "epoch": 0.5182198911894791,
      "grad_norm": 4.13192892074585,
      "learning_rate": 4.1364403388828e-05,
      "loss": 0.9166,
      "step": 9930
    },
    {
      "epoch": 0.5187417641916839,
      "grad_norm": 4.702728748321533,
      "learning_rate": 4.135570516500531e-05,
      "loss": 0.9678,
      "step": 9940
    },
    {
      "epoch": 0.5192636371938889,
      "grad_norm": 3.6983964443206787,
      "learning_rate": 4.1347006941182616e-05,
      "loss": 0.9698,
      "step": 9950
    },
    {
      "epoch": 0.5197855101960938,
      "grad_norm": 4.077053070068359,
      "learning_rate": 4.133830871735992e-05,
      "loss": 0.9883,
      "step": 9960
    },
    {
      "epoch": 0.5203073831982987,
      "grad_norm": 4.39384126663208,
      "learning_rate": 4.1329610493537224e-05,
      "loss": 0.9964,
      "step": 9970
    },
    {
      "epoch": 0.5208292562005036,
      "grad_norm": 4.75595235824585,
      "learning_rate": 4.132091226971453e-05,
      "loss": 0.9713,
      "step": 9980
    },
    {
      "epoch": 0.5213511292027085,
      "grad_norm": 3.623396158218384,
      "learning_rate": 4.131221404589183e-05,
      "loss": 0.968,
      "step": 9990
    },
    {
      "epoch": 0.5218730022049134,
      "grad_norm": 4.404087066650391,
      "learning_rate": 4.1303515822069134e-05,
      "loss": 1.0188,
      "step": 10000
    },
    {
      "epoch": 0.5223948752071184,
      "grad_norm": 3.833871603012085,
      "learning_rate": 4.129481759824644e-05,
      "loss": 0.9445,
      "step": 10010
    },
    {
      "epoch": 0.5229167482093232,
      "grad_norm": 5.057323455810547,
      "learning_rate": 4.128611937442374e-05,
      "loss": 0.9989,
      "step": 10020
    },
    {
      "epoch": 0.5234386212115282,
      "grad_norm": 5.013970375061035,
      "learning_rate": 4.1277421150601045e-05,
      "loss": 1.0433,
      "step": 10030
    },
    {
      "epoch": 0.5239604942137331,
      "grad_norm": 5.163576126098633,
      "learning_rate": 4.1268722926778355e-05,
      "loss": 1.0349,
      "step": 10040
    },
    {
      "epoch": 0.524482367215938,
      "grad_norm": 4.348964691162109,
      "learning_rate": 4.126002470295566e-05,
      "loss": 0.9008,
      "step": 10050
    },
    {
      "epoch": 0.525004240218143,
      "grad_norm": 4.344833850860596,
      "learning_rate": 4.125132647913296e-05,
      "loss": 0.9516,
      "step": 10060
    },
    {
      "epoch": 0.5255261132203478,
      "grad_norm": 3.7029497623443604,
      "learning_rate": 4.1242628255310266e-05,
      "loss": 0.9782,
      "step": 10070
    },
    {
      "epoch": 0.5260479862225528,
      "grad_norm": 4.371627330780029,
      "learning_rate": 4.123393003148757e-05,
      "loss": 0.9964,
      "step": 10080
    },
    {
      "epoch": 0.5265698592247576,
      "grad_norm": 5.214148998260498,
      "learning_rate": 4.122523180766488e-05,
      "loss": 1.0327,
      "step": 10090
    },
    {
      "epoch": 0.5270917322269626,
      "grad_norm": 5.2739644050598145,
      "learning_rate": 4.121653358384218e-05,
      "loss": 1.0139,
      "step": 10100
    },
    {
      "epoch": 0.5276136052291674,
      "grad_norm": 5.488528728485107,
      "learning_rate": 4.120783536001949e-05,
      "loss": 0.9992,
      "step": 10110
    },
    {
      "epoch": 0.5281354782313724,
      "grad_norm": 3.889547824859619,
      "learning_rate": 4.119913713619679e-05,
      "loss": 1.0004,
      "step": 10120
    },
    {
      "epoch": 0.5286573512335773,
      "grad_norm": 3.43449330329895,
      "learning_rate": 4.1190438912374094e-05,
      "loss": 1.052,
      "step": 10130
    },
    {
      "epoch": 0.5291792242357822,
      "grad_norm": 5.127185821533203,
      "learning_rate": 4.1181740688551404e-05,
      "loss": 1.0289,
      "step": 10140
    },
    {
      "epoch": 0.5297010972379871,
      "grad_norm": 5.728837490081787,
      "learning_rate": 4.117304246472871e-05,
      "loss": 1.0433,
      "step": 10150
    },
    {
      "epoch": 0.530222970240192,
      "grad_norm": 4.07493782043457,
      "learning_rate": 4.116434424090601e-05,
      "loss": 1.0338,
      "step": 10160
    },
    {
      "epoch": 0.530744843242397,
      "grad_norm": 4.598692417144775,
      "learning_rate": 4.1155646017083315e-05,
      "loss": 1.0237,
      "step": 10170
    },
    {
      "epoch": 0.5312667162446019,
      "grad_norm": 4.714569568634033,
      "learning_rate": 4.114694779326062e-05,
      "loss": 0.9617,
      "step": 10180
    },
    {
      "epoch": 0.5317885892468068,
      "grad_norm": 5.009101867675781,
      "learning_rate": 4.113911939182019e-05,
      "loss": 0.9893,
      "step": 10190
    },
    {
      "epoch": 0.5323104622490117,
      "grad_norm": 4.0555830001831055,
      "learning_rate": 4.11304211679975e-05,
      "loss": 1.0298,
      "step": 10200
    },
    {
      "epoch": 0.5328323352512166,
      "grad_norm": 5.212367534637451,
      "learning_rate": 4.11217229441748e-05,
      "loss": 0.941,
      "step": 10210
    },
    {
      "epoch": 0.5333542082534215,
      "grad_norm": 4.4958176612854,
      "learning_rate": 4.1113024720352106e-05,
      "loss": 0.9921,
      "step": 10220
    },
    {
      "epoch": 0.5338760812556265,
      "grad_norm": 4.594559669494629,
      "learning_rate": 4.110432649652941e-05,
      "loss": 1.109,
      "step": 10230
    },
    {
      "epoch": 0.5343979542578313,
      "grad_norm": 4.878511428833008,
      "learning_rate": 4.109562827270671e-05,
      "loss": 1.0934,
      "step": 10240
    },
    {
      "epoch": 0.5349198272600363,
      "grad_norm": 4.479773044586182,
      "learning_rate": 4.108693004888402e-05,
      "loss": 1.0138,
      "step": 10250
    },
    {
      "epoch": 0.5354417002622411,
      "grad_norm": 4.231977462768555,
      "learning_rate": 4.1078231825061327e-05,
      "loss": 1.0909,
      "step": 10260
    },
    {
      "epoch": 0.5359635732644461,
      "grad_norm": 4.8258185386657715,
      "learning_rate": 4.106953360123863e-05,
      "loss": 0.976,
      "step": 10270
    },
    {
      "epoch": 0.536485446266651,
      "grad_norm": 3.7343697547912598,
      "learning_rate": 4.1060835377415934e-05,
      "loss": 1.0666,
      "step": 10280
    },
    {
      "epoch": 0.5370073192688559,
      "grad_norm": 4.10330867767334,
      "learning_rate": 4.105213715359324e-05,
      "loss": 1.03,
      "step": 10290
    },
    {
      "epoch": 0.5375291922710609,
      "grad_norm": 4.6310834884643555,
      "learning_rate": 4.104343892977055e-05,
      "loss": 0.9871,
      "step": 10300
    },
    {
      "epoch": 0.5380510652732657,
      "grad_norm": 4.986695289611816,
      "learning_rate": 4.103474070594785e-05,
      "loss": 1.0312,
      "step": 10310
    },
    {
      "epoch": 0.5385729382754707,
      "grad_norm": 5.258022308349609,
      "learning_rate": 4.1026042482125155e-05,
      "loss": 1.0079,
      "step": 10320
    },
    {
      "epoch": 0.5390948112776756,
      "grad_norm": 4.556112289428711,
      "learning_rate": 4.101734425830246e-05,
      "loss": 0.9979,
      "step": 10330
    },
    {
      "epoch": 0.5396166842798805,
      "grad_norm": 3.8765673637390137,
      "learning_rate": 4.100864603447976e-05,
      "loss": 0.987,
      "step": 10340
    },
    {
      "epoch": 0.5401385572820854,
      "grad_norm": 4.208306312561035,
      "learning_rate": 4.0999947810657065e-05,
      "loss": 1.0625,
      "step": 10350
    },
    {
      "epoch": 0.5406604302842903,
      "grad_norm": 4.447131156921387,
      "learning_rate": 4.099124958683437e-05,
      "loss": 0.8933,
      "step": 10360
    },
    {
      "epoch": 0.5411823032864952,
      "grad_norm": 3.7191154956817627,
      "learning_rate": 4.098255136301167e-05,
      "loss": 1.0182,
      "step": 10370
    },
    {
      "epoch": 0.5417041762887002,
      "grad_norm": 4.208848476409912,
      "learning_rate": 4.0973853139188976e-05,
      "loss": 0.9364,
      "step": 10380
    },
    {
      "epoch": 0.542226049290905,
      "grad_norm": 5.133346080780029,
      "learning_rate": 4.096515491536628e-05,
      "loss": 1.0065,
      "step": 10390
    },
    {
      "epoch": 0.54274792229311,
      "grad_norm": 4.114108562469482,
      "learning_rate": 4.095645669154359e-05,
      "loss": 0.9121,
      "step": 10400
    },
    {
      "epoch": 0.5432697952953149,
      "grad_norm": 3.986029624938965,
      "learning_rate": 4.094775846772089e-05,
      "loss": 1.0551,
      "step": 10410
    },
    {
      "epoch": 0.5437916682975198,
      "grad_norm": 3.9931435585021973,
      "learning_rate": 4.09390602438982e-05,
      "loss": 1.0353,
      "step": 10420
    },
    {
      "epoch": 0.5443135412997248,
      "grad_norm": 3.4788825511932373,
      "learning_rate": 4.09303620200755e-05,
      "loss": 1.0432,
      "step": 10430
    },
    {
      "epoch": 0.5448354143019296,
      "grad_norm": 3.962688446044922,
      "learning_rate": 4.0921663796252804e-05,
      "loss": 0.9656,
      "step": 10440
    },
    {
      "epoch": 0.5453572873041346,
      "grad_norm": 4.362998962402344,
      "learning_rate": 4.0912965572430114e-05,
      "loss": 0.9641,
      "step": 10450
    },
    {
      "epoch": 0.5458791603063394,
      "grad_norm": 5.078681945800781,
      "learning_rate": 4.090426734860742e-05,
      "loss": 0.9683,
      "step": 10460
    },
    {
      "epoch": 0.5464010333085444,
      "grad_norm": 3.2372519969940186,
      "learning_rate": 4.089556912478472e-05,
      "loss": 0.9901,
      "step": 10470
    },
    {
      "epoch": 0.5469229063107492,
      "grad_norm": 4.722842216491699,
      "learning_rate": 4.0886870900962025e-05,
      "loss": 0.871,
      "step": 10480
    },
    {
      "epoch": 0.5474447793129542,
      "grad_norm": 4.037820339202881,
      "learning_rate": 4.087817267713933e-05,
      "loss": 1.0814,
      "step": 10490
    },
    {
      "epoch": 0.5479666523151591,
      "grad_norm": 4.950450897216797,
      "learning_rate": 4.086947445331664e-05,
      "loss": 1.0233,
      "step": 10500
    },
    {
      "epoch": 0.548488525317364,
      "grad_norm": 4.08882999420166,
      "learning_rate": 4.086077622949394e-05,
      "loss": 0.9364,
      "step": 10510
    },
    {
      "epoch": 0.5490103983195689,
      "grad_norm": 4.450945854187012,
      "learning_rate": 4.0852078005671246e-05,
      "loss": 1.0754,
      "step": 10520
    },
    {
      "epoch": 0.5495322713217738,
      "grad_norm": 5.121930122375488,
      "learning_rate": 4.084337978184855e-05,
      "loss": 1.0571,
      "step": 10530
    },
    {
      "epoch": 0.5500541443239788,
      "grad_norm": 5.05918025970459,
      "learning_rate": 4.083468155802585e-05,
      "loss": 1.033,
      "step": 10540
    },
    {
      "epoch": 0.5505760173261837,
      "grad_norm": 4.166794300079346,
      "learning_rate": 4.082598333420316e-05,
      "loss": 1.0164,
      "step": 10550
    },
    {
      "epoch": 0.5510978903283886,
      "grad_norm": 3.885931968688965,
      "learning_rate": 4.081728511038046e-05,
      "loss": 1.0583,
      "step": 10560
    },
    {
      "epoch": 0.5516197633305935,
      "grad_norm": 4.332244873046875,
      "learning_rate": 4.0808586886557764e-05,
      "loss": 0.9694,
      "step": 10570
    },
    {
      "epoch": 0.5521416363327984,
      "grad_norm": 4.219420433044434,
      "learning_rate": 4.079988866273507e-05,
      "loss": 1.0742,
      "step": 10580
    },
    {
      "epoch": 0.5526635093350033,
      "grad_norm": 4.153178691864014,
      "learning_rate": 4.079119043891238e-05,
      "loss": 1.0464,
      "step": 10590
    },
    {
      "epoch": 0.5531853823372083,
      "grad_norm": 4.034243106842041,
      "learning_rate": 4.078249221508968e-05,
      "loss": 1.0161,
      "step": 10600
    },
    {
      "epoch": 0.5537072553394131,
      "grad_norm": 4.29806661605835,
      "learning_rate": 4.0773793991266985e-05,
      "loss": 1.0199,
      "step": 10610
    },
    {
      "epoch": 0.5542291283416181,
      "grad_norm": 5.582324981689453,
      "learning_rate": 4.076509576744429e-05,
      "loss": 1.0609,
      "step": 10620
    },
    {
      "epoch": 0.5547510013438229,
      "grad_norm": 3.910510778427124,
      "learning_rate": 4.075639754362159e-05,
      "loss": 0.9443,
      "step": 10630
    },
    {
      "epoch": 0.5552728743460279,
      "grad_norm": 4.082221508026123,
      "learning_rate": 4.07476993197989e-05,
      "loss": 0.9982,
      "step": 10640
    },
    {
      "epoch": 0.5557947473482328,
      "grad_norm": 3.8515279293060303,
      "learning_rate": 4.0739001095976206e-05,
      "loss": 0.9165,
      "step": 10650
    },
    {
      "epoch": 0.5563166203504377,
      "grad_norm": 4.56990909576416,
      "learning_rate": 4.073030287215351e-05,
      "loss": 0.8932,
      "step": 10660
    },
    {
      "epoch": 0.5568384933526427,
      "grad_norm": 4.704471111297607,
      "learning_rate": 4.072160464833081e-05,
      "loss": 0.9917,
      "step": 10670
    },
    {
      "epoch": 0.5573603663548475,
      "grad_norm": 3.991743326187134,
      "learning_rate": 4.0712906424508116e-05,
      "loss": 1.0471,
      "step": 10680
    },
    {
      "epoch": 0.5578822393570525,
      "grad_norm": 3.5420029163360596,
      "learning_rate": 4.070420820068542e-05,
      "loss": 0.9195,
      "step": 10690
    },
    {
      "epoch": 0.5584041123592574,
      "grad_norm": 4.32249116897583,
      "learning_rate": 4.069550997686273e-05,
      "loss": 0.9515,
      "step": 10700
    },
    {
      "epoch": 0.5589259853614623,
      "grad_norm": 4.051712989807129,
      "learning_rate": 4.0686811753040034e-05,
      "loss": 0.9781,
      "step": 10710
    },
    {
      "epoch": 0.5594478583636672,
      "grad_norm": 5.273535251617432,
      "learning_rate": 4.067811352921734e-05,
      "loss": 1.1587,
      "step": 10720
    },
    {
      "epoch": 0.5599697313658721,
      "grad_norm": 4.000601768493652,
      "learning_rate": 4.066941530539464e-05,
      "loss": 0.9891,
      "step": 10730
    },
    {
      "epoch": 0.560491604368077,
      "grad_norm": 4.19407320022583,
      "learning_rate": 4.0660717081571944e-05,
      "loss": 0.923,
      "step": 10740
    },
    {
      "epoch": 0.561013477370282,
      "grad_norm": 5.169534683227539,
      "learning_rate": 4.0652018857749255e-05,
      "loss": 1.016,
      "step": 10750
    },
    {
      "epoch": 0.5615353503724868,
      "grad_norm": 3.7653145790100098,
      "learning_rate": 4.064332063392655e-05,
      "loss": 1.0601,
      "step": 10760
    },
    {
      "epoch": 0.5620572233746918,
      "grad_norm": 4.327469348907471,
      "learning_rate": 4.0634622410103855e-05,
      "loss": 0.9657,
      "step": 10770
    },
    {
      "epoch": 0.5625790963768967,
      "grad_norm": 4.840678691864014,
      "learning_rate": 4.062592418628116e-05,
      "loss": 1.0253,
      "step": 10780
    },
    {
      "epoch": 0.5631009693791016,
      "grad_norm": 5.071507930755615,
      "learning_rate": 4.061722596245847e-05,
      "loss": 0.9814,
      "step": 10790
    },
    {
      "epoch": 0.5636228423813066,
      "grad_norm": 3.8861172199249268,
      "learning_rate": 4.060852773863577e-05,
      "loss": 1.0802,
      "step": 10800
    },
    {
      "epoch": 0.5641447153835114,
      "grad_norm": 5.055481910705566,
      "learning_rate": 4.0599829514813076e-05,
      "loss": 1.031,
      "step": 10810
    },
    {
      "epoch": 0.5646665883857164,
      "grad_norm": 5.1255998611450195,
      "learning_rate": 4.059113129099038e-05,
      "loss": 1.0224,
      "step": 10820
    },
    {
      "epoch": 0.5651884613879212,
      "grad_norm": 4.3771586418151855,
      "learning_rate": 4.058243306716768e-05,
      "loss": 1.0314,
      "step": 10830
    },
    {
      "epoch": 0.5657103343901262,
      "grad_norm": 4.437894344329834,
      "learning_rate": 4.0573734843344993e-05,
      "loss": 1.0212,
      "step": 10840
    },
    {
      "epoch": 0.566232207392331,
      "grad_norm": 5.118402004241943,
      "learning_rate": 4.05650366195223e-05,
      "loss": 1.05,
      "step": 10850
    },
    {
      "epoch": 0.566754080394536,
      "grad_norm": 4.19305944442749,
      "learning_rate": 4.05563383956996e-05,
      "loss": 1.014,
      "step": 10860
    },
    {
      "epoch": 0.5672759533967409,
      "grad_norm": 5.052253246307373,
      "learning_rate": 4.0547640171876904e-05,
      "loss": 0.9782,
      "step": 10870
    },
    {
      "epoch": 0.5677978263989458,
      "grad_norm": 4.568429946899414,
      "learning_rate": 4.053894194805421e-05,
      "loss": 0.9602,
      "step": 10880
    },
    {
      "epoch": 0.5683196994011507,
      "grad_norm": 3.875739097595215,
      "learning_rate": 4.053024372423152e-05,
      "loss": 1.1012,
      "step": 10890
    },
    {
      "epoch": 0.5688415724033556,
      "grad_norm": 4.014747142791748,
      "learning_rate": 4.052154550040882e-05,
      "loss": 0.9531,
      "step": 10900
    },
    {
      "epoch": 0.5693634454055606,
      "grad_norm": 4.2434258460998535,
      "learning_rate": 4.0512847276586125e-05,
      "loss": 1.0482,
      "step": 10910
    },
    {
      "epoch": 0.5698853184077655,
      "grad_norm": 4.5898003578186035,
      "learning_rate": 4.050414905276343e-05,
      "loss": 0.9419,
      "step": 10920
    },
    {
      "epoch": 0.5704071914099704,
      "grad_norm": 4.715969085693359,
      "learning_rate": 4.049545082894073e-05,
      "loss": 1.0943,
      "step": 10930
    },
    {
      "epoch": 0.5709290644121753,
      "grad_norm": 4.484740257263184,
      "learning_rate": 4.0486752605118036e-05,
      "loss": 1.0356,
      "step": 10940
    },
    {
      "epoch": 0.5714509374143802,
      "grad_norm": 3.5987343788146973,
      "learning_rate": 4.0478054381295346e-05,
      "loss": 0.9088,
      "step": 10950
    },
    {
      "epoch": 0.5719728104165851,
      "grad_norm": 4.797375202178955,
      "learning_rate": 4.046935615747265e-05,
      "loss": 1.0563,
      "step": 10960
    },
    {
      "epoch": 0.5724946834187901,
      "grad_norm": 3.947686195373535,
      "learning_rate": 4.0460657933649946e-05,
      "loss": 0.948,
      "step": 10970
    },
    {
      "epoch": 0.5730165564209949,
      "grad_norm": 4.60366153717041,
      "learning_rate": 4.045195970982725e-05,
      "loss": 0.8959,
      "step": 10980
    },
    {
      "epoch": 0.5735384294231999,
      "grad_norm": 4.443986892700195,
      "learning_rate": 4.044326148600456e-05,
      "loss": 1.0094,
      "step": 10990
    },
    {
      "epoch": 0.5740603024254047,
      "grad_norm": 4.214719772338867,
      "learning_rate": 4.0434563262181864e-05,
      "loss": 0.9421,
      "step": 11000
    },
    {
      "epoch": 0.5745821754276097,
      "grad_norm": 3.9370920658111572,
      "learning_rate": 4.042586503835917e-05,
      "loss": 1.0512,
      "step": 11010
    },
    {
      "epoch": 0.5751040484298146,
      "grad_norm": 4.998509407043457,
      "learning_rate": 4.041716681453647e-05,
      "loss": 1.0162,
      "step": 11020
    },
    {
      "epoch": 0.5756259214320195,
      "grad_norm": 4.359465599060059,
      "learning_rate": 4.0408468590713774e-05,
      "loss": 1.0123,
      "step": 11030
    },
    {
      "epoch": 0.5761477944342245,
      "grad_norm": 3.8845162391662598,
      "learning_rate": 4.0399770366891085e-05,
      "loss": 1.0325,
      "step": 11040
    },
    {
      "epoch": 0.5766696674364293,
      "grad_norm": 3.7744433879852295,
      "learning_rate": 4.039107214306839e-05,
      "loss": 0.9908,
      "step": 11050
    },
    {
      "epoch": 0.5771915404386343,
      "grad_norm": 4.761927604675293,
      "learning_rate": 4.038237391924569e-05,
      "loss": 1.0897,
      "step": 11060
    },
    {
      "epoch": 0.5777134134408392,
      "grad_norm": 3.457737445831299,
      "learning_rate": 4.0373675695422995e-05,
      "loss": 0.9241,
      "step": 11070
    },
    {
      "epoch": 0.5782352864430441,
      "grad_norm": 4.8096604347229,
      "learning_rate": 4.03649774716003e-05,
      "loss": 1.027,
      "step": 11080
    },
    {
      "epoch": 0.578757159445249,
      "grad_norm": 4.72380256652832,
      "learning_rate": 4.035627924777761e-05,
      "loss": 0.9761,
      "step": 11090
    },
    {
      "epoch": 0.5792790324474539,
      "grad_norm": 4.695131778717041,
      "learning_rate": 4.034758102395491e-05,
      "loss": 1.0034,
      "step": 11100
    },
    {
      "epoch": 0.5798009054496588,
      "grad_norm": 4.257671356201172,
      "learning_rate": 4.0338882800132216e-05,
      "loss": 1.0188,
      "step": 11110
    },
    {
      "epoch": 0.5803227784518638,
      "grad_norm": 4.514563083648682,
      "learning_rate": 4.033018457630952e-05,
      "loss": 0.8381,
      "step": 11120
    },
    {
      "epoch": 0.5808446514540686,
      "grad_norm": 3.9804112911224365,
      "learning_rate": 4.0321486352486823e-05,
      "loss": 0.9738,
      "step": 11130
    },
    {
      "epoch": 0.5813665244562736,
      "grad_norm": 3.9819607734680176,
      "learning_rate": 4.0312788128664134e-05,
      "loss": 1.0321,
      "step": 11140
    },
    {
      "epoch": 0.5818883974584785,
      "grad_norm": 4.549758434295654,
      "learning_rate": 4.030408990484144e-05,
      "loss": 0.8814,
      "step": 11150
    },
    {
      "epoch": 0.5824102704606834,
      "grad_norm": 4.381561756134033,
      "learning_rate": 4.029539168101874e-05,
      "loss": 1.0571,
      "step": 11160
    },
    {
      "epoch": 0.5829321434628884,
      "grad_norm": 4.317471981048584,
      "learning_rate": 4.0286693457196044e-05,
      "loss": 0.931,
      "step": 11170
    },
    {
      "epoch": 0.5834540164650932,
      "grad_norm": 4.904918670654297,
      "learning_rate": 4.027799523337334e-05,
      "loss": 1.0759,
      "step": 11180
    },
    {
      "epoch": 0.5839758894672982,
      "grad_norm": 4.478842258453369,
      "learning_rate": 4.026929700955065e-05,
      "loss": 1.0152,
      "step": 11190
    },
    {
      "epoch": 0.584497762469503,
      "grad_norm": 3.5011329650878906,
      "learning_rate": 4.0260598785727955e-05,
      "loss": 0.9929,
      "step": 11200
    },
    {
      "epoch": 0.585019635471708,
      "grad_norm": 4.654521465301514,
      "learning_rate": 4.025190056190526e-05,
      "loss": 0.9927,
      "step": 11210
    },
    {
      "epoch": 0.5855415084739128,
      "grad_norm": 4.295360088348389,
      "learning_rate": 4.024320233808256e-05,
      "loss": 1.0293,
      "step": 11220
    },
    {
      "epoch": 0.5860633814761178,
      "grad_norm": 5.429722309112549,
      "learning_rate": 4.0234504114259866e-05,
      "loss": 1.0236,
      "step": 11230
    },
    {
      "epoch": 0.5865852544783227,
      "grad_norm": 4.690036773681641,
      "learning_rate": 4.0225805890437176e-05,
      "loss": 0.9763,
      "step": 11240
    },
    {
      "epoch": 0.5871071274805276,
      "grad_norm": 4.184182643890381,
      "learning_rate": 4.021710766661448e-05,
      "loss": 0.9751,
      "step": 11250
    },
    {
      "epoch": 0.5876290004827325,
      "grad_norm": 4.477751731872559,
      "learning_rate": 4.020840944279178e-05,
      "loss": 0.9124,
      "step": 11260
    },
    {
      "epoch": 0.5881508734849374,
      "grad_norm": 3.8231523036956787,
      "learning_rate": 4.019971121896909e-05,
      "loss": 0.9667,
      "step": 11270
    },
    {
      "epoch": 0.5886727464871424,
      "grad_norm": 4.365375518798828,
      "learning_rate": 4.019101299514639e-05,
      "loss": 0.9684,
      "step": 11280
    },
    {
      "epoch": 0.5891946194893473,
      "grad_norm": 4.130322456359863,
      "learning_rate": 4.01823147713237e-05,
      "loss": 0.9619,
      "step": 11290
    },
    {
      "epoch": 0.5897164924915522,
      "grad_norm": 3.8166308403015137,
      "learning_rate": 4.0173616547501004e-05,
      "loss": 1.0911,
      "step": 11300
    },
    {
      "epoch": 0.5902383654937571,
      "grad_norm": 5.842079162597656,
      "learning_rate": 4.016491832367831e-05,
      "loss": 1.0211,
      "step": 11310
    },
    {
      "epoch": 0.590760238495962,
      "grad_norm": 4.845814228057861,
      "learning_rate": 4.015622009985561e-05,
      "loss": 1.0036,
      "step": 11320
    },
    {
      "epoch": 0.5912821114981669,
      "grad_norm": 3.935382604598999,
      "learning_rate": 4.0147521876032915e-05,
      "loss": 1.0052,
      "step": 11330
    },
    {
      "epoch": 0.5918039845003719,
      "grad_norm": 4.760530471801758,
      "learning_rate": 4.0138823652210225e-05,
      "loss": 0.9821,
      "step": 11340
    },
    {
      "epoch": 0.5923258575025767,
      "grad_norm": 4.554553985595703,
      "learning_rate": 4.013012542838753e-05,
      "loss": 0.9622,
      "step": 11350
    },
    {
      "epoch": 0.5928477305047817,
      "grad_norm": 4.146503925323486,
      "learning_rate": 4.012142720456483e-05,
      "loss": 0.9833,
      "step": 11360
    },
    {
      "epoch": 0.5933696035069865,
      "grad_norm": 4.3987016677856445,
      "learning_rate": 4.0112728980742136e-05,
      "loss": 0.9558,
      "step": 11370
    },
    {
      "epoch": 0.5938914765091915,
      "grad_norm": 5.497679710388184,
      "learning_rate": 4.010403075691944e-05,
      "loss": 0.922,
      "step": 11380
    },
    {
      "epoch": 0.5944133495113963,
      "grad_norm": 4.935169696807861,
      "learning_rate": 4.009533253309674e-05,
      "loss": 1.0487,
      "step": 11390
    },
    {
      "epoch": 0.5949352225136013,
      "grad_norm": 4.821183204650879,
      "learning_rate": 4.0086634309274046e-05,
      "loss": 1.1051,
      "step": 11400
    },
    {
      "epoch": 0.5954570955158063,
      "grad_norm": 3.9957244396209717,
      "learning_rate": 4.007793608545135e-05,
      "loss": 0.9795,
      "step": 11410
    },
    {
      "epoch": 0.5959789685180111,
      "grad_norm": 4.848552227020264,
      "learning_rate": 4.0069237861628653e-05,
      "loss": 0.9598,
      "step": 11420
    },
    {
      "epoch": 0.5965008415202161,
      "grad_norm": 3.9134862422943115,
      "learning_rate": 4.0060539637805964e-05,
      "loss": 0.9341,
      "step": 11430
    },
    {
      "epoch": 0.597022714522421,
      "grad_norm": 4.148496150970459,
      "learning_rate": 4.005184141398327e-05,
      "loss": 1.0507,
      "step": 11440
    },
    {
      "epoch": 0.5975445875246259,
      "grad_norm": 4.549783706665039,
      "learning_rate": 4.004314319016057e-05,
      "loss": 1.0708,
      "step": 11450
    },
    {
      "epoch": 0.5980664605268308,
      "grad_norm": 5.003446578979492,
      "learning_rate": 4.0034444966337874e-05,
      "loss": 1.0345,
      "step": 11460
    },
    {
      "epoch": 0.5985883335290357,
      "grad_norm": 4.958085060119629,
      "learning_rate": 4.002574674251518e-05,
      "loss": 0.9132,
      "step": 11470
    },
    {
      "epoch": 0.5991102065312406,
      "grad_norm": 3.6387805938720703,
      "learning_rate": 4.001704851869248e-05,
      "loss": 0.9756,
      "step": 11480
    },
    {
      "epoch": 0.5996320795334456,
      "grad_norm": 4.433182239532471,
      "learning_rate": 4.000835029486979e-05,
      "loss": 1.0222,
      "step": 11490
    },
    {
      "epoch": 0.6001539525356504,
      "grad_norm": 4.7682976722717285,
      "learning_rate": 3.9999652071047095e-05,
      "loss": 1.0964,
      "step": 11500
    },
    {
      "epoch": 0.6006758255378554,
      "grad_norm": 3.9777560234069824,
      "learning_rate": 3.99909538472244e-05,
      "loss": 0.9866,
      "step": 11510
    },
    {
      "epoch": 0.6011976985400602,
      "grad_norm": 4.819380760192871,
      "learning_rate": 3.99822556234017e-05,
      "loss": 1.04,
      "step": 11520
    },
    {
      "epoch": 0.6017195715422652,
      "grad_norm": 4.462050914764404,
      "learning_rate": 3.9973557399579006e-05,
      "loss": 1.1114,
      "step": 11530
    },
    {
      "epoch": 0.6022414445444702,
      "grad_norm": 4.1053361892700195,
      "learning_rate": 3.9964859175756316e-05,
      "loss": 0.9563,
      "step": 11540
    },
    {
      "epoch": 0.602763317546675,
      "grad_norm": 4.889771938323975,
      "learning_rate": 3.995616095193362e-05,
      "loss": 0.9485,
      "step": 11550
    },
    {
      "epoch": 0.60328519054888,
      "grad_norm": 4.836592197418213,
      "learning_rate": 3.9947462728110923e-05,
      "loss": 1.0386,
      "step": 11560
    },
    {
      "epoch": 0.6038070635510848,
      "grad_norm": 4.315587997436523,
      "learning_rate": 3.993876450428823e-05,
      "loss": 1.0122,
      "step": 11570
    },
    {
      "epoch": 0.6043289365532898,
      "grad_norm": 4.143431186676025,
      "learning_rate": 3.993006628046553e-05,
      "loss": 0.9857,
      "step": 11580
    },
    {
      "epoch": 0.6048508095554946,
      "grad_norm": 4.827326774597168,
      "learning_rate": 3.9921368056642834e-05,
      "loss": 0.9774,
      "step": 11590
    },
    {
      "epoch": 0.6053726825576996,
      "grad_norm": 5.081207752227783,
      "learning_rate": 3.991266983282014e-05,
      "loss": 1.0221,
      "step": 11600
    },
    {
      "epoch": 0.6058945555599045,
      "grad_norm": 4.90275764465332,
      "learning_rate": 3.990397160899744e-05,
      "loss": 1.0266,
      "step": 11610
    },
    {
      "epoch": 0.6064164285621094,
      "grad_norm": 4.875006198883057,
      "learning_rate": 3.9895273385174745e-05,
      "loss": 1.0102,
      "step": 11620
    },
    {
      "epoch": 0.6069383015643143,
      "grad_norm": 3.557157516479492,
      "learning_rate": 3.9886575161352055e-05,
      "loss": 0.9791,
      "step": 11630
    },
    {
      "epoch": 0.6074601745665192,
      "grad_norm": 3.737837314605713,
      "learning_rate": 3.987787693752936e-05,
      "loss": 1.0486,
      "step": 11640
    },
    {
      "epoch": 0.6079820475687242,
      "grad_norm": 4.541733264923096,
      "learning_rate": 3.986917871370666e-05,
      "loss": 1.1017,
      "step": 11650
    },
    {
      "epoch": 0.6085039205709291,
      "grad_norm": 5.492738723754883,
      "learning_rate": 3.9860480489883966e-05,
      "loss": 1.0928,
      "step": 11660
    },
    {
      "epoch": 0.609025793573134,
      "grad_norm": 4.425593852996826,
      "learning_rate": 3.985178226606127e-05,
      "loss": 0.8951,
      "step": 11670
    },
    {
      "epoch": 0.6095476665753389,
      "grad_norm": 4.140890121459961,
      "learning_rate": 3.984308404223858e-05,
      "loss": 0.8945,
      "step": 11680
    },
    {
      "epoch": 0.6100695395775438,
      "grad_norm": 4.237869739532471,
      "learning_rate": 3.983438581841588e-05,
      "loss": 1.086,
      "step": 11690
    },
    {
      "epoch": 0.6105914125797487,
      "grad_norm": 4.32716178894043,
      "learning_rate": 3.982568759459319e-05,
      "loss": 1.0231,
      "step": 11700
    },
    {
      "epoch": 0.6111132855819537,
      "grad_norm": 4.685368537902832,
      "learning_rate": 3.981698937077049e-05,
      "loss": 1.0374,
      "step": 11710
    },
    {
      "epoch": 0.6116351585841585,
      "grad_norm": 4.358645915985107,
      "learning_rate": 3.9808291146947794e-05,
      "loss": 1.09,
      "step": 11720
    },
    {
      "epoch": 0.6121570315863635,
      "grad_norm": 5.220900058746338,
      "learning_rate": 3.9799592923125104e-05,
      "loss": 0.9744,
      "step": 11730
    },
    {
      "epoch": 0.6126789045885683,
      "grad_norm": 4.480169296264648,
      "learning_rate": 3.979089469930241e-05,
      "loss": 0.9789,
      "step": 11740
    },
    {
      "epoch": 0.6132007775907733,
      "grad_norm": 4.557882785797119,
      "learning_rate": 3.978219647547971e-05,
      "loss": 1.0628,
      "step": 11750
    },
    {
      "epoch": 0.6137226505929781,
      "grad_norm": 3.9843828678131104,
      "learning_rate": 3.9773498251657015e-05,
      "loss": 0.8896,
      "step": 11760
    },
    {
      "epoch": 0.6142445235951831,
      "grad_norm": 4.344514846801758,
      "learning_rate": 3.976480002783432e-05,
      "loss": 1.0015,
      "step": 11770
    },
    {
      "epoch": 0.6147663965973881,
      "grad_norm": 4.433228969573975,
      "learning_rate": 3.975610180401162e-05,
      "loss": 0.9783,
      "step": 11780
    },
    {
      "epoch": 0.6152882695995929,
      "grad_norm": 3.8545479774475098,
      "learning_rate": 3.974740358018893e-05,
      "loss": 0.9697,
      "step": 11790
    },
    {
      "epoch": 0.6158101426017979,
      "grad_norm": 4.195274829864502,
      "learning_rate": 3.973870535636623e-05,
      "loss": 0.9274,
      "step": 11800
    },
    {
      "epoch": 0.6163320156040027,
      "grad_norm": 3.7905001640319824,
      "learning_rate": 3.973000713254353e-05,
      "loss": 0.9634,
      "step": 11810
    },
    {
      "epoch": 0.6168538886062077,
      "grad_norm": 4.009769439697266,
      "learning_rate": 3.9721308908720836e-05,
      "loss": 0.9075,
      "step": 11820
    },
    {
      "epoch": 0.6173757616084126,
      "grad_norm": 2.9726641178131104,
      "learning_rate": 3.9712610684898146e-05,
      "loss": 0.963,
      "step": 11830
    },
    {
      "epoch": 0.6178976346106175,
      "grad_norm": 4.962761402130127,
      "learning_rate": 3.970391246107545e-05,
      "loss": 0.9598,
      "step": 11840
    },
    {
      "epoch": 0.6184195076128224,
      "grad_norm": 3.420971632003784,
      "learning_rate": 3.9695214237252754e-05,
      "loss": 1.0324,
      "step": 11850
    },
    {
      "epoch": 0.6189413806150273,
      "grad_norm": 4.390898704528809,
      "learning_rate": 3.968651601343006e-05,
      "loss": 1.0277,
      "step": 11860
    },
    {
      "epoch": 0.6194632536172322,
      "grad_norm": 4.094369411468506,
      "learning_rate": 3.967781778960736e-05,
      "loss": 0.9264,
      "step": 11870
    },
    {
      "epoch": 0.6199851266194372,
      "grad_norm": 4.455066680908203,
      "learning_rate": 3.966911956578467e-05,
      "loss": 0.9744,
      "step": 11880
    },
    {
      "epoch": 0.620506999621642,
      "grad_norm": 3.783024787902832,
      "learning_rate": 3.9660421341961974e-05,
      "loss": 0.9713,
      "step": 11890
    },
    {
      "epoch": 0.621028872623847,
      "grad_norm": 4.557201385498047,
      "learning_rate": 3.965172311813928e-05,
      "loss": 0.9859,
      "step": 11900
    },
    {
      "epoch": 0.621550745626052,
      "grad_norm": 4.587818622589111,
      "learning_rate": 3.964302489431658e-05,
      "loss": 1.003,
      "step": 11910
    },
    {
      "epoch": 0.6220726186282568,
      "grad_norm": 4.449170112609863,
      "learning_rate": 3.9634326670493885e-05,
      "loss": 1.0339,
      "step": 11920
    },
    {
      "epoch": 0.6225944916304618,
      "grad_norm": 4.389455318450928,
      "learning_rate": 3.9625628446671195e-05,
      "loss": 1.0228,
      "step": 11930
    },
    {
      "epoch": 0.6231163646326666,
      "grad_norm": 4.3817572593688965,
      "learning_rate": 3.96169302228485e-05,
      "loss": 0.8934,
      "step": 11940
    },
    {
      "epoch": 0.6236382376348716,
      "grad_norm": 4.684286117553711,
      "learning_rate": 3.96082319990258e-05,
      "loss": 0.9103,
      "step": 11950
    },
    {
      "epoch": 0.6241601106370764,
      "grad_norm": 4.665548324584961,
      "learning_rate": 3.9599533775203106e-05,
      "loss": 0.9195,
      "step": 11960
    },
    {
      "epoch": 0.6246819836392814,
      "grad_norm": 3.372640371322632,
      "learning_rate": 3.959083555138041e-05,
      "loss": 0.9235,
      "step": 11970
    },
    {
      "epoch": 0.6252038566414863,
      "grad_norm": 5.039543628692627,
      "learning_rate": 3.958213732755772e-05,
      "loss": 0.9585,
      "step": 11980
    },
    {
      "epoch": 0.6257257296436912,
      "grad_norm": 5.5469865798950195,
      "learning_rate": 3.9573439103735024e-05,
      "loss": 0.9106,
      "step": 11990
    },
    {
      "epoch": 0.6262476026458961,
      "grad_norm": 3.340111494064331,
      "learning_rate": 3.956474087991233e-05,
      "loss": 1.0282,
      "step": 12000
    },
    {
      "epoch": 0.626769475648101,
      "grad_norm": 4.964080333709717,
      "learning_rate": 3.9556042656089624e-05,
      "loss": 1.0363,
      "step": 12010
    },
    {
      "epoch": 0.627291348650306,
      "grad_norm": 4.728692054748535,
      "learning_rate": 3.954734443226693e-05,
      "loss": 0.9177,
      "step": 12020
    },
    {
      "epoch": 0.6278132216525109,
      "grad_norm": 3.9858591556549072,
      "learning_rate": 3.953864620844424e-05,
      "loss": 1.0167,
      "step": 12030
    },
    {
      "epoch": 0.6283350946547158,
      "grad_norm": 4.071171283721924,
      "learning_rate": 3.952994798462154e-05,
      "loss": 0.9866,
      "step": 12040
    },
    {
      "epoch": 0.6288569676569207,
      "grad_norm": 4.088650226593018,
      "learning_rate": 3.9521249760798845e-05,
      "loss": 1.0217,
      "step": 12050
    },
    {
      "epoch": 0.6293788406591256,
      "grad_norm": 4.605535984039307,
      "learning_rate": 3.951255153697615e-05,
      "loss": 1.0067,
      "step": 12060
    },
    {
      "epoch": 0.6299007136613305,
      "grad_norm": 4.192454814910889,
      "learning_rate": 3.950385331315345e-05,
      "loss": 1.1144,
      "step": 12070
    },
    {
      "epoch": 0.6304225866635355,
      "grad_norm": 4.766256809234619,
      "learning_rate": 3.949515508933076e-05,
      "loss": 0.9887,
      "step": 12080
    },
    {
      "epoch": 0.6309444596657403,
      "grad_norm": 4.502245903015137,
      "learning_rate": 3.9486456865508066e-05,
      "loss": 0.974,
      "step": 12090
    },
    {
      "epoch": 0.6314663326679453,
      "grad_norm": 4.316849231719971,
      "learning_rate": 3.947775864168537e-05,
      "loss": 0.976,
      "step": 12100
    },
    {
      "epoch": 0.6319882056701501,
      "grad_norm": 4.6031270027160645,
      "learning_rate": 3.946906041786267e-05,
      "loss": 0.9215,
      "step": 12110
    },
    {
      "epoch": 0.6325100786723551,
      "grad_norm": 4.274054050445557,
      "learning_rate": 3.9460362194039976e-05,
      "loss": 0.878,
      "step": 12120
    },
    {
      "epoch": 0.63303195167456,
      "grad_norm": 3.3125252723693848,
      "learning_rate": 3.945166397021729e-05,
      "loss": 1.0005,
      "step": 12130
    },
    {
      "epoch": 0.6335538246767649,
      "grad_norm": 4.352528095245361,
      "learning_rate": 3.944296574639459e-05,
      "loss": 1.1328,
      "step": 12140
    },
    {
      "epoch": 0.6340756976789699,
      "grad_norm": 3.449298143386841,
      "learning_rate": 3.9434267522571894e-05,
      "loss": 0.9823,
      "step": 12150
    },
    {
      "epoch": 0.6345975706811747,
      "grad_norm": 4.952544689178467,
      "learning_rate": 3.94255692987492e-05,
      "loss": 0.9411,
      "step": 12160
    },
    {
      "epoch": 0.6351194436833797,
      "grad_norm": 3.595538854598999,
      "learning_rate": 3.94168710749265e-05,
      "loss": 1.0639,
      "step": 12170
    },
    {
      "epoch": 0.6356413166855845,
      "grad_norm": 3.5141794681549072,
      "learning_rate": 3.940817285110381e-05,
      "loss": 1.0109,
      "step": 12180
    },
    {
      "epoch": 0.6361631896877895,
      "grad_norm": 5.102491855621338,
      "learning_rate": 3.940034444966338e-05,
      "loss": 1.1627,
      "step": 12190
    },
    {
      "epoch": 0.6366850626899944,
      "grad_norm": 4.549642562866211,
      "learning_rate": 3.9391646225840685e-05,
      "loss": 1.0385,
      "step": 12200
    },
    {
      "epoch": 0.6372069356921993,
      "grad_norm": 4.015674591064453,
      "learning_rate": 3.938294800201799e-05,
      "loss": 1.023,
      "step": 12210
    },
    {
      "epoch": 0.6377288086944042,
      "grad_norm": 4.219295024871826,
      "learning_rate": 3.937424977819529e-05,
      "loss": 1.0023,
      "step": 12220
    },
    {
      "epoch": 0.6382506816966091,
      "grad_norm": 4.304481029510498,
      "learning_rate": 3.9365551554372595e-05,
      "loss": 0.9294,
      "step": 12230
    },
    {
      "epoch": 0.638772554698814,
      "grad_norm": 4.746326923370361,
      "learning_rate": 3.9356853330549906e-05,
      "loss": 1.0029,
      "step": 12240
    },
    {
      "epoch": 0.639294427701019,
      "grad_norm": 4.215249538421631,
      "learning_rate": 3.934815510672721e-05,
      "loss": 0.9052,
      "step": 12250
    },
    {
      "epoch": 0.6398163007032238,
      "grad_norm": 4.3216118812561035,
      "learning_rate": 3.933945688290451e-05,
      "loss": 1.0974,
      "step": 12260
    },
    {
      "epoch": 0.6403381737054288,
      "grad_norm": 5.334471702575684,
      "learning_rate": 3.9330758659081816e-05,
      "loss": 1.1039,
      "step": 12270
    },
    {
      "epoch": 0.6408600467076337,
      "grad_norm": 4.959348201751709,
      "learning_rate": 3.932206043525912e-05,
      "loss": 0.9917,
      "step": 12280
    },
    {
      "epoch": 0.6413819197098386,
      "grad_norm": 4.9935126304626465,
      "learning_rate": 3.931336221143643e-05,
      "loss": 1.0354,
      "step": 12290
    },
    {
      "epoch": 0.6419037927120436,
      "grad_norm": 4.111158847808838,
      "learning_rate": 3.9304663987613734e-05,
      "loss": 0.9393,
      "step": 12300
    },
    {
      "epoch": 0.6424256657142484,
      "grad_norm": 5.6073808670043945,
      "learning_rate": 3.929596576379104e-05,
      "loss": 0.9456,
      "step": 12310
    },
    {
      "epoch": 0.6429475387164534,
      "grad_norm": 4.9971394538879395,
      "learning_rate": 3.928726753996834e-05,
      "loss": 0.9103,
      "step": 12320
    },
    {
      "epoch": 0.6434694117186582,
      "grad_norm": 4.2441816329956055,
      "learning_rate": 3.9278569316145644e-05,
      "loss": 0.9525,
      "step": 12330
    },
    {
      "epoch": 0.6439912847208632,
      "grad_norm": 4.660870552062988,
      "learning_rate": 3.9269871092322955e-05,
      "loss": 1.0211,
      "step": 12340
    },
    {
      "epoch": 0.644513157723068,
      "grad_norm": 4.061165809631348,
      "learning_rate": 3.926117286850026e-05,
      "loss": 0.9691,
      "step": 12350
    },
    {
      "epoch": 0.645035030725273,
      "grad_norm": 4.240479469299316,
      "learning_rate": 3.9252474644677555e-05,
      "loss": 0.9342,
      "step": 12360
    },
    {
      "epoch": 0.6455569037274779,
      "grad_norm": 4.875671863555908,
      "learning_rate": 3.924377642085486e-05,
      "loss": 1.0048,
      "step": 12370
    },
    {
      "epoch": 0.6460787767296828,
      "grad_norm": 4.0457072257995605,
      "learning_rate": 3.923507819703217e-05,
      "loss": 0.9961,
      "step": 12380
    },
    {
      "epoch": 0.6466006497318878,
      "grad_norm": 3.9407012462615967,
      "learning_rate": 3.922637997320947e-05,
      "loss": 0.9859,
      "step": 12390
    },
    {
      "epoch": 0.6471225227340927,
      "grad_norm": 3.7346205711364746,
      "learning_rate": 3.9217681749386776e-05,
      "loss": 0.9965,
      "step": 12400
    },
    {
      "epoch": 0.6476443957362976,
      "grad_norm": 3.949495553970337,
      "learning_rate": 3.920898352556408e-05,
      "loss": 0.9825,
      "step": 12410
    },
    {
      "epoch": 0.6481662687385025,
      "grad_norm": 4.159783840179443,
      "learning_rate": 3.920028530174138e-05,
      "loss": 0.9427,
      "step": 12420
    },
    {
      "epoch": 0.6486881417407074,
      "grad_norm": 4.262495040893555,
      "learning_rate": 3.919158707791869e-05,
      "loss": 0.773,
      "step": 12430
    },
    {
      "epoch": 0.6492100147429123,
      "grad_norm": 4.551809787750244,
      "learning_rate": 3.9182888854096e-05,
      "loss": 0.9609,
      "step": 12440
    },
    {
      "epoch": 0.6497318877451173,
      "grad_norm": 4.802885055541992,
      "learning_rate": 3.91741906302733e-05,
      "loss": 1.0041,
      "step": 12450
    },
    {
      "epoch": 0.6502537607473221,
      "grad_norm": 4.892461776733398,
      "learning_rate": 3.9165492406450604e-05,
      "loss": 1.0761,
      "step": 12460
    },
    {
      "epoch": 0.6507756337495271,
      "grad_norm": 5.188627243041992,
      "learning_rate": 3.915679418262791e-05,
      "loss": 0.9709,
      "step": 12470
    },
    {
      "epoch": 0.6512975067517319,
      "grad_norm": 4.609193801879883,
      "learning_rate": 3.914809595880521e-05,
      "loss": 0.8701,
      "step": 12480
    },
    {
      "epoch": 0.6518193797539369,
      "grad_norm": 3.891709327697754,
      "learning_rate": 3.913939773498252e-05,
      "loss": 0.9441,
      "step": 12490
    },
    {
      "epoch": 0.6523412527561417,
      "grad_norm": 4.825997352600098,
      "learning_rate": 3.9130699511159825e-05,
      "loss": 0.963,
      "step": 12500
    },
    {
      "epoch": 0.6528631257583467,
      "grad_norm": 4.3172831535339355,
      "learning_rate": 3.912200128733713e-05,
      "loss": 1.0546,
      "step": 12510
    },
    {
      "epoch": 0.6533849987605517,
      "grad_norm": 4.156033515930176,
      "learning_rate": 3.911330306351443e-05,
      "loss": 0.9627,
      "step": 12520
    },
    {
      "epoch": 0.6539068717627565,
      "grad_norm": 4.363024711608887,
      "learning_rate": 3.9104604839691736e-05,
      "loss": 0.9665,
      "step": 12530
    },
    {
      "epoch": 0.6544287447649615,
      "grad_norm": 5.255128860473633,
      "learning_rate": 3.9095906615869046e-05,
      "loss": 1.0374,
      "step": 12540
    },
    {
      "epoch": 0.6549506177671663,
      "grad_norm": 5.719875335693359,
      "learning_rate": 3.908720839204635e-05,
      "loss": 0.9385,
      "step": 12550
    },
    {
      "epoch": 0.6554724907693713,
      "grad_norm": 4.262136936187744,
      "learning_rate": 3.907851016822365e-05,
      "loss": 0.9232,
      "step": 12560
    },
    {
      "epoch": 0.6559943637715762,
      "grad_norm": 5.522268772125244,
      "learning_rate": 3.906981194440095e-05,
      "loss": 1.0324,
      "step": 12570
    },
    {
      "epoch": 0.6565162367737811,
      "grad_norm": 4.300605297088623,
      "learning_rate": 3.906111372057826e-05,
      "loss": 1.1734,
      "step": 12580
    },
    {
      "epoch": 0.657038109775986,
      "grad_norm": 4.327541828155518,
      "learning_rate": 3.9052415496755564e-05,
      "loss": 1.0094,
      "step": 12590
    },
    {
      "epoch": 0.6575599827781909,
      "grad_norm": 4.6099138259887695,
      "learning_rate": 3.904371727293287e-05,
      "loss": 0.9895,
      "step": 12600
    },
    {
      "epoch": 0.6580818557803958,
      "grad_norm": 4.282014846801758,
      "learning_rate": 3.903501904911017e-05,
      "loss": 0.9963,
      "step": 12610
    },
    {
      "epoch": 0.6586037287826008,
      "grad_norm": 4.294571876525879,
      "learning_rate": 3.9026320825287474e-05,
      "loss": 1.0419,
      "step": 12620
    },
    {
      "epoch": 0.6591256017848056,
      "grad_norm": 4.072945594787598,
      "learning_rate": 3.9017622601464785e-05,
      "loss": 0.9052,
      "step": 12630
    },
    {
      "epoch": 0.6596474747870106,
      "grad_norm": 4.566275119781494,
      "learning_rate": 3.900892437764209e-05,
      "loss": 0.9742,
      "step": 12640
    },
    {
      "epoch": 0.6601693477892155,
      "grad_norm": 4.43483829498291,
      "learning_rate": 3.900022615381939e-05,
      "loss": 0.9538,
      "step": 12650
    },
    {
      "epoch": 0.6606912207914204,
      "grad_norm": 4.299802780151367,
      "learning_rate": 3.8991527929996695e-05,
      "loss": 0.9707,
      "step": 12660
    },
    {
      "epoch": 0.6612130937936254,
      "grad_norm": 4.617239952087402,
      "learning_rate": 3.8982829706174e-05,
      "loss": 0.9703,
      "step": 12670
    },
    {
      "epoch": 0.6617349667958302,
      "grad_norm": 4.525234699249268,
      "learning_rate": 3.897413148235131e-05,
      "loss": 0.9892,
      "step": 12680
    },
    {
      "epoch": 0.6622568397980352,
      "grad_norm": 4.094823837280273,
      "learning_rate": 3.896543325852861e-05,
      "loss": 0.9895,
      "step": 12690
    },
    {
      "epoch": 0.66277871280024,
      "grad_norm": 3.721665382385254,
      "learning_rate": 3.8956735034705916e-05,
      "loss": 0.9238,
      "step": 12700
    },
    {
      "epoch": 0.663300585802445,
      "grad_norm": 4.062065601348877,
      "learning_rate": 3.894803681088322e-05,
      "loss": 1.0392,
      "step": 12710
    },
    {
      "epoch": 0.6638224588046499,
      "grad_norm": 4.734936237335205,
      "learning_rate": 3.8939338587060523e-05,
      "loss": 1.0343,
      "step": 12720
    },
    {
      "epoch": 0.6643443318068548,
      "grad_norm": 5.782529354095459,
      "learning_rate": 3.893064036323783e-05,
      "loss": 0.9712,
      "step": 12730
    },
    {
      "epoch": 0.6648662048090597,
      "grad_norm": 4.392060279846191,
      "learning_rate": 3.892194213941514e-05,
      "loss": 0.9817,
      "step": 12740
    },
    {
      "epoch": 0.6653880778112646,
      "grad_norm": 3.898172616958618,
      "learning_rate": 3.891324391559244e-05,
      "loss": 0.9955,
      "step": 12750
    },
    {
      "epoch": 0.6659099508134695,
      "grad_norm": 4.638741493225098,
      "learning_rate": 3.8904545691769744e-05,
      "loss": 0.9972,
      "step": 12760
    },
    {
      "epoch": 0.6664318238156745,
      "grad_norm": 4.311978340148926,
      "learning_rate": 3.889584746794705e-05,
      "loss": 1.0744,
      "step": 12770
    },
    {
      "epoch": 0.6669536968178794,
      "grad_norm": 4.579848289489746,
      "learning_rate": 3.888714924412435e-05,
      "loss": 0.9742,
      "step": 12780
    },
    {
      "epoch": 0.6674755698200843,
      "grad_norm": 4.248999118804932,
      "learning_rate": 3.8878451020301655e-05,
      "loss": 0.9741,
      "step": 12790
    },
    {
      "epoch": 0.6679974428222892,
      "grad_norm": 3.563765048980713,
      "learning_rate": 3.886975279647896e-05,
      "loss": 0.9922,
      "step": 12800
    },
    {
      "epoch": 0.6685193158244941,
      "grad_norm": 3.566601037979126,
      "learning_rate": 3.886105457265626e-05,
      "loss": 0.9887,
      "step": 12810
    },
    {
      "epoch": 0.669041188826699,
      "grad_norm": 4.227099418640137,
      "learning_rate": 3.8852356348833566e-05,
      "loss": 0.9562,
      "step": 12820
    },
    {
      "epoch": 0.6695630618289039,
      "grad_norm": 4.714145660400391,
      "learning_rate": 3.8843658125010876e-05,
      "loss": 0.9603,
      "step": 12830
    },
    {
      "epoch": 0.6700849348311089,
      "grad_norm": 5.024501800537109,
      "learning_rate": 3.883495990118818e-05,
      "loss": 0.9706,
      "step": 12840
    },
    {
      "epoch": 0.6706068078333137,
      "grad_norm": 4.276447296142578,
      "learning_rate": 3.882626167736548e-05,
      "loss": 0.977,
      "step": 12850
    },
    {
      "epoch": 0.6711286808355187,
      "grad_norm": 4.4123711585998535,
      "learning_rate": 3.881756345354279e-05,
      "loss": 1.0445,
      "step": 12860
    },
    {
      "epoch": 0.6716505538377235,
      "grad_norm": 4.781631946563721,
      "learning_rate": 3.880886522972009e-05,
      "loss": 1.0382,
      "step": 12870
    },
    {
      "epoch": 0.6721724268399285,
      "grad_norm": 4.226989269256592,
      "learning_rate": 3.88001670058974e-05,
      "loss": 0.8929,
      "step": 12880
    },
    {
      "epoch": 0.6726942998421335,
      "grad_norm": 4.750431060791016,
      "learning_rate": 3.8791468782074704e-05,
      "loss": 0.977,
      "step": 12890
    },
    {
      "epoch": 0.6732161728443383,
      "grad_norm": 4.610230445861816,
      "learning_rate": 3.878277055825201e-05,
      "loss": 1.147,
      "step": 12900
    },
    {
      "epoch": 0.6737380458465433,
      "grad_norm": 4.568647861480713,
      "learning_rate": 3.877407233442931e-05,
      "loss": 0.9692,
      "step": 12910
    },
    {
      "epoch": 0.6742599188487481,
      "grad_norm": 4.221235275268555,
      "learning_rate": 3.8765374110606615e-05,
      "loss": 0.975,
      "step": 12920
    },
    {
      "epoch": 0.6747817918509531,
      "grad_norm": 4.548746585845947,
      "learning_rate": 3.8756675886783925e-05,
      "loss": 1.0648,
      "step": 12930
    },
    {
      "epoch": 0.675303664853158,
      "grad_norm": 4.710885047912598,
      "learning_rate": 3.874797766296123e-05,
      "loss": 0.97,
      "step": 12940
    },
    {
      "epoch": 0.6758255378553629,
      "grad_norm": 4.133819103240967,
      "learning_rate": 3.873927943913853e-05,
      "loss": 1.0658,
      "step": 12950
    },
    {
      "epoch": 0.6763474108575678,
      "grad_norm": 5.008528709411621,
      "learning_rate": 3.8730581215315836e-05,
      "loss": 0.9875,
      "step": 12960
    },
    {
      "epoch": 0.6768692838597727,
      "grad_norm": 3.9200973510742188,
      "learning_rate": 3.872188299149314e-05,
      "loss": 0.967,
      "step": 12970
    },
    {
      "epoch": 0.6773911568619776,
      "grad_norm": 4.982259273529053,
      "learning_rate": 3.871318476767045e-05,
      "loss": 0.9444,
      "step": 12980
    },
    {
      "epoch": 0.6779130298641826,
      "grad_norm": 3.5277483463287354,
      "learning_rate": 3.8704486543847746e-05,
      "loss": 0.9937,
      "step": 12990
    },
    {
      "epoch": 0.6784349028663874,
      "grad_norm": 3.622974157333374,
      "learning_rate": 3.869578832002505e-05,
      "loss": 1.0224,
      "step": 13000
    },
    {
      "epoch": 0.6789567758685924,
      "grad_norm": 4.5521111488342285,
      "learning_rate": 3.8687090096202353e-05,
      "loss": 0.9513,
      "step": 13010
    },
    {
      "epoch": 0.6794786488707973,
      "grad_norm": 3.777340888977051,
      "learning_rate": 3.867839187237966e-05,
      "loss": 0.979,
      "step": 13020
    },
    {
      "epoch": 0.6800005218730022,
      "grad_norm": 4.723694324493408,
      "learning_rate": 3.866969364855697e-05,
      "loss": 1.0566,
      "step": 13030
    },
    {
      "epoch": 0.6805223948752072,
      "grad_norm": 3.9492948055267334,
      "learning_rate": 3.866099542473427e-05,
      "loss": 1.0569,
      "step": 13040
    },
    {
      "epoch": 0.681044267877412,
      "grad_norm": 4.671494007110596,
      "learning_rate": 3.8652297200911574e-05,
      "loss": 1.009,
      "step": 13050
    },
    {
      "epoch": 0.681566140879617,
      "grad_norm": 4.743765354156494,
      "learning_rate": 3.864359897708888e-05,
      "loss": 0.9761,
      "step": 13060
    },
    {
      "epoch": 0.6820880138818218,
      "grad_norm": 4.777174949645996,
      "learning_rate": 3.863490075326618e-05,
      "loss": 0.9472,
      "step": 13070
    },
    {
      "epoch": 0.6826098868840268,
      "grad_norm": 4.113392353057861,
      "learning_rate": 3.862620252944349e-05,
      "loss": 1.0157,
      "step": 13080
    },
    {
      "epoch": 0.6831317598862316,
      "grad_norm": 5.161752223968506,
      "learning_rate": 3.8617504305620795e-05,
      "loss": 0.9948,
      "step": 13090
    },
    {
      "epoch": 0.6836536328884366,
      "grad_norm": 3.9017648696899414,
      "learning_rate": 3.86088060817981e-05,
      "loss": 0.9408,
      "step": 13100
    },
    {
      "epoch": 0.6841755058906415,
      "grad_norm": 4.550145626068115,
      "learning_rate": 3.86001078579754e-05,
      "loss": 1.0295,
      "step": 13110
    },
    {
      "epoch": 0.6846973788928464,
      "grad_norm": 4.491450786590576,
      "learning_rate": 3.8591409634152706e-05,
      "loss": 1.0349,
      "step": 13120
    },
    {
      "epoch": 0.6852192518950513,
      "grad_norm": 4.35959529876709,
      "learning_rate": 3.8582711410330016e-05,
      "loss": 0.9452,
      "step": 13130
    },
    {
      "epoch": 0.6857411248972562,
      "grad_norm": 4.05695104598999,
      "learning_rate": 3.857401318650732e-05,
      "loss": 1.0685,
      "step": 13140
    },
    {
      "epoch": 0.6862629978994612,
      "grad_norm": 3.673384428024292,
      "learning_rate": 3.8565314962684623e-05,
      "loss": 1.0683,
      "step": 13150
    },
    {
      "epoch": 0.6867848709016661,
      "grad_norm": 4.414787292480469,
      "learning_rate": 3.855661673886193e-05,
      "loss": 0.9662,
      "step": 13160
    },
    {
      "epoch": 0.687306743903871,
      "grad_norm": 4.448155403137207,
      "learning_rate": 3.854791851503923e-05,
      "loss": 0.9489,
      "step": 13170
    },
    {
      "epoch": 0.6878286169060759,
      "grad_norm": 3.689194440841675,
      "learning_rate": 3.853922029121654e-05,
      "loss": 0.9169,
      "step": 13180
    },
    {
      "epoch": 0.6883504899082808,
      "grad_norm": 4.02055025100708,
      "learning_rate": 3.8530522067393844e-05,
      "loss": 1.0366,
      "step": 13190
    },
    {
      "epoch": 0.6888723629104857,
      "grad_norm": 4.48296594619751,
      "learning_rate": 3.852182384357114e-05,
      "loss": 1.015,
      "step": 13200
    },
    {
      "epoch": 0.6893942359126907,
      "grad_norm": 4.571890354156494,
      "learning_rate": 3.8513125619748445e-05,
      "loss": 0.9597,
      "step": 13210
    },
    {
      "epoch": 0.6899161089148955,
      "grad_norm": 5.5949506759643555,
      "learning_rate": 3.8504427395925755e-05,
      "loss": 0.9552,
      "step": 13220
    },
    {
      "epoch": 0.6904379819171005,
      "grad_norm": 4.152929306030273,
      "learning_rate": 3.849572917210306e-05,
      "loss": 0.9553,
      "step": 13230
    },
    {
      "epoch": 0.6909598549193053,
      "grad_norm": 4.338495254516602,
      "learning_rate": 3.848703094828036e-05,
      "loss": 1.0621,
      "step": 13240
    },
    {
      "epoch": 0.6914817279215103,
      "grad_norm": 4.759878158569336,
      "learning_rate": 3.8478332724457666e-05,
      "loss": 0.9857,
      "step": 13250
    },
    {
      "epoch": 0.6920036009237153,
      "grad_norm": 4.731082439422607,
      "learning_rate": 3.846963450063497e-05,
      "loss": 1.058,
      "step": 13260
    },
    {
      "epoch": 0.6925254739259201,
      "grad_norm": 3.620713233947754,
      "learning_rate": 3.846093627681227e-05,
      "loss": 0.9703,
      "step": 13270
    },
    {
      "epoch": 0.6930473469281251,
      "grad_norm": 4.5308837890625,
      "learning_rate": 3.845223805298958e-05,
      "loss": 0.9688,
      "step": 13280
    },
    {
      "epoch": 0.6935692199303299,
      "grad_norm": 3.5915565490722656,
      "learning_rate": 3.844353982916689e-05,
      "loss": 0.9198,
      "step": 13290
    },
    {
      "epoch": 0.6940910929325349,
      "grad_norm": 4.246015548706055,
      "learning_rate": 3.843484160534419e-05,
      "loss": 0.9441,
      "step": 13300
    },
    {
      "epoch": 0.6946129659347398,
      "grad_norm": 3.9093782901763916,
      "learning_rate": 3.8426143381521494e-05,
      "loss": 0.9326,
      "step": 13310
    },
    {
      "epoch": 0.6951348389369447,
      "grad_norm": 4.826907634735107,
      "learning_rate": 3.84174451576988e-05,
      "loss": 0.946,
      "step": 13320
    },
    {
      "epoch": 0.6956567119391496,
      "grad_norm": 4.014623641967773,
      "learning_rate": 3.840874693387611e-05,
      "loss": 1.0169,
      "step": 13330
    },
    {
      "epoch": 0.6961785849413545,
      "grad_norm": 4.7451348304748535,
      "learning_rate": 3.840004871005341e-05,
      "loss": 0.972,
      "step": 13340
    },
    {
      "epoch": 0.6967004579435594,
      "grad_norm": 4.61384916305542,
      "learning_rate": 3.8391350486230715e-05,
      "loss": 1.0087,
      "step": 13350
    },
    {
      "epoch": 0.6972223309457644,
      "grad_norm": 4.021354675292969,
      "learning_rate": 3.838265226240802e-05,
      "loss": 1.0222,
      "step": 13360
    },
    {
      "epoch": 0.6977442039479692,
      "grad_norm": 3.7945449352264404,
      "learning_rate": 3.837395403858532e-05,
      "loss": 0.9599,
      "step": 13370
    },
    {
      "epoch": 0.6982660769501742,
      "grad_norm": 3.9413208961486816,
      "learning_rate": 3.836525581476263e-05,
      "loss": 0.8533,
      "step": 13380
    },
    {
      "epoch": 0.6987879499523791,
      "grad_norm": 4.021562576293945,
      "learning_rate": 3.8356557590939936e-05,
      "loss": 1.008,
      "step": 13390
    },
    {
      "epoch": 0.699309822954584,
      "grad_norm": 3.879730701446533,
      "learning_rate": 3.834785936711724e-05,
      "loss": 0.9496,
      "step": 13400
    },
    {
      "epoch": 0.699831695956789,
      "grad_norm": 4.0872111320495605,
      "learning_rate": 3.8339161143294536e-05,
      "loss": 1.0121,
      "step": 13410
    },
    {
      "epoch": 0.7003535689589938,
      "grad_norm": 4.600280284881592,
      "learning_rate": 3.8330462919471846e-05,
      "loss": 0.9397,
      "step": 13420
    },
    {
      "epoch": 0.7008754419611988,
      "grad_norm": 3.1081440448760986,
      "learning_rate": 3.832176469564915e-05,
      "loss": 0.9786,
      "step": 13430
    },
    {
      "epoch": 0.7013973149634036,
      "grad_norm": 4.6871232986450195,
      "learning_rate": 3.8313066471826453e-05,
      "loss": 0.9223,
      "step": 13440
    },
    {
      "epoch": 0.7019191879656086,
      "grad_norm": 4.816762447357178,
      "learning_rate": 3.830436824800376e-05,
      "loss": 1.0375,
      "step": 13450
    },
    {
      "epoch": 0.7024410609678134,
      "grad_norm": 4.3358073234558105,
      "learning_rate": 3.829567002418106e-05,
      "loss": 1.0375,
      "step": 13460
    },
    {
      "epoch": 0.7029629339700184,
      "grad_norm": 4.364450931549072,
      "learning_rate": 3.828697180035837e-05,
      "loss": 0.9601,
      "step": 13470
    },
    {
      "epoch": 0.7034848069722233,
      "grad_norm": 4.173943519592285,
      "learning_rate": 3.8278273576535674e-05,
      "loss": 0.9446,
      "step": 13480
    },
    {
      "epoch": 0.7040066799744282,
      "grad_norm": 3.5243003368377686,
      "learning_rate": 3.826957535271298e-05,
      "loss": 0.9942,
      "step": 13490
    },
    {
      "epoch": 0.7045285529766331,
      "grad_norm": 4.263195037841797,
      "learning_rate": 3.826087712889028e-05,
      "loss": 1.05,
      "step": 13500
    },
    {
      "epoch": 0.705050425978838,
      "grad_norm": 4.298685073852539,
      "learning_rate": 3.8252178905067585e-05,
      "loss": 0.93,
      "step": 13510
    },
    {
      "epoch": 0.705572298981043,
      "grad_norm": 4.697154998779297,
      "learning_rate": 3.8243480681244895e-05,
      "loss": 0.9945,
      "step": 13520
    },
    {
      "epoch": 0.7060941719832479,
      "grad_norm": 4.42631721496582,
      "learning_rate": 3.82347824574222e-05,
      "loss": 0.987,
      "step": 13530
    },
    {
      "epoch": 0.7066160449854528,
      "grad_norm": 4.812012672424316,
      "learning_rate": 3.82260842335995e-05,
      "loss": 1.0921,
      "step": 13540
    },
    {
      "epoch": 0.7071379179876577,
      "grad_norm": 3.5860848426818848,
      "learning_rate": 3.8217386009776806e-05,
      "loss": 1.0673,
      "step": 13550
    },
    {
      "epoch": 0.7076597909898626,
      "grad_norm": 3.8516554832458496,
      "learning_rate": 3.820868778595411e-05,
      "loss": 1.0157,
      "step": 13560
    },
    {
      "epoch": 0.7081816639920675,
      "grad_norm": 4.378089904785156,
      "learning_rate": 3.819998956213141e-05,
      "loss": 0.9063,
      "step": 13570
    },
    {
      "epoch": 0.7087035369942725,
      "grad_norm": 3.529991388320923,
      "learning_rate": 3.8191291338308723e-05,
      "loss": 0.9861,
      "step": 13580
    },
    {
      "epoch": 0.7092254099964773,
      "grad_norm": 4.200531959533691,
      "learning_rate": 3.818259311448603e-05,
      "loss": 0.9058,
      "step": 13590
    },
    {
      "epoch": 0.7097472829986823,
      "grad_norm": 3.7780401706695557,
      "learning_rate": 3.817389489066333e-05,
      "loss": 0.9213,
      "step": 13600
    },
    {
      "epoch": 0.7102691560008871,
      "grad_norm": 3.787706136703491,
      "learning_rate": 3.816519666684063e-05,
      "loss": 1.057,
      "step": 13610
    },
    {
      "epoch": 0.7107910290030921,
      "grad_norm": 4.31032133102417,
      "learning_rate": 3.815649844301794e-05,
      "loss": 0.8452,
      "step": 13620
    },
    {
      "epoch": 0.7113129020052971,
      "grad_norm": 4.226261615753174,
      "learning_rate": 3.814780021919524e-05,
      "loss": 0.95,
      "step": 13630
    },
    {
      "epoch": 0.7118347750075019,
      "grad_norm": 4.176239490509033,
      "learning_rate": 3.8139101995372545e-05,
      "loss": 0.9919,
      "step": 13640
    },
    {
      "epoch": 0.7123566480097069,
      "grad_norm": 4.772751331329346,
      "learning_rate": 3.813040377154985e-05,
      "loss": 0.9887,
      "step": 13650
    },
    {
      "epoch": 0.7128785210119117,
      "grad_norm": 5.273766040802002,
      "learning_rate": 3.812170554772715e-05,
      "loss": 1.1306,
      "step": 13660
    },
    {
      "epoch": 0.7134003940141167,
      "grad_norm": 4.754718780517578,
      "learning_rate": 3.811300732390446e-05,
      "loss": 1.0822,
      "step": 13670
    },
    {
      "epoch": 0.7139222670163216,
      "grad_norm": 4.310832500457764,
      "learning_rate": 3.8104309100081766e-05,
      "loss": 1.0053,
      "step": 13680
    },
    {
      "epoch": 0.7144441400185265,
      "grad_norm": 5.294986248016357,
      "learning_rate": 3.809561087625907e-05,
      "loss": 1.0153,
      "step": 13690
    },
    {
      "epoch": 0.7149660130207314,
      "grad_norm": 4.3059773445129395,
      "learning_rate": 3.808691265243637e-05,
      "loss": 0.9678,
      "step": 13700
    },
    {
      "epoch": 0.7154878860229363,
      "grad_norm": 4.1592535972595215,
      "learning_rate": 3.8078214428613676e-05,
      "loss": 1.0311,
      "step": 13710
    },
    {
      "epoch": 0.7160097590251412,
      "grad_norm": 4.494657516479492,
      "learning_rate": 3.806951620479099e-05,
      "loss": 0.9978,
      "step": 13720
    },
    {
      "epoch": 0.7165316320273462,
      "grad_norm": 4.8290300369262695,
      "learning_rate": 3.806081798096829e-05,
      "loss": 0.9046,
      "step": 13730
    },
    {
      "epoch": 0.717053505029551,
      "grad_norm": 4.582231044769287,
      "learning_rate": 3.8052119757145594e-05,
      "loss": 0.977,
      "step": 13740
    },
    {
      "epoch": 0.717575378031756,
      "grad_norm": 4.428091049194336,
      "learning_rate": 3.80434215333229e-05,
      "loss": 0.9838,
      "step": 13750
    },
    {
      "epoch": 0.7180972510339609,
      "grad_norm": 4.399441242218018,
      "learning_rate": 3.80347233095002e-05,
      "loss": 0.9132,
      "step": 13760
    },
    {
      "epoch": 0.7186191240361658,
      "grad_norm": 5.415102481842041,
      "learning_rate": 3.802602508567751e-05,
      "loss": 1.0035,
      "step": 13770
    },
    {
      "epoch": 0.7191409970383708,
      "grad_norm": 3.805238723754883,
      "learning_rate": 3.8017326861854815e-05,
      "loss": 0.9851,
      "step": 13780
    },
    {
      "epoch": 0.7196628700405756,
      "grad_norm": 3.9296951293945312,
      "learning_rate": 3.800862863803212e-05,
      "loss": 0.8601,
      "step": 13790
    },
    {
      "epoch": 0.7201847430427806,
      "grad_norm": 4.427764415740967,
      "learning_rate": 3.799993041420942e-05,
      "loss": 0.9465,
      "step": 13800
    },
    {
      "epoch": 0.7207066160449854,
      "grad_norm": 3.998262643814087,
      "learning_rate": 3.7991232190386725e-05,
      "loss": 0.8574,
      "step": 13810
    },
    {
      "epoch": 0.7212284890471904,
      "grad_norm": 4.723665714263916,
      "learning_rate": 3.798253396656403e-05,
      "loss": 0.9125,
      "step": 13820
    },
    {
      "epoch": 0.7217503620493952,
      "grad_norm": 4.744340419769287,
      "learning_rate": 3.797383574274133e-05,
      "loss": 0.9011,
      "step": 13830
    },
    {
      "epoch": 0.7222722350516002,
      "grad_norm": 4.806562423706055,
      "learning_rate": 3.7965137518918636e-05,
      "loss": 0.9839,
      "step": 13840
    },
    {
      "epoch": 0.7227941080538051,
      "grad_norm": 4.422819137573242,
      "learning_rate": 3.795643929509594e-05,
      "loss": 1.1133,
      "step": 13850
    },
    {
      "epoch": 0.72331598105601,
      "grad_norm": 4.661141872406006,
      "learning_rate": 3.794774107127324e-05,
      "loss": 0.9932,
      "step": 13860
    },
    {
      "epoch": 0.7238378540582149,
      "grad_norm": 3.69327974319458,
      "learning_rate": 3.7939042847450553e-05,
      "loss": 0.9125,
      "step": 13870
    },
    {
      "epoch": 0.7243597270604198,
      "grad_norm": 5.237557411193848,
      "learning_rate": 3.793034462362786e-05,
      "loss": 1.0628,
      "step": 13880
    },
    {
      "epoch": 0.7248816000626248,
      "grad_norm": 4.249222755432129,
      "learning_rate": 3.792164639980516e-05,
      "loss": 0.9958,
      "step": 13890
    },
    {
      "epoch": 0.7254034730648297,
      "grad_norm": 4.381783962249756,
      "learning_rate": 3.7912948175982464e-05,
      "loss": 0.989,
      "step": 13900
    },
    {
      "epoch": 0.7259253460670346,
      "grad_norm": 4.8763813972473145,
      "learning_rate": 3.790424995215977e-05,
      "loss": 0.911,
      "step": 13910
    },
    {
      "epoch": 0.7264472190692395,
      "grad_norm": 4.149188995361328,
      "learning_rate": 3.789555172833708e-05,
      "loss": 0.9864,
      "step": 13920
    },
    {
      "epoch": 0.7269690920714444,
      "grad_norm": 4.615305423736572,
      "learning_rate": 3.788685350451438e-05,
      "loss": 0.9862,
      "step": 13930
    },
    {
      "epoch": 0.7274909650736493,
      "grad_norm": 3.2570836544036865,
      "learning_rate": 3.7878155280691685e-05,
      "loss": 0.9231,
      "step": 13940
    },
    {
      "epoch": 0.7280128380758543,
      "grad_norm": 4.643311500549316,
      "learning_rate": 3.786945705686899e-05,
      "loss": 0.9742,
      "step": 13950
    },
    {
      "epoch": 0.7285347110780591,
      "grad_norm": 4.4227166175842285,
      "learning_rate": 3.786075883304629e-05,
      "loss": 1.0066,
      "step": 13960
    },
    {
      "epoch": 0.7290565840802641,
      "grad_norm": 4.544559478759766,
      "learning_rate": 3.78520606092236e-05,
      "loss": 0.926,
      "step": 13970
    },
    {
      "epoch": 0.7295784570824689,
      "grad_norm": 4.491657733917236,
      "learning_rate": 3.7843362385400906e-05,
      "loss": 0.9328,
      "step": 13980
    },
    {
      "epoch": 0.7301003300846739,
      "grad_norm": 4.8018879890441895,
      "learning_rate": 3.783466416157821e-05,
      "loss": 0.9346,
      "step": 13990
    },
    {
      "epoch": 0.7306222030868789,
      "grad_norm": 3.252596378326416,
      "learning_rate": 3.782596593775551e-05,
      "loss": 0.8658,
      "step": 14000
    },
    {
      "epoch": 0.7311440760890837,
      "grad_norm": 4.385624408721924,
      "learning_rate": 3.781726771393282e-05,
      "loss": 0.8873,
      "step": 14010
    },
    {
      "epoch": 0.7316659490912887,
      "grad_norm": 4.657416343688965,
      "learning_rate": 3.780856949011013e-05,
      "loss": 1.0846,
      "step": 14020
    },
    {
      "epoch": 0.7321878220934935,
      "grad_norm": 3.741259813308716,
      "learning_rate": 3.7799871266287424e-05,
      "loss": 0.8913,
      "step": 14030
    },
    {
      "epoch": 0.7327096950956985,
      "grad_norm": 4.231512546539307,
      "learning_rate": 3.779117304246473e-05,
      "loss": 0.9544,
      "step": 14040
    },
    {
      "epoch": 0.7332315680979034,
      "grad_norm": 4.987084865570068,
      "learning_rate": 3.778247481864203e-05,
      "loss": 0.9635,
      "step": 14050
    },
    {
      "epoch": 0.7337534411001083,
      "grad_norm": 4.025521755218506,
      "learning_rate": 3.7773776594819334e-05,
      "loss": 0.9517,
      "step": 14060
    },
    {
      "epoch": 0.7342753141023132,
      "grad_norm": 4.74338960647583,
      "learning_rate": 3.7765078370996645e-05,
      "loss": 0.9749,
      "step": 14070
    },
    {
      "epoch": 0.7347971871045181,
      "grad_norm": 3.9127535820007324,
      "learning_rate": 3.775638014717395e-05,
      "loss": 0.873,
      "step": 14080
    },
    {
      "epoch": 0.735319060106723,
      "grad_norm": 4.872260093688965,
      "learning_rate": 3.774768192335125e-05,
      "loss": 0.9359,
      "step": 14090
    },
    {
      "epoch": 0.735840933108928,
      "grad_norm": 4.6687092781066895,
      "learning_rate": 3.7738983699528555e-05,
      "loss": 0.9793,
      "step": 14100
    },
    {
      "epoch": 0.7363628061111328,
      "grad_norm": 4.393532752990723,
      "learning_rate": 3.773028547570586e-05,
      "loss": 0.9925,
      "step": 14110
    },
    {
      "epoch": 0.7368846791133378,
      "grad_norm": 4.0397138595581055,
      "learning_rate": 3.772158725188317e-05,
      "loss": 0.9947,
      "step": 14120
    },
    {
      "epoch": 0.7374065521155427,
      "grad_norm": 4.10598611831665,
      "learning_rate": 3.771288902806047e-05,
      "loss": 0.9511,
      "step": 14130
    },
    {
      "epoch": 0.7379284251177476,
      "grad_norm": 5.132713794708252,
      "learning_rate": 3.7704190804237776e-05,
      "loss": 0.986,
      "step": 14140
    },
    {
      "epoch": 0.7384502981199526,
      "grad_norm": 4.663928985595703,
      "learning_rate": 3.769549258041508e-05,
      "loss": 0.8864,
      "step": 14150
    },
    {
      "epoch": 0.7389721711221574,
      "grad_norm": 4.950418472290039,
      "learning_rate": 3.7686794356592384e-05,
      "loss": 0.9906,
      "step": 14160
    },
    {
      "epoch": 0.7394940441243624,
      "grad_norm": 3.752354145050049,
      "learning_rate": 3.7678096132769694e-05,
      "loss": 0.9295,
      "step": 14170
    },
    {
      "epoch": 0.7400159171265672,
      "grad_norm": 3.828221321105957,
      "learning_rate": 3.7669397908947e-05,
      "loss": 0.8896,
      "step": 14180
    },
    {
      "epoch": 0.7405377901287722,
      "grad_norm": 5.2240166664123535,
      "learning_rate": 3.76606996851243e-05,
      "loss": 0.9605,
      "step": 14190
    },
    {
      "epoch": 0.741059663130977,
      "grad_norm": 4.786566257476807,
      "learning_rate": 3.7652001461301604e-05,
      "loss": 0.9684,
      "step": 14200
    },
    {
      "epoch": 0.741581536133182,
      "grad_norm": 4.054124355316162,
      "learning_rate": 3.764330323747891e-05,
      "loss": 0.924,
      "step": 14210
    },
    {
      "epoch": 0.7421034091353869,
      "grad_norm": 4.898019313812256,
      "learning_rate": 3.763460501365622e-05,
      "loss": 1.018,
      "step": 14220
    },
    {
      "epoch": 0.7426252821375918,
      "grad_norm": 4.7271528244018555,
      "learning_rate": 3.762590678983352e-05,
      "loss": 0.9531,
      "step": 14230
    },
    {
      "epoch": 0.7431471551397967,
      "grad_norm": 4.5141377449035645,
      "learning_rate": 3.761720856601082e-05,
      "loss": 0.9362,
      "step": 14240
    },
    {
      "epoch": 0.7436690281420016,
      "grad_norm": 4.796915054321289,
      "learning_rate": 3.760851034218812e-05,
      "loss": 1.051,
      "step": 14250
    },
    {
      "epoch": 0.7441909011442066,
      "grad_norm": 4.527780532836914,
      "learning_rate": 3.759981211836543e-05,
      "loss": 1.0098,
      "step": 14260
    },
    {
      "epoch": 0.7447127741464115,
      "grad_norm": 5.241762161254883,
      "learning_rate": 3.7591113894542736e-05,
      "loss": 0.997,
      "step": 14270
    },
    {
      "epoch": 0.7452346471486164,
      "grad_norm": 5.337599754333496,
      "learning_rate": 3.758241567072004e-05,
      "loss": 0.9247,
      "step": 14280
    },
    {
      "epoch": 0.7457565201508213,
      "grad_norm": 4.730704307556152,
      "learning_rate": 3.757371744689734e-05,
      "loss": 0.9912,
      "step": 14290
    },
    {
      "epoch": 0.7462783931530262,
      "grad_norm": 5.078736782073975,
      "learning_rate": 3.756501922307465e-05,
      "loss": 0.9756,
      "step": 14300
    },
    {
      "epoch": 0.7468002661552311,
      "grad_norm": 4.310504913330078,
      "learning_rate": 3.755632099925196e-05,
      "loss": 0.8971,
      "step": 14310
    },
    {
      "epoch": 0.7473221391574361,
      "grad_norm": 4.464778423309326,
      "learning_rate": 3.754762277542926e-05,
      "loss": 0.989,
      "step": 14320
    },
    {
      "epoch": 0.7478440121596409,
      "grad_norm": 3.718242883682251,
      "learning_rate": 3.7538924551606564e-05,
      "loss": 1.0076,
      "step": 14330
    },
    {
      "epoch": 0.7483658851618459,
      "grad_norm": 3.667759895324707,
      "learning_rate": 3.753022632778387e-05,
      "loss": 1.0187,
      "step": 14340
    },
    {
      "epoch": 0.7488877581640507,
      "grad_norm": 3.961104393005371,
      "learning_rate": 3.752152810396117e-05,
      "loss": 0.9338,
      "step": 14350
    },
    {
      "epoch": 0.7494096311662557,
      "grad_norm": 4.973008155822754,
      "learning_rate": 3.7512829880138475e-05,
      "loss": 0.9117,
      "step": 14360
    },
    {
      "epoch": 0.7499315041684605,
      "grad_norm": 3.8038783073425293,
      "learning_rate": 3.7504131656315785e-05,
      "loss": 0.9583,
      "step": 14370
    },
    {
      "epoch": 0.7504533771706655,
      "grad_norm": 6.149051189422607,
      "learning_rate": 3.749543343249309e-05,
      "loss": 0.9373,
      "step": 14380
    },
    {
      "epoch": 0.7509752501728705,
      "grad_norm": 3.253612756729126,
      "learning_rate": 3.748673520867039e-05,
      "loss": 0.9534,
      "step": 14390
    },
    {
      "epoch": 0.7514971231750753,
      "grad_norm": 3.234933614730835,
      "learning_rate": 3.7478036984847696e-05,
      "loss": 0.8769,
      "step": 14400
    },
    {
      "epoch": 0.7520189961772803,
      "grad_norm": 4.5020904541015625,
      "learning_rate": 3.7469338761025e-05,
      "loss": 1.0024,
      "step": 14410
    },
    {
      "epoch": 0.7525408691794852,
      "grad_norm": 4.426030158996582,
      "learning_rate": 3.746064053720231e-05,
      "loss": 1.0059,
      "step": 14420
    },
    {
      "epoch": 0.7530627421816901,
      "grad_norm": 3.927126407623291,
      "learning_rate": 3.745194231337961e-05,
      "loss": 0.9595,
      "step": 14430
    },
    {
      "epoch": 0.753584615183895,
      "grad_norm": 5.269240856170654,
      "learning_rate": 3.744324408955692e-05,
      "loss": 0.9885,
      "step": 14440
    },
    {
      "epoch": 0.7541064881860999,
      "grad_norm": 4.374817848205566,
      "learning_rate": 3.7434545865734214e-05,
      "loss": 0.9664,
      "step": 14450
    },
    {
      "epoch": 0.7546283611883048,
      "grad_norm": 3.9000084400177,
      "learning_rate": 3.7425847641911524e-05,
      "loss": 1.0161,
      "step": 14460
    },
    {
      "epoch": 0.7551502341905098,
      "grad_norm": 5.107619762420654,
      "learning_rate": 3.741714941808883e-05,
      "loss": 0.971,
      "step": 14470
    },
    {
      "epoch": 0.7556721071927146,
      "grad_norm": 3.946220636367798,
      "learning_rate": 3.740845119426613e-05,
      "loss": 0.9698,
      "step": 14480
    },
    {
      "epoch": 0.7561939801949196,
      "grad_norm": 4.218505859375,
      "learning_rate": 3.7399752970443435e-05,
      "loss": 1.0374,
      "step": 14490
    },
    {
      "epoch": 0.7567158531971245,
      "grad_norm": 4.1882548332214355,
      "learning_rate": 3.739105474662074e-05,
      "loss": 1.0259,
      "step": 14500
    },
    {
      "epoch": 0.7572377261993294,
      "grad_norm": 3.4281280040740967,
      "learning_rate": 3.738235652279805e-05,
      "loss": 0.9603,
      "step": 14510
    },
    {
      "epoch": 0.7577595992015344,
      "grad_norm": 3.9654672145843506,
      "learning_rate": 3.737365829897535e-05,
      "loss": 0.9435,
      "step": 14520
    },
    {
      "epoch": 0.7582814722037392,
      "grad_norm": 4.004446506500244,
      "learning_rate": 3.7364960075152655e-05,
      "loss": 0.9146,
      "step": 14530
    },
    {
      "epoch": 0.7588033452059442,
      "grad_norm": 5.139282703399658,
      "learning_rate": 3.735626185132996e-05,
      "loss": 0.9241,
      "step": 14540
    },
    {
      "epoch": 0.759325218208149,
      "grad_norm": 5.427370071411133,
      "learning_rate": 3.734756362750726e-05,
      "loss": 1.0543,
      "step": 14550
    },
    {
      "epoch": 0.759847091210354,
      "grad_norm": 4.489597797393799,
      "learning_rate": 3.733886540368457e-05,
      "loss": 0.9378,
      "step": 14560
    },
    {
      "epoch": 0.7603689642125588,
      "grad_norm": 4.515807628631592,
      "learning_rate": 3.7330167179861876e-05,
      "loss": 0.9805,
      "step": 14570
    },
    {
      "epoch": 0.7608908372147638,
      "grad_norm": 4.966231346130371,
      "learning_rate": 3.732146895603918e-05,
      "loss": 0.9415,
      "step": 14580
    },
    {
      "epoch": 0.7614127102169687,
      "grad_norm": 3.937648057937622,
      "learning_rate": 3.7312770732216484e-05,
      "loss": 0.9611,
      "step": 14590
    },
    {
      "epoch": 0.7619345832191736,
      "grad_norm": 4.264194965362549,
      "learning_rate": 3.730407250839379e-05,
      "loss": 1.0611,
      "step": 14600
    },
    {
      "epoch": 0.7624564562213785,
      "grad_norm": 4.212636947631836,
      "learning_rate": 3.72953742845711e-05,
      "loss": 0.9831,
      "step": 14610
    },
    {
      "epoch": 0.7629783292235834,
      "grad_norm": 4.132932186126709,
      "learning_rate": 3.72866760607484e-05,
      "loss": 0.9585,
      "step": 14620
    },
    {
      "epoch": 0.7635002022257884,
      "grad_norm": 5.492583274841309,
      "learning_rate": 3.7277977836925705e-05,
      "loss": 0.9188,
      "step": 14630
    },
    {
      "epoch": 0.7640220752279933,
      "grad_norm": 5.405674457550049,
      "learning_rate": 3.726927961310301e-05,
      "loss": 1.0041,
      "step": 14640
    },
    {
      "epoch": 0.7645439482301982,
      "grad_norm": 3.658876895904541,
      "learning_rate": 3.726058138928031e-05,
      "loss": 0.9506,
      "step": 14650
    },
    {
      "epoch": 0.7650658212324031,
      "grad_norm": 3.5626285076141357,
      "learning_rate": 3.7251883165457615e-05,
      "loss": 0.8371,
      "step": 14660
    },
    {
      "epoch": 0.765587694234608,
      "grad_norm": 4.856328964233398,
      "learning_rate": 3.724318494163492e-05,
      "loss": 1.0121,
      "step": 14670
    },
    {
      "epoch": 0.7661095672368129,
      "grad_norm": 3.7731378078460693,
      "learning_rate": 3.723448671781222e-05,
      "loss": 1.039,
      "step": 14680
    },
    {
      "epoch": 0.7666314402390179,
      "grad_norm": 4.4998016357421875,
      "learning_rate": 3.7225788493989526e-05,
      "loss": 0.9592,
      "step": 14690
    },
    {
      "epoch": 0.7671533132412227,
      "grad_norm": 4.308333396911621,
      "learning_rate": 3.721709027016683e-05,
      "loss": 0.9433,
      "step": 14700
    },
    {
      "epoch": 0.7676751862434277,
      "grad_norm": 4.6575775146484375,
      "learning_rate": 3.720839204634414e-05,
      "loss": 1.0213,
      "step": 14710
    },
    {
      "epoch": 0.7681970592456325,
      "grad_norm": 4.038580894470215,
      "learning_rate": 3.719969382252144e-05,
      "loss": 0.936,
      "step": 14720
    },
    {
      "epoch": 0.7687189322478375,
      "grad_norm": 5.309543132781982,
      "learning_rate": 3.719099559869875e-05,
      "loss": 0.9483,
      "step": 14730
    },
    {
      "epoch": 0.7692408052500423,
      "grad_norm": 5.340073585510254,
      "learning_rate": 3.718229737487605e-05,
      "loss": 1.0387,
      "step": 14740
    },
    {
      "epoch": 0.7697626782522473,
      "grad_norm": 3.5370304584503174,
      "learning_rate": 3.7173599151053354e-05,
      "loss": 0.9745,
      "step": 14750
    },
    {
      "epoch": 0.7702845512544523,
      "grad_norm": 4.7218098640441895,
      "learning_rate": 3.7164900927230664e-05,
      "loss": 1.0397,
      "step": 14760
    },
    {
      "epoch": 0.7708064242566571,
      "grad_norm": 5.216379642486572,
      "learning_rate": 3.715620270340797e-05,
      "loss": 0.9958,
      "step": 14770
    },
    {
      "epoch": 0.7713282972588621,
      "grad_norm": 4.592358112335205,
      "learning_rate": 3.714750447958527e-05,
      "loss": 0.927,
      "step": 14780
    },
    {
      "epoch": 0.771850170261067,
      "grad_norm": 4.0324788093566895,
      "learning_rate": 3.7138806255762575e-05,
      "loss": 0.9198,
      "step": 14790
    },
    {
      "epoch": 0.7723720432632719,
      "grad_norm": 4.233147144317627,
      "learning_rate": 3.713010803193988e-05,
      "loss": 0.9404,
      "step": 14800
    },
    {
      "epoch": 0.7728939162654768,
      "grad_norm": 3.632503032684326,
      "learning_rate": 3.712140980811719e-05,
      "loss": 0.9809,
      "step": 14810
    },
    {
      "epoch": 0.7734157892676817,
      "grad_norm": 3.805752754211426,
      "learning_rate": 3.711271158429449e-05,
      "loss": 0.8344,
      "step": 14820
    },
    {
      "epoch": 0.7739376622698866,
      "grad_norm": 3.9874558448791504,
      "learning_rate": 3.7104013360471796e-05,
      "loss": 0.949,
      "step": 14830
    },
    {
      "epoch": 0.7744595352720915,
      "grad_norm": 4.531088352203369,
      "learning_rate": 3.70953151366491e-05,
      "loss": 0.9511,
      "step": 14840
    },
    {
      "epoch": 0.7749814082742964,
      "grad_norm": 4.2629523277282715,
      "learning_rate": 3.70866169128264e-05,
      "loss": 0.9511,
      "step": 14850
    },
    {
      "epoch": 0.7755032812765014,
      "grad_norm": 4.055787086486816,
      "learning_rate": 3.7077918689003706e-05,
      "loss": 0.9088,
      "step": 14860
    },
    {
      "epoch": 0.7760251542787063,
      "grad_norm": 4.358226776123047,
      "learning_rate": 3.706922046518101e-05,
      "loss": 0.9138,
      "step": 14870
    },
    {
      "epoch": 0.7765470272809112,
      "grad_norm": 4.5209527015686035,
      "learning_rate": 3.7060522241358314e-05,
      "loss": 0.9559,
      "step": 14880
    },
    {
      "epoch": 0.7770689002831161,
      "grad_norm": 4.278618335723877,
      "learning_rate": 3.705182401753562e-05,
      "loss": 0.9716,
      "step": 14890
    },
    {
      "epoch": 0.777590773285321,
      "grad_norm": 4.2098493576049805,
      "learning_rate": 3.704312579371292e-05,
      "loss": 1.056,
      "step": 14900
    },
    {
      "epoch": 0.778112646287526,
      "grad_norm": 4.300238609313965,
      "learning_rate": 3.703442756989023e-05,
      "loss": 1.0369,
      "step": 14910
    },
    {
      "epoch": 0.7786345192897308,
      "grad_norm": 4.103745460510254,
      "learning_rate": 3.7025729346067535e-05,
      "loss": 0.9974,
      "step": 14920
    },
    {
      "epoch": 0.7791563922919358,
      "grad_norm": 4.666528224945068,
      "learning_rate": 3.701703112224484e-05,
      "loss": 0.9662,
      "step": 14930
    },
    {
      "epoch": 0.7796782652941406,
      "grad_norm": 5.05417537689209,
      "learning_rate": 3.700833289842214e-05,
      "loss": 0.9446,
      "step": 14940
    },
    {
      "epoch": 0.7802001382963456,
      "grad_norm": 4.832796573638916,
      "learning_rate": 3.6999634674599445e-05,
      "loss": 1.0366,
      "step": 14950
    },
    {
      "epoch": 0.7807220112985505,
      "grad_norm": 4.740368843078613,
      "learning_rate": 3.6990936450776756e-05,
      "loss": 0.8583,
      "step": 14960
    },
    {
      "epoch": 0.7812438843007554,
      "grad_norm": 4.930429935455322,
      "learning_rate": 3.698223822695406e-05,
      "loss": 1.0461,
      "step": 14970
    },
    {
      "epoch": 0.7817657573029603,
      "grad_norm": 4.419333457946777,
      "learning_rate": 3.697354000313136e-05,
      "loss": 0.9085,
      "step": 14980
    },
    {
      "epoch": 0.7822876303051652,
      "grad_norm": 3.6372621059417725,
      "learning_rate": 3.6964841779308666e-05,
      "loss": 0.8827,
      "step": 14990
    },
    {
      "epoch": 0.7828095033073702,
      "grad_norm": 4.052815914154053,
      "learning_rate": 3.695614355548597e-05,
      "loss": 0.9015,
      "step": 15000
    },
    {
      "epoch": 0.7833313763095751,
      "grad_norm": 5.160681247711182,
      "learning_rate": 3.694744533166328e-05,
      "loss": 1.0199,
      "step": 15010
    },
    {
      "epoch": 0.78385324931178,
      "grad_norm": 4.263344764709473,
      "learning_rate": 3.6938747107840584e-05,
      "loss": 0.9791,
      "step": 15020
    },
    {
      "epoch": 0.7843751223139849,
      "grad_norm": 4.084190845489502,
      "learning_rate": 3.693004888401789e-05,
      "loss": 0.8817,
      "step": 15030
    },
    {
      "epoch": 0.7848969953161898,
      "grad_norm": 4.0205254554748535,
      "learning_rate": 3.692135066019519e-05,
      "loss": 0.9678,
      "step": 15040
    },
    {
      "epoch": 0.7854188683183947,
      "grad_norm": 4.250985145568848,
      "learning_rate": 3.6912652436372494e-05,
      "loss": 0.9501,
      "step": 15050
    },
    {
      "epoch": 0.7859407413205997,
      "grad_norm": 3.9996132850646973,
      "learning_rate": 3.6903954212549805e-05,
      "loss": 0.986,
      "step": 15060
    },
    {
      "epoch": 0.7864626143228045,
      "grad_norm": 4.530031681060791,
      "learning_rate": 3.68952559887271e-05,
      "loss": 1.084,
      "step": 15070
    },
    {
      "epoch": 0.7869844873250095,
      "grad_norm": 4.427491188049316,
      "learning_rate": 3.6886557764904405e-05,
      "loss": 0.9667,
      "step": 15080
    },
    {
      "epoch": 0.7875063603272143,
      "grad_norm": 4.107735633850098,
      "learning_rate": 3.687785954108171e-05,
      "loss": 1.0129,
      "step": 15090
    },
    {
      "epoch": 0.7880282333294193,
      "grad_norm": 4.627134323120117,
      "learning_rate": 3.686916131725902e-05,
      "loss": 1.0092,
      "step": 15100
    },
    {
      "epoch": 0.7885501063316241,
      "grad_norm": 5.351053714752197,
      "learning_rate": 3.686046309343632e-05,
      "loss": 0.9527,
      "step": 15110
    },
    {
      "epoch": 0.7890719793338291,
      "grad_norm": 3.7693347930908203,
      "learning_rate": 3.6851764869613626e-05,
      "loss": 0.9672,
      "step": 15120
    },
    {
      "epoch": 0.7895938523360341,
      "grad_norm": 4.851033687591553,
      "learning_rate": 3.684306664579093e-05,
      "loss": 0.9723,
      "step": 15130
    },
    {
      "epoch": 0.7901157253382389,
      "grad_norm": 4.1279425621032715,
      "learning_rate": 3.683436842196823e-05,
      "loss": 0.9697,
      "step": 15140
    },
    {
      "epoch": 0.7906375983404439,
      "grad_norm": 4.635321140289307,
      "learning_rate": 3.6825670198145537e-05,
      "loss": 0.9908,
      "step": 15150
    },
    {
      "epoch": 0.7911594713426487,
      "grad_norm": 3.8612537384033203,
      "learning_rate": 3.681697197432285e-05,
      "loss": 0.975,
      "step": 15160
    },
    {
      "epoch": 0.7916813443448537,
      "grad_norm": 3.9852023124694824,
      "learning_rate": 3.680827375050015e-05,
      "loss": 0.9974,
      "step": 15170
    },
    {
      "epoch": 0.7922032173470586,
      "grad_norm": 4.7160186767578125,
      "learning_rate": 3.6799575526677454e-05,
      "loss": 0.9785,
      "step": 15180
    },
    {
      "epoch": 0.7927250903492635,
      "grad_norm": 4.296873092651367,
      "learning_rate": 3.679087730285476e-05,
      "loss": 0.9335,
      "step": 15190
    },
    {
      "epoch": 0.7932469633514684,
      "grad_norm": 4.881336212158203,
      "learning_rate": 3.678217907903206e-05,
      "loss": 0.915,
      "step": 15200
    },
    {
      "epoch": 0.7937688363536733,
      "grad_norm": 4.343094348907471,
      "learning_rate": 3.677348085520937e-05,
      "loss": 0.9469,
      "step": 15210
    },
    {
      "epoch": 0.7942907093558782,
      "grad_norm": 3.222247362136841,
      "learning_rate": 3.6764782631386675e-05,
      "loss": 0.9298,
      "step": 15220
    },
    {
      "epoch": 0.7948125823580832,
      "grad_norm": 4.810613632202148,
      "learning_rate": 3.675608440756398e-05,
      "loss": 1.059,
      "step": 15230
    },
    {
      "epoch": 0.7953344553602881,
      "grad_norm": 5.1259379386901855,
      "learning_rate": 3.674738618374128e-05,
      "loss": 1.046,
      "step": 15240
    },
    {
      "epoch": 0.795856328362493,
      "grad_norm": 5.143657684326172,
      "learning_rate": 3.6738687959918586e-05,
      "loss": 0.9942,
      "step": 15250
    },
    {
      "epoch": 0.796378201364698,
      "grad_norm": 3.5353448390960693,
      "learning_rate": 3.6729989736095896e-05,
      "loss": 0.9656,
      "step": 15260
    },
    {
      "epoch": 0.7969000743669028,
      "grad_norm": 3.642017126083374,
      "learning_rate": 3.67212915122732e-05,
      "loss": 1.0332,
      "step": 15270
    },
    {
      "epoch": 0.7974219473691078,
      "grad_norm": 4.702576637268066,
      "learning_rate": 3.6712593288450496e-05,
      "loss": 0.9765,
      "step": 15280
    },
    {
      "epoch": 0.7979438203713126,
      "grad_norm": 3.770653486251831,
      "learning_rate": 3.67038950646278e-05,
      "loss": 0.8572,
      "step": 15290
    },
    {
      "epoch": 0.7984656933735176,
      "grad_norm": 4.226595401763916,
      "learning_rate": 3.669519684080511e-05,
      "loss": 0.9591,
      "step": 15300
    },
    {
      "epoch": 0.7989875663757224,
      "grad_norm": 4.5805983543396,
      "learning_rate": 3.6686498616982414e-05,
      "loss": 1.0311,
      "step": 15310
    },
    {
      "epoch": 0.7995094393779274,
      "grad_norm": 4.110890865325928,
      "learning_rate": 3.667780039315972e-05,
      "loss": 1.0749,
      "step": 15320
    },
    {
      "epoch": 0.8000313123801323,
      "grad_norm": 4.985821723937988,
      "learning_rate": 3.666910216933702e-05,
      "loss": 0.9478,
      "step": 15330
    },
    {
      "epoch": 0.8005531853823372,
      "grad_norm": 5.0113396644592285,
      "learning_rate": 3.6660403945514324e-05,
      "loss": 1.0868,
      "step": 15340
    },
    {
      "epoch": 0.8010750583845421,
      "grad_norm": 4.261971950531006,
      "learning_rate": 3.6651705721691635e-05,
      "loss": 0.9402,
      "step": 15350
    },
    {
      "epoch": 0.801596931386747,
      "grad_norm": 4.344806671142578,
      "learning_rate": 3.664300749786894e-05,
      "loss": 0.9551,
      "step": 15360
    },
    {
      "epoch": 0.802118804388952,
      "grad_norm": 6.000804901123047,
      "learning_rate": 3.663430927404624e-05,
      "loss": 0.9855,
      "step": 15370
    },
    {
      "epoch": 0.8026406773911569,
      "grad_norm": 4.451313495635986,
      "learning_rate": 3.6625611050223545e-05,
      "loss": 0.8995,
      "step": 15380
    },
    {
      "epoch": 0.8031625503933618,
      "grad_norm": 4.503860950469971,
      "learning_rate": 3.661691282640085e-05,
      "loss": 0.8908,
      "step": 15390
    },
    {
      "epoch": 0.8036844233955667,
      "grad_norm": 4.952674865722656,
      "learning_rate": 3.660821460257816e-05,
      "loss": 0.9813,
      "step": 15400
    },
    {
      "epoch": 0.8042062963977716,
      "grad_norm": 3.2647933959960938,
      "learning_rate": 3.659951637875546e-05,
      "loss": 0.8984,
      "step": 15410
    },
    {
      "epoch": 0.8047281693999765,
      "grad_norm": 4.845026016235352,
      "learning_rate": 3.6590818154932766e-05,
      "loss": 1.0416,
      "step": 15420
    },
    {
      "epoch": 0.8052500424021815,
      "grad_norm": 5.112083911895752,
      "learning_rate": 3.658211993111007e-05,
      "loss": 1.139,
      "step": 15430
    },
    {
      "epoch": 0.8057719154043863,
      "grad_norm": 3.8269011974334717,
      "learning_rate": 3.657342170728737e-05,
      "loss": 0.9118,
      "step": 15440
    },
    {
      "epoch": 0.8062937884065913,
      "grad_norm": 4.847650527954102,
      "learning_rate": 3.656472348346468e-05,
      "loss": 0.9904,
      "step": 15450
    },
    {
      "epoch": 0.8068156614087961,
      "grad_norm": 4.884047508239746,
      "learning_rate": 3.655602525964199e-05,
      "loss": 0.8917,
      "step": 15460
    },
    {
      "epoch": 0.8073375344110011,
      "grad_norm": 3.740830421447754,
      "learning_rate": 3.654732703581929e-05,
      "loss": 1.0136,
      "step": 15470
    },
    {
      "epoch": 0.8078594074132059,
      "grad_norm": 4.035199165344238,
      "learning_rate": 3.6538628811996594e-05,
      "loss": 0.938,
      "step": 15480
    },
    {
      "epoch": 0.8083812804154109,
      "grad_norm": 4.2030158042907715,
      "learning_rate": 3.652993058817389e-05,
      "loss": 0.9212,
      "step": 15490
    },
    {
      "epoch": 0.8089031534176159,
      "grad_norm": 4.306994915008545,
      "learning_rate": 3.65212323643512e-05,
      "loss": 1.0437,
      "step": 15500
    },
    {
      "epoch": 0.8094250264198207,
      "grad_norm": 4.748897552490234,
      "learning_rate": 3.6512534140528505e-05,
      "loss": 0.9534,
      "step": 15510
    },
    {
      "epoch": 0.8099468994220257,
      "grad_norm": 4.77669620513916,
      "learning_rate": 3.650383591670581e-05,
      "loss": 0.999,
      "step": 15520
    },
    {
      "epoch": 0.8104687724242305,
      "grad_norm": 4.198853492736816,
      "learning_rate": 3.649513769288311e-05,
      "loss": 0.9822,
      "step": 15530
    },
    {
      "epoch": 0.8109906454264355,
      "grad_norm": 4.29819393157959,
      "learning_rate": 3.6486439469060416e-05,
      "loss": 0.9497,
      "step": 15540
    },
    {
      "epoch": 0.8115125184286404,
      "grad_norm": 4.969133377075195,
      "learning_rate": 3.6477741245237726e-05,
      "loss": 1.0812,
      "step": 15550
    },
    {
      "epoch": 0.8120343914308453,
      "grad_norm": 3.943925380706787,
      "learning_rate": 3.646904302141503e-05,
      "loss": 1.0168,
      "step": 15560
    },
    {
      "epoch": 0.8125562644330502,
      "grad_norm": 3.7152934074401855,
      "learning_rate": 3.646034479759233e-05,
      "loss": 1.0139,
      "step": 15570
    },
    {
      "epoch": 0.8130781374352551,
      "grad_norm": 4.777909278869629,
      "learning_rate": 3.6451646573769637e-05,
      "loss": 0.8814,
      "step": 15580
    },
    {
      "epoch": 0.81360001043746,
      "grad_norm": 5.493232727050781,
      "learning_rate": 3.644294834994694e-05,
      "loss": 0.9126,
      "step": 15590
    },
    {
      "epoch": 0.814121883439665,
      "grad_norm": 4.135010719299316,
      "learning_rate": 3.643425012612425e-05,
      "loss": 0.9457,
      "step": 15600
    },
    {
      "epoch": 0.8146437564418699,
      "grad_norm": 4.8101806640625,
      "learning_rate": 3.6425551902301554e-05,
      "loss": 0.999,
      "step": 15610
    },
    {
      "epoch": 0.8151656294440748,
      "grad_norm": 4.766963005065918,
      "learning_rate": 3.641685367847886e-05,
      "loss": 1.0467,
      "step": 15620
    },
    {
      "epoch": 0.8156875024462797,
      "grad_norm": 5.198890209197998,
      "learning_rate": 3.640815545465616e-05,
      "loss": 0.9227,
      "step": 15630
    },
    {
      "epoch": 0.8162093754484846,
      "grad_norm": 3.9693081378936768,
      "learning_rate": 3.6399457230833465e-05,
      "loss": 0.9029,
      "step": 15640
    },
    {
      "epoch": 0.8167312484506896,
      "grad_norm": 3.7716164588928223,
      "learning_rate": 3.6390759007010775e-05,
      "loss": 0.9244,
      "step": 15650
    },
    {
      "epoch": 0.8172531214528944,
      "grad_norm": 3.882425546646118,
      "learning_rate": 3.638206078318808e-05,
      "loss": 0.9203,
      "step": 15660
    },
    {
      "epoch": 0.8177749944550994,
      "grad_norm": 4.8220343589782715,
      "learning_rate": 3.637336255936538e-05,
      "loss": 1.0054,
      "step": 15670
    },
    {
      "epoch": 0.8182968674573042,
      "grad_norm": 4.867295265197754,
      "learning_rate": 3.6364664335542686e-05,
      "loss": 0.9812,
      "step": 15680
    },
    {
      "epoch": 0.8188187404595092,
      "grad_norm": 4.37408971786499,
      "learning_rate": 3.635596611171999e-05,
      "loss": 1.0298,
      "step": 15690
    },
    {
      "epoch": 0.819340613461714,
      "grad_norm": 4.399059772491455,
      "learning_rate": 3.634726788789729e-05,
      "loss": 0.9948,
      "step": 15700
    },
    {
      "epoch": 0.819862486463919,
      "grad_norm": 4.8574442863464355,
      "learning_rate": 3.6338569664074596e-05,
      "loss": 0.9045,
      "step": 15710
    },
    {
      "epoch": 0.8203843594661239,
      "grad_norm": 4.15709924697876,
      "learning_rate": 3.63298714402519e-05,
      "loss": 0.8846,
      "step": 15720
    },
    {
      "epoch": 0.8209062324683288,
      "grad_norm": 4.23260498046875,
      "learning_rate": 3.63211732164292e-05,
      "loss": 0.984,
      "step": 15730
    },
    {
      "epoch": 0.8214281054705338,
      "grad_norm": 4.362105846405029,
      "learning_rate": 3.631247499260651e-05,
      "loss": 0.9784,
      "step": 15740
    },
    {
      "epoch": 0.8219499784727387,
      "grad_norm": 4.5287933349609375,
      "learning_rate": 3.630377676878382e-05,
      "loss": 0.9579,
      "step": 15750
    },
    {
      "epoch": 0.8224718514749436,
      "grad_norm": 3.679133892059326,
      "learning_rate": 3.629507854496112e-05,
      "loss": 1.0275,
      "step": 15760
    },
    {
      "epoch": 0.8229937244771485,
      "grad_norm": 4.223835468292236,
      "learning_rate": 3.6286380321138424e-05,
      "loss": 0.9743,
      "step": 15770
    },
    {
      "epoch": 0.8235155974793534,
      "grad_norm": 4.155837535858154,
      "learning_rate": 3.627768209731573e-05,
      "loss": 0.8783,
      "step": 15780
    },
    {
      "epoch": 0.8240374704815583,
      "grad_norm": 5.016645431518555,
      "learning_rate": 3.626898387349303e-05,
      "loss": 0.927,
      "step": 15790
    },
    {
      "epoch": 0.8245593434837633,
      "grad_norm": 4.546300888061523,
      "learning_rate": 3.626028564967034e-05,
      "loss": 0.9792,
      "step": 15800
    },
    {
      "epoch": 0.8250812164859681,
      "grad_norm": 4.137806415557861,
      "learning_rate": 3.6251587425847645e-05,
      "loss": 0.9158,
      "step": 15810
    },
    {
      "epoch": 0.8256030894881731,
      "grad_norm": 3.806532621383667,
      "learning_rate": 3.624288920202495e-05,
      "loss": 0.963,
      "step": 15820
    },
    {
      "epoch": 0.8261249624903779,
      "grad_norm": 4.899830341339111,
      "learning_rate": 3.623419097820225e-05,
      "loss": 0.9516,
      "step": 15830
    },
    {
      "epoch": 0.8266468354925829,
      "grad_norm": 5.050591468811035,
      "learning_rate": 3.6225492754379556e-05,
      "loss": 0.875,
      "step": 15840
    },
    {
      "epoch": 0.8271687084947877,
      "grad_norm": 4.017223358154297,
      "learning_rate": 3.6216794530556866e-05,
      "loss": 0.9257,
      "step": 15850
    },
    {
      "epoch": 0.8276905814969927,
      "grad_norm": 4.631962776184082,
      "learning_rate": 3.620809630673417e-05,
      "loss": 0.8715,
      "step": 15860
    },
    {
      "epoch": 0.8282124544991977,
      "grad_norm": 4.113039493560791,
      "learning_rate": 3.619939808291147e-05,
      "loss": 0.9766,
      "step": 15870
    },
    {
      "epoch": 0.8287343275014025,
      "grad_norm": 4.382024765014648,
      "learning_rate": 3.619069985908878e-05,
      "loss": 1.0199,
      "step": 15880
    },
    {
      "epoch": 0.8292562005036075,
      "grad_norm": 4.792873859405518,
      "learning_rate": 3.618200163526608e-05,
      "loss": 0.9465,
      "step": 15890
    },
    {
      "epoch": 0.8297780735058123,
      "grad_norm": 4.509963512420654,
      "learning_rate": 3.617330341144339e-05,
      "loss": 0.8981,
      "step": 15900
    },
    {
      "epoch": 0.8302999465080173,
      "grad_norm": 3.803313970565796,
      "learning_rate": 3.616460518762069e-05,
      "loss": 0.9468,
      "step": 15910
    },
    {
      "epoch": 0.8308218195102222,
      "grad_norm": 3.390712022781372,
      "learning_rate": 3.615590696379799e-05,
      "loss": 0.8878,
      "step": 15920
    },
    {
      "epoch": 0.8313436925124271,
      "grad_norm": 3.9948651790618896,
      "learning_rate": 3.6147208739975295e-05,
      "loss": 1.0029,
      "step": 15930
    },
    {
      "epoch": 0.831865565514632,
      "grad_norm": 4.519622325897217,
      "learning_rate": 3.6138510516152605e-05,
      "loss": 0.9621,
      "step": 15940
    },
    {
      "epoch": 0.8323874385168369,
      "grad_norm": 3.4961049556732178,
      "learning_rate": 3.6130682114712175e-05,
      "loss": 0.9978,
      "step": 15950
    },
    {
      "epoch": 0.8329093115190418,
      "grad_norm": 4.205027103424072,
      "learning_rate": 3.6121983890889485e-05,
      "loss": 0.9624,
      "step": 15960
    },
    {
      "epoch": 0.8334311845212468,
      "grad_norm": 4.500489711761475,
      "learning_rate": 3.611328566706679e-05,
      "loss": 0.9683,
      "step": 15970
    },
    {
      "epoch": 0.8339530575234516,
      "grad_norm": 4.286945343017578,
      "learning_rate": 3.610458744324409e-05,
      "loss": 0.9063,
      "step": 15980
    },
    {
      "epoch": 0.8344749305256566,
      "grad_norm": 4.118316650390625,
      "learning_rate": 3.6095889219421396e-05,
      "loss": 0.9501,
      "step": 15990
    },
    {
      "epoch": 0.8349968035278615,
      "grad_norm": 4.178095817565918,
      "learning_rate": 3.60871909955987e-05,
      "loss": 0.9196,
      "step": 16000
    },
    {
      "epoch": 0.8355186765300664,
      "grad_norm": 4.070044040679932,
      "learning_rate": 3.607849277177601e-05,
      "loss": 0.9532,
      "step": 16010
    },
    {
      "epoch": 0.8360405495322714,
      "grad_norm": 3.626641035079956,
      "learning_rate": 3.606979454795331e-05,
      "loss": 0.899,
      "step": 16020
    },
    {
      "epoch": 0.8365624225344762,
      "grad_norm": 4.299939155578613,
      "learning_rate": 3.606109632413062e-05,
      "loss": 0.972,
      "step": 16030
    },
    {
      "epoch": 0.8370842955366812,
      "grad_norm": 4.774345874786377,
      "learning_rate": 3.605239810030792e-05,
      "loss": 0.9833,
      "step": 16040
    },
    {
      "epoch": 0.837606168538886,
      "grad_norm": 4.579228401184082,
      "learning_rate": 3.6043699876485224e-05,
      "loss": 0.9744,
      "step": 16050
    },
    {
      "epoch": 0.838128041541091,
      "grad_norm": 3.668844223022461,
      "learning_rate": 3.603500165266253e-05,
      "loss": 0.9662,
      "step": 16060
    },
    {
      "epoch": 0.8386499145432958,
      "grad_norm": 5.062201023101807,
      "learning_rate": 3.602630342883983e-05,
      "loss": 0.9205,
      "step": 16070
    },
    {
      "epoch": 0.8391717875455008,
      "grad_norm": 4.551519393920898,
      "learning_rate": 3.6017605205017134e-05,
      "loss": 1.0192,
      "step": 16080
    },
    {
      "epoch": 0.8396936605477057,
      "grad_norm": 4.959543228149414,
      "learning_rate": 3.600890698119444e-05,
      "loss": 0.9183,
      "step": 16090
    },
    {
      "epoch": 0.8402155335499106,
      "grad_norm": 4.567152500152588,
      "learning_rate": 3.600020875737175e-05,
      "loss": 0.9613,
      "step": 16100
    },
    {
      "epoch": 0.8407374065521156,
      "grad_norm": 4.20648717880249,
      "learning_rate": 3.599151053354905e-05,
      "loss": 0.9409,
      "step": 16110
    },
    {
      "epoch": 0.8412592795543204,
      "grad_norm": 3.822348117828369,
      "learning_rate": 3.5982812309726355e-05,
      "loss": 0.9779,
      "step": 16120
    },
    {
      "epoch": 0.8417811525565254,
      "grad_norm": 4.376189708709717,
      "learning_rate": 3.597411408590366e-05,
      "loss": 1.0283,
      "step": 16130
    },
    {
      "epoch": 0.8423030255587303,
      "grad_norm": 4.580440044403076,
      "learning_rate": 3.596541586208096e-05,
      "loss": 0.9613,
      "step": 16140
    },
    {
      "epoch": 0.8428248985609352,
      "grad_norm": 3.822655439376831,
      "learning_rate": 3.5956717638258266e-05,
      "loss": 0.9412,
      "step": 16150
    },
    {
      "epoch": 0.8433467715631401,
      "grad_norm": 4.570822238922119,
      "learning_rate": 3.5948019414435576e-05,
      "loss": 0.9132,
      "step": 16160
    },
    {
      "epoch": 0.843868644565345,
      "grad_norm": 4.838237762451172,
      "learning_rate": 3.593932119061288e-05,
      "loss": 0.9361,
      "step": 16170
    },
    {
      "epoch": 0.8443905175675499,
      "grad_norm": 4.638830661773682,
      "learning_rate": 3.5930622966790184e-05,
      "loss": 0.9882,
      "step": 16180
    },
    {
      "epoch": 0.8449123905697549,
      "grad_norm": 4.451633453369141,
      "learning_rate": 3.592192474296749e-05,
      "loss": 1.0321,
      "step": 16190
    },
    {
      "epoch": 0.8454342635719597,
      "grad_norm": 4.834510326385498,
      "learning_rate": 3.591322651914479e-05,
      "loss": 0.9731,
      "step": 16200
    },
    {
      "epoch": 0.8459561365741647,
      "grad_norm": 4.027637481689453,
      "learning_rate": 3.59045282953221e-05,
      "loss": 1.0948,
      "step": 16210
    },
    {
      "epoch": 0.8464780095763695,
      "grad_norm": 4.139930725097656,
      "learning_rate": 3.5895830071499404e-05,
      "loss": 0.9419,
      "step": 16220
    },
    {
      "epoch": 0.8469998825785745,
      "grad_norm": 4.652525901794434,
      "learning_rate": 3.588713184767671e-05,
      "loss": 1.0739,
      "step": 16230
    },
    {
      "epoch": 0.8475217555807795,
      "grad_norm": 3.991380453109741,
      "learning_rate": 3.587843362385401e-05,
      "loss": 0.9504,
      "step": 16240
    },
    {
      "epoch": 0.8480436285829843,
      "grad_norm": 4.7300591468811035,
      "learning_rate": 3.5869735400031315e-05,
      "loss": 1.0641,
      "step": 16250
    },
    {
      "epoch": 0.8485655015851893,
      "grad_norm": 3.883671522140503,
      "learning_rate": 3.586103717620862e-05,
      "loss": 0.9003,
      "step": 16260
    },
    {
      "epoch": 0.8490873745873941,
      "grad_norm": 4.022228717803955,
      "learning_rate": 3.585233895238592e-05,
      "loss": 0.9767,
      "step": 16270
    },
    {
      "epoch": 0.8496092475895991,
      "grad_norm": 4.636504650115967,
      "learning_rate": 3.5843640728563226e-05,
      "loss": 0.9409,
      "step": 16280
    },
    {
      "epoch": 0.850131120591804,
      "grad_norm": 4.473073959350586,
      "learning_rate": 3.583494250474053e-05,
      "loss": 0.9044,
      "step": 16290
    },
    {
      "epoch": 0.8506529935940089,
      "grad_norm": 4.8514556884765625,
      "learning_rate": 3.582624428091784e-05,
      "loss": 0.9337,
      "step": 16300
    },
    {
      "epoch": 0.8511748665962138,
      "grad_norm": 4.083102226257324,
      "learning_rate": 3.581754605709514e-05,
      "loss": 0.9538,
      "step": 16310
    },
    {
      "epoch": 0.8516967395984187,
      "grad_norm": 4.476700782775879,
      "learning_rate": 3.580884783327245e-05,
      "loss": 1.0804,
      "step": 16320
    },
    {
      "epoch": 0.8522186126006236,
      "grad_norm": 3.2424843311309814,
      "learning_rate": 3.580014960944975e-05,
      "loss": 0.9159,
      "step": 16330
    },
    {
      "epoch": 0.8527404856028286,
      "grad_norm": 3.5531444549560547,
      "learning_rate": 3.5791451385627054e-05,
      "loss": 0.9263,
      "step": 16340
    },
    {
      "epoch": 0.8532623586050334,
      "grad_norm": 4.696898460388184,
      "learning_rate": 3.5782753161804364e-05,
      "loss": 0.927,
      "step": 16350
    },
    {
      "epoch": 0.8537842316072384,
      "grad_norm": 4.673110008239746,
      "learning_rate": 3.577405493798167e-05,
      "loss": 1.0274,
      "step": 16360
    },
    {
      "epoch": 0.8543061046094433,
      "grad_norm": 3.9336793422698975,
      "learning_rate": 3.576535671415897e-05,
      "loss": 1.0423,
      "step": 16370
    },
    {
      "epoch": 0.8548279776116482,
      "grad_norm": 3.6364598274230957,
      "learning_rate": 3.5756658490336275e-05,
      "loss": 1.0732,
      "step": 16380
    },
    {
      "epoch": 0.8553498506138532,
      "grad_norm": 4.109457492828369,
      "learning_rate": 3.574796026651358e-05,
      "loss": 0.9395,
      "step": 16390
    },
    {
      "epoch": 0.855871723616058,
      "grad_norm": 4.557648658752441,
      "learning_rate": 3.573926204269088e-05,
      "loss": 0.945,
      "step": 16400
    },
    {
      "epoch": 0.856393596618263,
      "grad_norm": 4.192256927490234,
      "learning_rate": 3.573056381886819e-05,
      "loss": 1.0747,
      "step": 16410
    },
    {
      "epoch": 0.8569154696204678,
      "grad_norm": 5.225505352020264,
      "learning_rate": 3.5721865595045496e-05,
      "loss": 1.0273,
      "step": 16420
    },
    {
      "epoch": 0.8574373426226728,
      "grad_norm": 4.6643595695495605,
      "learning_rate": 3.57131673712228e-05,
      "loss": 1.003,
      "step": 16430
    },
    {
      "epoch": 0.8579592156248776,
      "grad_norm": 3.9571337699890137,
      "learning_rate": 3.57044691474001e-05,
      "loss": 0.9799,
      "step": 16440
    },
    {
      "epoch": 0.8584810886270826,
      "grad_norm": 4.564661502838135,
      "learning_rate": 3.5695770923577406e-05,
      "loss": 0.8793,
      "step": 16450
    },
    {
      "epoch": 0.8590029616292875,
      "grad_norm": 4.450599193572998,
      "learning_rate": 3.568707269975472e-05,
      "loss": 1.0264,
      "step": 16460
    },
    {
      "epoch": 0.8595248346314924,
      "grad_norm": 3.6603493690490723,
      "learning_rate": 3.5678374475932014e-05,
      "loss": 0.9081,
      "step": 16470
    },
    {
      "epoch": 0.8600467076336974,
      "grad_norm": 4.550339698791504,
      "learning_rate": 3.566967625210932e-05,
      "loss": 1.0406,
      "step": 16480
    },
    {
      "epoch": 0.8605685806359022,
      "grad_norm": 5.065789699554443,
      "learning_rate": 3.566097802828662e-05,
      "loss": 0.8915,
      "step": 16490
    },
    {
      "epoch": 0.8610904536381072,
      "grad_norm": 4.391520977020264,
      "learning_rate": 3.565227980446393e-05,
      "loss": 0.8671,
      "step": 16500
    },
    {
      "epoch": 0.8616123266403121,
      "grad_norm": 3.9073455333709717,
      "learning_rate": 3.5643581580641235e-05,
      "loss": 0.884,
      "step": 16510
    },
    {
      "epoch": 0.862134199642517,
      "grad_norm": 4.069136619567871,
      "learning_rate": 3.563488335681854e-05,
      "loss": 0.8903,
      "step": 16520
    },
    {
      "epoch": 0.8626560726447219,
      "grad_norm": 3.4019834995269775,
      "learning_rate": 3.562618513299584e-05,
      "loss": 0.9573,
      "step": 16530
    },
    {
      "epoch": 0.8631779456469268,
      "grad_norm": 4.365748405456543,
      "learning_rate": 3.5617486909173145e-05,
      "loss": 0.9163,
      "step": 16540
    },
    {
      "epoch": 0.8636998186491317,
      "grad_norm": 5.05256986618042,
      "learning_rate": 3.5608788685350455e-05,
      "loss": 1.0084,
      "step": 16550
    },
    {
      "epoch": 0.8642216916513367,
      "grad_norm": 4.048901557922363,
      "learning_rate": 3.560009046152776e-05,
      "loss": 0.9804,
      "step": 16560
    },
    {
      "epoch": 0.8647435646535415,
      "grad_norm": 4.515707492828369,
      "learning_rate": 3.559139223770506e-05,
      "loss": 1.0087,
      "step": 16570
    },
    {
      "epoch": 0.8652654376557465,
      "grad_norm": 4.360445499420166,
      "learning_rate": 3.5582694013882366e-05,
      "loss": 0.9271,
      "step": 16580
    },
    {
      "epoch": 0.8657873106579513,
      "grad_norm": 3.971034526824951,
      "learning_rate": 3.557399579005967e-05,
      "loss": 0.9496,
      "step": 16590
    },
    {
      "epoch": 0.8663091836601563,
      "grad_norm": 3.6218581199645996,
      "learning_rate": 3.556529756623698e-05,
      "loss": 0.9249,
      "step": 16600
    },
    {
      "epoch": 0.8668310566623613,
      "grad_norm": 4.47128963470459,
      "learning_rate": 3.5556599342414284e-05,
      "loss": 1.0158,
      "step": 16610
    },
    {
      "epoch": 0.8673529296645661,
      "grad_norm": 5.005715370178223,
      "learning_rate": 3.554790111859159e-05,
      "loss": 1.0604,
      "step": 16620
    },
    {
      "epoch": 0.8678748026667711,
      "grad_norm": 4.181454658508301,
      "learning_rate": 3.553920289476889e-05,
      "loss": 0.966,
      "step": 16630
    },
    {
      "epoch": 0.8683966756689759,
      "grad_norm": 4.413832187652588,
      "learning_rate": 3.5530504670946194e-05,
      "loss": 0.9561,
      "step": 16640
    },
    {
      "epoch": 0.8689185486711809,
      "grad_norm": 5.457117080688477,
      "learning_rate": 3.5521806447123505e-05,
      "loss": 0.9493,
      "step": 16650
    },
    {
      "epoch": 0.8694404216733858,
      "grad_norm": 3.8361740112304688,
      "learning_rate": 3.551310822330081e-05,
      "loss": 0.9479,
      "step": 16660
    },
    {
      "epoch": 0.8699622946755907,
      "grad_norm": 5.028188228607178,
      "learning_rate": 3.550440999947811e-05,
      "loss": 0.9953,
      "step": 16670
    },
    {
      "epoch": 0.8704841676777956,
      "grad_norm": 4.801181793212891,
      "learning_rate": 3.549571177565541e-05,
      "loss": 0.9414,
      "step": 16680
    },
    {
      "epoch": 0.8710060406800005,
      "grad_norm": 4.624977111816406,
      "learning_rate": 3.548701355183271e-05,
      "loss": 0.8499,
      "step": 16690
    },
    {
      "epoch": 0.8715279136822054,
      "grad_norm": 4.567729949951172,
      "learning_rate": 3.547831532801002e-05,
      "loss": 0.935,
      "step": 16700
    },
    {
      "epoch": 0.8720497866844104,
      "grad_norm": 4.329297065734863,
      "learning_rate": 3.5469617104187326e-05,
      "loss": 1.0397,
      "step": 16710
    },
    {
      "epoch": 0.8725716596866152,
      "grad_norm": 4.761486530303955,
      "learning_rate": 3.546091888036463e-05,
      "loss": 0.957,
      "step": 16720
    },
    {
      "epoch": 0.8730935326888202,
      "grad_norm": 5.364248275756836,
      "learning_rate": 3.545222065654193e-05,
      "loss": 0.8896,
      "step": 16730
    },
    {
      "epoch": 0.8736154056910251,
      "grad_norm": 5.241560935974121,
      "learning_rate": 3.5443522432719236e-05,
      "loss": 0.9087,
      "step": 16740
    },
    {
      "epoch": 0.87413727869323,
      "grad_norm": 5.19167423248291,
      "learning_rate": 3.543482420889655e-05,
      "loss": 1.0118,
      "step": 16750
    },
    {
      "epoch": 0.874659151695435,
      "grad_norm": 4.09034538269043,
      "learning_rate": 3.542612598507385e-05,
      "loss": 0.9294,
      "step": 16760
    },
    {
      "epoch": 0.8751810246976398,
      "grad_norm": 4.912235736846924,
      "learning_rate": 3.5417427761251154e-05,
      "loss": 0.9638,
      "step": 16770
    },
    {
      "epoch": 0.8757028976998448,
      "grad_norm": 4.93720006942749,
      "learning_rate": 3.540872953742846e-05,
      "loss": 0.9504,
      "step": 16780
    },
    {
      "epoch": 0.8762247707020496,
      "grad_norm": 4.816463470458984,
      "learning_rate": 3.540003131360576e-05,
      "loss": 0.9492,
      "step": 16790
    },
    {
      "epoch": 0.8767466437042546,
      "grad_norm": 4.863452434539795,
      "learning_rate": 3.539133308978307e-05,
      "loss": 0.9725,
      "step": 16800
    },
    {
      "epoch": 0.8772685167064594,
      "grad_norm": 3.914443254470825,
      "learning_rate": 3.5382634865960375e-05,
      "loss": 0.8496,
      "step": 16810
    },
    {
      "epoch": 0.8777903897086644,
      "grad_norm": 3.8170981407165527,
      "learning_rate": 3.537393664213768e-05,
      "loss": 0.8899,
      "step": 16820
    },
    {
      "epoch": 0.8783122627108693,
      "grad_norm": 3.9982149600982666,
      "learning_rate": 3.536523841831498e-05,
      "loss": 0.8684,
      "step": 16830
    },
    {
      "epoch": 0.8788341357130742,
      "grad_norm": 3.810898542404175,
      "learning_rate": 3.5356540194492285e-05,
      "loss": 0.8678,
      "step": 16840
    },
    {
      "epoch": 0.8793560087152792,
      "grad_norm": 5.057525634765625,
      "learning_rate": 3.5347841970669596e-05,
      "loss": 0.9325,
      "step": 16850
    },
    {
      "epoch": 0.879877881717484,
      "grad_norm": 4.444547653198242,
      "learning_rate": 3.53391437468469e-05,
      "loss": 0.9218,
      "step": 16860
    },
    {
      "epoch": 0.880399754719689,
      "grad_norm": 4.588117599487305,
      "learning_rate": 3.53304455230242e-05,
      "loss": 0.827,
      "step": 16870
    },
    {
      "epoch": 0.8809216277218939,
      "grad_norm": 3.9119632244110107,
      "learning_rate": 3.5321747299201506e-05,
      "loss": 0.9446,
      "step": 16880
    },
    {
      "epoch": 0.8814435007240988,
      "grad_norm": 4.6088738441467285,
      "learning_rate": 3.531304907537881e-05,
      "loss": 0.9918,
      "step": 16890
    },
    {
      "epoch": 0.8819653737263037,
      "grad_norm": 4.345151901245117,
      "learning_rate": 3.5304350851556114e-05,
      "loss": 0.9419,
      "step": 16900
    },
    {
      "epoch": 0.8824872467285086,
      "grad_norm": 4.376438140869141,
      "learning_rate": 3.529565262773342e-05,
      "loss": 0.9322,
      "step": 16910
    },
    {
      "epoch": 0.8830091197307135,
      "grad_norm": 4.218664646148682,
      "learning_rate": 3.528695440391072e-05,
      "loss": 1.0528,
      "step": 16920
    },
    {
      "epoch": 0.8835309927329185,
      "grad_norm": 4.6205973625183105,
      "learning_rate": 3.5278256180088024e-05,
      "loss": 0.965,
      "step": 16930
    },
    {
      "epoch": 0.8840528657351233,
      "grad_norm": 3.3583993911743164,
      "learning_rate": 3.526955795626533e-05,
      "loss": 0.9328,
      "step": 16940
    },
    {
      "epoch": 0.8845747387373283,
      "grad_norm": 4.526172161102295,
      "learning_rate": 3.526085973244264e-05,
      "loss": 0.9736,
      "step": 16950
    },
    {
      "epoch": 0.8850966117395331,
      "grad_norm": 5.226861953735352,
      "learning_rate": 3.525216150861994e-05,
      "loss": 0.9888,
      "step": 16960
    },
    {
      "epoch": 0.8856184847417381,
      "grad_norm": 4.662356853485107,
      "learning_rate": 3.5243463284797245e-05,
      "loss": 0.9045,
      "step": 16970
    },
    {
      "epoch": 0.8861403577439431,
      "grad_norm": 3.6462111473083496,
      "learning_rate": 3.523476506097455e-05,
      "loss": 0.9264,
      "step": 16980
    },
    {
      "epoch": 0.8866622307461479,
      "grad_norm": 4.944679260253906,
      "learning_rate": 3.522606683715185e-05,
      "loss": 0.9391,
      "step": 16990
    },
    {
      "epoch": 0.8871841037483529,
      "grad_norm": 5.452703952789307,
      "learning_rate": 3.521736861332916e-05,
      "loss": 0.9661,
      "step": 17000
    },
    {
      "epoch": 0.8877059767505577,
      "grad_norm": 4.99844217300415,
      "learning_rate": 3.5208670389506466e-05,
      "loss": 1.0451,
      "step": 17010
    },
    {
      "epoch": 0.8882278497527627,
      "grad_norm": 3.5960376262664795,
      "learning_rate": 3.519997216568377e-05,
      "loss": 0.9445,
      "step": 17020
    },
    {
      "epoch": 0.8887497227549676,
      "grad_norm": 5.072401523590088,
      "learning_rate": 3.519127394186107e-05,
      "loss": 0.9772,
      "step": 17030
    },
    {
      "epoch": 0.8892715957571725,
      "grad_norm": 3.9916586875915527,
      "learning_rate": 3.518257571803838e-05,
      "loss": 0.8869,
      "step": 17040
    },
    {
      "epoch": 0.8897934687593774,
      "grad_norm": 4.4755377769470215,
      "learning_rate": 3.517387749421569e-05,
      "loss": 0.9822,
      "step": 17050
    },
    {
      "epoch": 0.8903153417615823,
      "grad_norm": 4.810412883758545,
      "learning_rate": 3.516517927039299e-05,
      "loss": 0.916,
      "step": 17060
    },
    {
      "epoch": 0.8908372147637872,
      "grad_norm": 4.6748504638671875,
      "learning_rate": 3.5156481046570294e-05,
      "loss": 0.9248,
      "step": 17070
    },
    {
      "epoch": 0.8913590877659922,
      "grad_norm": 4.111107349395752,
      "learning_rate": 3.51477828227476e-05,
      "loss": 0.9681,
      "step": 17080
    },
    {
      "epoch": 0.891880960768197,
      "grad_norm": 4.726551055908203,
      "learning_rate": 3.51390845989249e-05,
      "loss": 0.9351,
      "step": 17090
    },
    {
      "epoch": 0.892402833770402,
      "grad_norm": 4.561950206756592,
      "learning_rate": 3.5130386375102205e-05,
      "loss": 0.9689,
      "step": 17100
    },
    {
      "epoch": 0.8929247067726069,
      "grad_norm": 4.7888922691345215,
      "learning_rate": 3.512168815127951e-05,
      "loss": 0.998,
      "step": 17110
    },
    {
      "epoch": 0.8934465797748118,
      "grad_norm": 3.755718469619751,
      "learning_rate": 3.511298992745681e-05,
      "loss": 0.9388,
      "step": 17120
    },
    {
      "epoch": 0.8939684527770168,
      "grad_norm": 4.545248031616211,
      "learning_rate": 3.5104291703634116e-05,
      "loss": 1.0417,
      "step": 17130
    },
    {
      "epoch": 0.8944903257792216,
      "grad_norm": 4.8267130851745605,
      "learning_rate": 3.5095593479811426e-05,
      "loss": 0.8821,
      "step": 17140
    },
    {
      "epoch": 0.8950121987814266,
      "grad_norm": 3.69416880607605,
      "learning_rate": 3.508689525598873e-05,
      "loss": 0.9742,
      "step": 17150
    },
    {
      "epoch": 0.8955340717836314,
      "grad_norm": 4.074703216552734,
      "learning_rate": 3.507819703216603e-05,
      "loss": 0.9635,
      "step": 17160
    },
    {
      "epoch": 0.8960559447858364,
      "grad_norm": 4.4739251136779785,
      "learning_rate": 3.5069498808343336e-05,
      "loss": 1.0071,
      "step": 17170
    },
    {
      "epoch": 0.8965778177880412,
      "grad_norm": 4.580466270446777,
      "learning_rate": 3.506080058452064e-05,
      "loss": 1.0234,
      "step": 17180
    },
    {
      "epoch": 0.8970996907902462,
      "grad_norm": 3.950220823287964,
      "learning_rate": 3.505210236069795e-05,
      "loss": 0.8899,
      "step": 17190
    },
    {
      "epoch": 0.8976215637924511,
      "grad_norm": 4.440337181091309,
      "learning_rate": 3.5043404136875254e-05,
      "loss": 1.0044,
      "step": 17200
    },
    {
      "epoch": 0.898143436794656,
      "grad_norm": 4.957957744598389,
      "learning_rate": 3.503470591305256e-05,
      "loss": 1.003,
      "step": 17210
    },
    {
      "epoch": 0.898665309796861,
      "grad_norm": 4.083487033843994,
      "learning_rate": 3.502600768922986e-05,
      "loss": 0.9607,
      "step": 17220
    },
    {
      "epoch": 0.8991871827990658,
      "grad_norm": 5.525565147399902,
      "learning_rate": 3.5017309465407165e-05,
      "loss": 0.9912,
      "step": 17230
    },
    {
      "epoch": 0.8997090558012708,
      "grad_norm": 4.903099536895752,
      "learning_rate": 3.500861124158447e-05,
      "loss": 0.95,
      "step": 17240
    },
    {
      "epoch": 0.9002309288034757,
      "grad_norm": 4.7376322746276855,
      "learning_rate": 3.499991301776178e-05,
      "loss": 0.9901,
      "step": 17250
    },
    {
      "epoch": 0.9007528018056806,
      "grad_norm": 3.9255032539367676,
      "learning_rate": 3.499121479393908e-05,
      "loss": 0.9121,
      "step": 17260
    },
    {
      "epoch": 0.9012746748078855,
      "grad_norm": 4.490588665008545,
      "learning_rate": 3.4982516570116386e-05,
      "loss": 0.9843,
      "step": 17270
    },
    {
      "epoch": 0.9017965478100904,
      "grad_norm": 5.0246686935424805,
      "learning_rate": 3.497381834629369e-05,
      "loss": 1.0275,
      "step": 17280
    },
    {
      "epoch": 0.9023184208122953,
      "grad_norm": 4.715975761413574,
      "learning_rate": 3.496512012247099e-05,
      "loss": 0.9299,
      "step": 17290
    },
    {
      "epoch": 0.9028402938145003,
      "grad_norm": 3.8667235374450684,
      "learning_rate": 3.4956421898648296e-05,
      "loss": 0.9758,
      "step": 17300
    },
    {
      "epoch": 0.9033621668167051,
      "grad_norm": 5.474824905395508,
      "learning_rate": 3.49477236748256e-05,
      "loss": 0.8952,
      "step": 17310
    },
    {
      "epoch": 0.9038840398189101,
      "grad_norm": 4.142940998077393,
      "learning_rate": 3.49390254510029e-05,
      "loss": 0.9292,
      "step": 17320
    },
    {
      "epoch": 0.9044059128211149,
      "grad_norm": 4.455004692077637,
      "learning_rate": 3.493032722718021e-05,
      "loss": 0.9819,
      "step": 17330
    },
    {
      "epoch": 0.9049277858233199,
      "grad_norm": 4.108315467834473,
      "learning_rate": 3.492162900335752e-05,
      "loss": 0.9168,
      "step": 17340
    },
    {
      "epoch": 0.9054496588255249,
      "grad_norm": 4.173915386199951,
      "learning_rate": 3.491293077953482e-05,
      "loss": 0.876,
      "step": 17350
    },
    {
      "epoch": 0.9059715318277297,
      "grad_norm": 3.8317923545837402,
      "learning_rate": 3.4904232555712124e-05,
      "loss": 0.9297,
      "step": 17360
    },
    {
      "epoch": 0.9064934048299347,
      "grad_norm": 4.127354145050049,
      "learning_rate": 3.489553433188943e-05,
      "loss": 0.8728,
      "step": 17370
    },
    {
      "epoch": 0.9070152778321395,
      "grad_norm": 4.554301738739014,
      "learning_rate": 3.488683610806673e-05,
      "loss": 0.9064,
      "step": 17380
    },
    {
      "epoch": 0.9075371508343445,
      "grad_norm": 4.769381523132324,
      "learning_rate": 3.487813788424404e-05,
      "loss": 0.9087,
      "step": 17390
    },
    {
      "epoch": 0.9080590238365494,
      "grad_norm": 4.286296844482422,
      "learning_rate": 3.4869439660421345e-05,
      "loss": 0.9175,
      "step": 17400
    },
    {
      "epoch": 0.9085808968387543,
      "grad_norm": 4.1056976318359375,
      "learning_rate": 3.486074143659865e-05,
      "loss": 1.0224,
      "step": 17410
    },
    {
      "epoch": 0.9091027698409592,
      "grad_norm": 4.8430585861206055,
      "learning_rate": 3.485204321277595e-05,
      "loss": 0.9739,
      "step": 17420
    },
    {
      "epoch": 0.9096246428431641,
      "grad_norm": 5.034581661224365,
      "learning_rate": 3.4843344988953256e-05,
      "loss": 1.0022,
      "step": 17430
    },
    {
      "epoch": 0.910146515845369,
      "grad_norm": 5.048388957977295,
      "learning_rate": 3.4834646765130566e-05,
      "loss": 0.9419,
      "step": 17440
    },
    {
      "epoch": 0.910668388847574,
      "grad_norm": 3.4313318729400635,
      "learning_rate": 3.482594854130787e-05,
      "loss": 0.9179,
      "step": 17450
    },
    {
      "epoch": 0.9111902618497788,
      "grad_norm": 3.5656914710998535,
      "learning_rate": 3.481725031748517e-05,
      "loss": 0.8944,
      "step": 17460
    },
    {
      "epoch": 0.9117121348519838,
      "grad_norm": 4.055779457092285,
      "learning_rate": 3.480855209366248e-05,
      "loss": 0.8715,
      "step": 17470
    },
    {
      "epoch": 0.9122340078541887,
      "grad_norm": 3.7568235397338867,
      "learning_rate": 3.479985386983978e-05,
      "loss": 0.9059,
      "step": 17480
    },
    {
      "epoch": 0.9127558808563936,
      "grad_norm": 3.896280288696289,
      "learning_rate": 3.4791155646017084e-05,
      "loss": 0.946,
      "step": 17490
    },
    {
      "epoch": 0.9132777538585986,
      "grad_norm": 5.480481147766113,
      "learning_rate": 3.4782457422194394e-05,
      "loss": 0.8811,
      "step": 17500
    },
    {
      "epoch": 0.9137996268608034,
      "grad_norm": 5.012757301330566,
      "learning_rate": 3.477375919837169e-05,
      "loss": 0.9342,
      "step": 17510
    },
    {
      "epoch": 0.9143214998630084,
      "grad_norm": 3.74371600151062,
      "learning_rate": 3.4765060974548995e-05,
      "loss": 0.9372,
      "step": 17520
    },
    {
      "epoch": 0.9148433728652132,
      "grad_norm": 4.403192043304443,
      "learning_rate": 3.47563627507263e-05,
      "loss": 1.0123,
      "step": 17530
    },
    {
      "epoch": 0.9153652458674182,
      "grad_norm": 4.9912614822387695,
      "learning_rate": 3.474766452690361e-05,
      "loss": 1.0231,
      "step": 17540
    },
    {
      "epoch": 0.915887118869623,
      "grad_norm": 4.352266311645508,
      "learning_rate": 3.473896630308091e-05,
      "loss": 0.968,
      "step": 17550
    },
    {
      "epoch": 0.916408991871828,
      "grad_norm": 3.910755157470703,
      "learning_rate": 3.4730268079258216e-05,
      "loss": 1.0078,
      "step": 17560
    },
    {
      "epoch": 0.9169308648740329,
      "grad_norm": 3.464947462081909,
      "learning_rate": 3.472156985543552e-05,
      "loss": 0.8612,
      "step": 17570
    },
    {
      "epoch": 0.9174527378762378,
      "grad_norm": 4.372321128845215,
      "learning_rate": 3.471287163161282e-05,
      "loss": 0.9922,
      "step": 17580
    },
    {
      "epoch": 0.9179746108784427,
      "grad_norm": 3.8202853202819824,
      "learning_rate": 3.470417340779013e-05,
      "loss": 0.914,
      "step": 17590
    },
    {
      "epoch": 0.9184964838806476,
      "grad_norm": 4.53436279296875,
      "learning_rate": 3.4695475183967437e-05,
      "loss": 0.975,
      "step": 17600
    },
    {
      "epoch": 0.9190183568828526,
      "grad_norm": 5.7158966064453125,
      "learning_rate": 3.468677696014474e-05,
      "loss": 0.9302,
      "step": 17610
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 4.611123561859131,
      "learning_rate": 3.4678078736322044e-05,
      "loss": 0.9084,
      "step": 17620
    },
    {
      "epoch": 0.9200621028872624,
      "grad_norm": 4.54705286026001,
      "learning_rate": 3.466938051249935e-05,
      "loss": 0.9451,
      "step": 17630
    },
    {
      "epoch": 0.9205839758894673,
      "grad_norm": 4.193542003631592,
      "learning_rate": 3.466068228867666e-05,
      "loss": 0.9789,
      "step": 17640
    },
    {
      "epoch": 0.9211058488916722,
      "grad_norm": 4.649352550506592,
      "learning_rate": 3.465198406485396e-05,
      "loss": 1.038,
      "step": 17650
    },
    {
      "epoch": 0.9216277218938771,
      "grad_norm": 5.114502429962158,
      "learning_rate": 3.4643285841031265e-05,
      "loss": 0.972,
      "step": 17660
    },
    {
      "epoch": 0.9221495948960821,
      "grad_norm": 4.832277774810791,
      "learning_rate": 3.463458761720857e-05,
      "loss": 0.9726,
      "step": 17670
    },
    {
      "epoch": 0.9226714678982869,
      "grad_norm": 3.9763710498809814,
      "learning_rate": 3.462588939338587e-05,
      "loss": 0.9244,
      "step": 17680
    },
    {
      "epoch": 0.9231933409004919,
      "grad_norm": 4.057667255401611,
      "learning_rate": 3.461719116956318e-05,
      "loss": 0.8891,
      "step": 17690
    },
    {
      "epoch": 0.9237152139026967,
      "grad_norm": 4.595880508422852,
      "learning_rate": 3.4608492945740486e-05,
      "loss": 0.9762,
      "step": 17700
    },
    {
      "epoch": 0.9242370869049017,
      "grad_norm": 3.730841636657715,
      "learning_rate": 3.459979472191779e-05,
      "loss": 0.9059,
      "step": 17710
    },
    {
      "epoch": 0.9247589599071067,
      "grad_norm": 4.070366859436035,
      "learning_rate": 3.4591096498095086e-05,
      "loss": 0.9217,
      "step": 17720
    },
    {
      "epoch": 0.9252808329093115,
      "grad_norm": 3.909538984298706,
      "learning_rate": 3.4582398274272396e-05,
      "loss": 0.9003,
      "step": 17730
    },
    {
      "epoch": 0.9258027059115165,
      "grad_norm": 4.362107276916504,
      "learning_rate": 3.45737000504497e-05,
      "loss": 0.9589,
      "step": 17740
    },
    {
      "epoch": 0.9263245789137213,
      "grad_norm": 3.9171361923217773,
      "learning_rate": 3.4565001826627e-05,
      "loss": 0.8775,
      "step": 17750
    },
    {
      "epoch": 0.9268464519159263,
      "grad_norm": 4.485328674316406,
      "learning_rate": 3.455630360280431e-05,
      "loss": 0.9777,
      "step": 17760
    },
    {
      "epoch": 0.9273683249181311,
      "grad_norm": 4.187316417694092,
      "learning_rate": 3.454760537898161e-05,
      "loss": 0.9429,
      "step": 17770
    },
    {
      "epoch": 0.9278901979203361,
      "grad_norm": 4.141664028167725,
      "learning_rate": 3.4538907155158914e-05,
      "loss": 0.9297,
      "step": 17780
    },
    {
      "epoch": 0.928412070922541,
      "grad_norm": 3.8670709133148193,
      "learning_rate": 3.4530208931336224e-05,
      "loss": 1.0127,
      "step": 17790
    },
    {
      "epoch": 0.9289339439247459,
      "grad_norm": 4.188093662261963,
      "learning_rate": 3.452151070751353e-05,
      "loss": 0.9927,
      "step": 17800
    },
    {
      "epoch": 0.9294558169269508,
      "grad_norm": 4.654306411743164,
      "learning_rate": 3.451281248369083e-05,
      "loss": 0.9779,
      "step": 17810
    },
    {
      "epoch": 0.9299776899291557,
      "grad_norm": 4.606435298919678,
      "learning_rate": 3.4504114259868135e-05,
      "loss": 0.9329,
      "step": 17820
    },
    {
      "epoch": 0.9304995629313606,
      "grad_norm": 4.299210548400879,
      "learning_rate": 3.449541603604544e-05,
      "loss": 0.9892,
      "step": 17830
    },
    {
      "epoch": 0.9310214359335656,
      "grad_norm": 4.05151891708374,
      "learning_rate": 3.448671781222275e-05,
      "loss": 0.9762,
      "step": 17840
    },
    {
      "epoch": 0.9315433089357705,
      "grad_norm": 4.687490940093994,
      "learning_rate": 3.447801958840005e-05,
      "loss": 0.9531,
      "step": 17850
    },
    {
      "epoch": 0.9320651819379754,
      "grad_norm": 5.397168159484863,
      "learning_rate": 3.4469321364577356e-05,
      "loss": 0.9484,
      "step": 17860
    },
    {
      "epoch": 0.9325870549401803,
      "grad_norm": 4.632364749908447,
      "learning_rate": 3.446062314075466e-05,
      "loss": 0.9275,
      "step": 17870
    },
    {
      "epoch": 0.9331089279423852,
      "grad_norm": 4.45050573348999,
      "learning_rate": 3.445192491693196e-05,
      "loss": 1.0038,
      "step": 17880
    },
    {
      "epoch": 0.9336308009445902,
      "grad_norm": 4.015772819519043,
      "learning_rate": 3.444322669310927e-05,
      "loss": 0.9199,
      "step": 17890
    },
    {
      "epoch": 0.934152673946795,
      "grad_norm": 3.896822452545166,
      "learning_rate": 3.443452846928658e-05,
      "loss": 0.8817,
      "step": 17900
    },
    {
      "epoch": 0.934674546949,
      "grad_norm": 3.834338426589966,
      "learning_rate": 3.442583024546388e-05,
      "loss": 0.8809,
      "step": 17910
    },
    {
      "epoch": 0.9351964199512048,
      "grad_norm": 3.832954168319702,
      "learning_rate": 3.4417132021641184e-05,
      "loss": 0.9853,
      "step": 17920
    },
    {
      "epoch": 0.9357182929534098,
      "grad_norm": 4.295454502105713,
      "learning_rate": 3.440843379781849e-05,
      "loss": 0.8629,
      "step": 17930
    },
    {
      "epoch": 0.9362401659556147,
      "grad_norm": 5.061343193054199,
      "learning_rate": 3.439973557399579e-05,
      "loss": 1.0587,
      "step": 17940
    },
    {
      "epoch": 0.9367620389578196,
      "grad_norm": 4.291093826293945,
      "learning_rate": 3.4391037350173095e-05,
      "loss": 0.971,
      "step": 17950
    },
    {
      "epoch": 0.9372839119600245,
      "grad_norm": 4.745819091796875,
      "learning_rate": 3.43823391263504e-05,
      "loss": 0.9331,
      "step": 17960
    },
    {
      "epoch": 0.9378057849622294,
      "grad_norm": 4.4198317527771,
      "learning_rate": 3.43736409025277e-05,
      "loss": 0.804,
      "step": 17970
    },
    {
      "epoch": 0.9383276579644344,
      "grad_norm": 3.618152618408203,
      "learning_rate": 3.436494267870501e-05,
      "loss": 0.8947,
      "step": 17980
    },
    {
      "epoch": 0.9388495309666393,
      "grad_norm": 4.209441661834717,
      "learning_rate": 3.4356244454882316e-05,
      "loss": 0.8874,
      "step": 17990
    },
    {
      "epoch": 0.9393714039688442,
      "grad_norm": 4.376565456390381,
      "learning_rate": 3.434754623105962e-05,
      "loss": 0.9797,
      "step": 18000
    },
    {
      "epoch": 0.9398932769710491,
      "grad_norm": 4.093678951263428,
      "learning_rate": 3.433884800723692e-05,
      "loss": 0.8652,
      "step": 18010
    },
    {
      "epoch": 0.940415149973254,
      "grad_norm": 4.039905548095703,
      "learning_rate": 3.4330149783414226e-05,
      "loss": 1.0066,
      "step": 18020
    },
    {
      "epoch": 0.9409370229754589,
      "grad_norm": 4.322059154510498,
      "learning_rate": 3.432145155959153e-05,
      "loss": 0.9445,
      "step": 18030
    },
    {
      "epoch": 0.9414588959776639,
      "grad_norm": 4.739468574523926,
      "learning_rate": 3.431275333576884e-05,
      "loss": 0.994,
      "step": 18040
    },
    {
      "epoch": 0.9419807689798687,
      "grad_norm": 4.3469438552856445,
      "learning_rate": 3.4304055111946144e-05,
      "loss": 0.9529,
      "step": 18050
    },
    {
      "epoch": 0.9425026419820737,
      "grad_norm": 3.9496755599975586,
      "learning_rate": 3.429535688812345e-05,
      "loss": 0.8923,
      "step": 18060
    },
    {
      "epoch": 0.9430245149842785,
      "grad_norm": 4.132821083068848,
      "learning_rate": 3.428665866430075e-05,
      "loss": 0.8449,
      "step": 18070
    },
    {
      "epoch": 0.9435463879864835,
      "grad_norm": 4.322865962982178,
      "learning_rate": 3.4277960440478054e-05,
      "loss": 0.9388,
      "step": 18080
    },
    {
      "epoch": 0.9440682609886885,
      "grad_norm": 4.467659950256348,
      "learning_rate": 3.4269262216655365e-05,
      "loss": 0.908,
      "step": 18090
    },
    {
      "epoch": 0.9445901339908933,
      "grad_norm": 4.787820339202881,
      "learning_rate": 3.426056399283267e-05,
      "loss": 0.9023,
      "step": 18100
    },
    {
      "epoch": 0.9451120069930983,
      "grad_norm": 4.633548736572266,
      "learning_rate": 3.425186576900997e-05,
      "loss": 0.8974,
      "step": 18110
    },
    {
      "epoch": 0.9456338799953031,
      "grad_norm": 4.217833042144775,
      "learning_rate": 3.4243167545187275e-05,
      "loss": 0.8454,
      "step": 18120
    },
    {
      "epoch": 0.9461557529975081,
      "grad_norm": 5.262409687042236,
      "learning_rate": 3.423446932136458e-05,
      "loss": 0.9151,
      "step": 18130
    },
    {
      "epoch": 0.946677625999713,
      "grad_norm": 4.781214714050293,
      "learning_rate": 3.422577109754188e-05,
      "loss": 1.0133,
      "step": 18140
    },
    {
      "epoch": 0.9471994990019179,
      "grad_norm": 5.172260761260986,
      "learning_rate": 3.4217072873719186e-05,
      "loss": 1.0268,
      "step": 18150
    },
    {
      "epoch": 0.9477213720041228,
      "grad_norm": 4.591944217681885,
      "learning_rate": 3.420837464989649e-05,
      "loss": 0.8009,
      "step": 18160
    },
    {
      "epoch": 0.9482432450063277,
      "grad_norm": 4.219757080078125,
      "learning_rate": 3.419967642607379e-05,
      "loss": 0.9506,
      "step": 18170
    },
    {
      "epoch": 0.9487651180085326,
      "grad_norm": 3.933184862136841,
      "learning_rate": 3.41909782022511e-05,
      "loss": 0.8326,
      "step": 18180
    },
    {
      "epoch": 0.9492869910107375,
      "grad_norm": 5.135036945343018,
      "learning_rate": 3.418227997842841e-05,
      "loss": 0.926,
      "step": 18190
    },
    {
      "epoch": 0.9498088640129424,
      "grad_norm": 5.113805294036865,
      "learning_rate": 3.417358175460571e-05,
      "loss": 1.0101,
      "step": 18200
    },
    {
      "epoch": 0.9503307370151474,
      "grad_norm": 4.056586742401123,
      "learning_rate": 3.4164883530783014e-05,
      "loss": 0.9099,
      "step": 18210
    },
    {
      "epoch": 0.9508526100173523,
      "grad_norm": 4.89378547668457,
      "learning_rate": 3.415618530696032e-05,
      "loss": 0.9403,
      "step": 18220
    },
    {
      "epoch": 0.9513744830195572,
      "grad_norm": 4.714542865753174,
      "learning_rate": 3.414748708313763e-05,
      "loss": 0.9379,
      "step": 18230
    },
    {
      "epoch": 0.9518963560217621,
      "grad_norm": 4.225874423980713,
      "learning_rate": 3.413878885931493e-05,
      "loss": 0.8746,
      "step": 18240
    },
    {
      "epoch": 0.952418229023967,
      "grad_norm": 3.8113479614257812,
      "learning_rate": 3.4130090635492235e-05,
      "loss": 0.9563,
      "step": 18250
    },
    {
      "epoch": 0.952940102026172,
      "grad_norm": 4.559427738189697,
      "learning_rate": 3.412139241166954e-05,
      "loss": 0.9825,
      "step": 18260
    },
    {
      "epoch": 0.9534619750283768,
      "grad_norm": 4.451852798461914,
      "learning_rate": 3.411269418784684e-05,
      "loss": 0.9723,
      "step": 18270
    },
    {
      "epoch": 0.9539838480305818,
      "grad_norm": 3.9942662715911865,
      "learning_rate": 3.410399596402415e-05,
      "loss": 0.8353,
      "step": 18280
    },
    {
      "epoch": 0.9545057210327866,
      "grad_norm": 4.334115505218506,
      "learning_rate": 3.4095297740201456e-05,
      "loss": 0.9688,
      "step": 18290
    },
    {
      "epoch": 0.9550275940349916,
      "grad_norm": 3.6419079303741455,
      "learning_rate": 3.408659951637876e-05,
      "loss": 0.9671,
      "step": 18300
    },
    {
      "epoch": 0.9555494670371965,
      "grad_norm": 4.493753433227539,
      "learning_rate": 3.407790129255606e-05,
      "loss": 0.859,
      "step": 18310
    },
    {
      "epoch": 0.9560713400394014,
      "grad_norm": 4.733723163604736,
      "learning_rate": 3.4069203068733367e-05,
      "loss": 0.9876,
      "step": 18320
    },
    {
      "epoch": 0.9565932130416063,
      "grad_norm": 4.13408088684082,
      "learning_rate": 3.406050484491067e-05,
      "loss": 0.8385,
      "step": 18330
    },
    {
      "epoch": 0.9571150860438112,
      "grad_norm": 4.501567363739014,
      "learning_rate": 3.4051806621087974e-05,
      "loss": 0.9182,
      "step": 18340
    },
    {
      "epoch": 0.9576369590460162,
      "grad_norm": 4.205788612365723,
      "learning_rate": 3.404310839726528e-05,
      "loss": 0.9648,
      "step": 18350
    },
    {
      "epoch": 0.958158832048221,
      "grad_norm": 3.7816977500915527,
      "learning_rate": 3.403441017344258e-05,
      "loss": 0.8597,
      "step": 18360
    },
    {
      "epoch": 0.958680705050426,
      "grad_norm": 4.07661247253418,
      "learning_rate": 3.4025711949619884e-05,
      "loss": 0.9261,
      "step": 18370
    },
    {
      "epoch": 0.9592025780526309,
      "grad_norm": 4.877561092376709,
      "learning_rate": 3.4017013725797195e-05,
      "loss": 0.8576,
      "step": 18380
    },
    {
      "epoch": 0.9597244510548358,
      "grad_norm": 4.04196834564209,
      "learning_rate": 3.40083155019745e-05,
      "loss": 0.9138,
      "step": 18390
    },
    {
      "epoch": 0.9602463240570407,
      "grad_norm": 4.619895935058594,
      "learning_rate": 3.39996172781518e-05,
      "loss": 1.0016,
      "step": 18400
    },
    {
      "epoch": 0.9607681970592457,
      "grad_norm": 4.9246721267700195,
      "learning_rate": 3.3990919054329105e-05,
      "loss": 1.0078,
      "step": 18410
    },
    {
      "epoch": 0.9612900700614505,
      "grad_norm": 4.507828235626221,
      "learning_rate": 3.398222083050641e-05,
      "loss": 0.9553,
      "step": 18420
    },
    {
      "epoch": 0.9618119430636555,
      "grad_norm": 4.299561977386475,
      "learning_rate": 3.397352260668372e-05,
      "loss": 0.9368,
      "step": 18430
    },
    {
      "epoch": 0.9623338160658603,
      "grad_norm": 4.861922740936279,
      "learning_rate": 3.396482438286102e-05,
      "loss": 0.935,
      "step": 18440
    },
    {
      "epoch": 0.9628556890680653,
      "grad_norm": 5.086082458496094,
      "learning_rate": 3.3956126159038326e-05,
      "loss": 0.9527,
      "step": 18450
    },
    {
      "epoch": 0.9633775620702703,
      "grad_norm": 3.9440999031066895,
      "learning_rate": 3.394742793521563e-05,
      "loss": 0.9419,
      "step": 18460
    },
    {
      "epoch": 0.9638994350724751,
      "grad_norm": 4.1448974609375,
      "learning_rate": 3.393872971139293e-05,
      "loss": 0.9375,
      "step": 18470
    },
    {
      "epoch": 0.9644213080746801,
      "grad_norm": 3.7388880252838135,
      "learning_rate": 3.3930031487570244e-05,
      "loss": 0.9433,
      "step": 18480
    },
    {
      "epoch": 0.9649431810768849,
      "grad_norm": 4.630144119262695,
      "learning_rate": 3.392133326374755e-05,
      "loss": 0.902,
      "step": 18490
    },
    {
      "epoch": 0.9654650540790899,
      "grad_norm": 4.807101249694824,
      "learning_rate": 3.391263503992485e-05,
      "loss": 0.9162,
      "step": 18500
    },
    {
      "epoch": 0.9659869270812947,
      "grad_norm": 4.976070880889893,
      "learning_rate": 3.3903936816102154e-05,
      "loss": 0.9584,
      "step": 18510
    },
    {
      "epoch": 0.9665088000834997,
      "grad_norm": 4.355734348297119,
      "learning_rate": 3.389523859227946e-05,
      "loss": 0.844,
      "step": 18520
    },
    {
      "epoch": 0.9670306730857046,
      "grad_norm": 3.9913458824157715,
      "learning_rate": 3.388654036845677e-05,
      "loss": 0.8897,
      "step": 18530
    },
    {
      "epoch": 0.9675525460879095,
      "grad_norm": 4.045324325561523,
      "learning_rate": 3.387784214463407e-05,
      "loss": 0.9193,
      "step": 18540
    },
    {
      "epoch": 0.9680744190901144,
      "grad_norm": 4.3341803550720215,
      "learning_rate": 3.386914392081137e-05,
      "loss": 0.8592,
      "step": 18550
    },
    {
      "epoch": 0.9685962920923193,
      "grad_norm": 5.282574653625488,
      "learning_rate": 3.386044569698867e-05,
      "loss": 1.0273,
      "step": 18560
    },
    {
      "epoch": 0.9691181650945242,
      "grad_norm": 3.482039451599121,
      "learning_rate": 3.3851747473165976e-05,
      "loss": 0.9306,
      "step": 18570
    },
    {
      "epoch": 0.9696400380967292,
      "grad_norm": 3.78123140335083,
      "learning_rate": 3.3843049249343286e-05,
      "loss": 0.9277,
      "step": 18580
    },
    {
      "epoch": 0.9701619110989341,
      "grad_norm": 4.41936731338501,
      "learning_rate": 3.383435102552059e-05,
      "loss": 0.9605,
      "step": 18590
    },
    {
      "epoch": 0.970683784101139,
      "grad_norm": 4.292596340179443,
      "learning_rate": 3.382565280169789e-05,
      "loss": 0.9188,
      "step": 18600
    },
    {
      "epoch": 0.971205657103344,
      "grad_norm": 4.516805648803711,
      "learning_rate": 3.38169545778752e-05,
      "loss": 0.9651,
      "step": 18610
    },
    {
      "epoch": 0.9717275301055488,
      "grad_norm": 4.989660739898682,
      "learning_rate": 3.38082563540525e-05,
      "loss": 0.94,
      "step": 18620
    },
    {
      "epoch": 0.9722494031077538,
      "grad_norm": 4.051508903503418,
      "learning_rate": 3.379955813022981e-05,
      "loss": 0.9955,
      "step": 18630
    },
    {
      "epoch": 0.9727712761099586,
      "grad_norm": 4.6002302169799805,
      "learning_rate": 3.3790859906407114e-05,
      "loss": 0.9787,
      "step": 18640
    },
    {
      "epoch": 0.9732931491121636,
      "grad_norm": 3.9557273387908936,
      "learning_rate": 3.378216168258442e-05,
      "loss": 0.9824,
      "step": 18650
    },
    {
      "epoch": 0.9738150221143684,
      "grad_norm": 3.7409632205963135,
      "learning_rate": 3.377346345876172e-05,
      "loss": 0.932,
      "step": 18660
    },
    {
      "epoch": 0.9743368951165734,
      "grad_norm": 4.468768119812012,
      "learning_rate": 3.3764765234939025e-05,
      "loss": 0.9525,
      "step": 18670
    },
    {
      "epoch": 0.9748587681187783,
      "grad_norm": 5.415150165557861,
      "learning_rate": 3.3756067011116335e-05,
      "loss": 0.9736,
      "step": 18680
    },
    {
      "epoch": 0.9753806411209832,
      "grad_norm": 4.301346778869629,
      "learning_rate": 3.374736878729364e-05,
      "loss": 0.9493,
      "step": 18690
    },
    {
      "epoch": 0.9759025141231881,
      "grad_norm": 4.420175552368164,
      "learning_rate": 3.373867056347094e-05,
      "loss": 0.9768,
      "step": 18700
    },
    {
      "epoch": 0.976424387125393,
      "grad_norm": 3.4488532543182373,
      "learning_rate": 3.3729972339648246e-05,
      "loss": 0.8698,
      "step": 18710
    },
    {
      "epoch": 0.976946260127598,
      "grad_norm": 5.689232349395752,
      "learning_rate": 3.372127411582555e-05,
      "loss": 0.9371,
      "step": 18720
    },
    {
      "epoch": 0.9774681331298029,
      "grad_norm": 5.085878849029541,
      "learning_rate": 3.371257589200286e-05,
      "loss": 1.052,
      "step": 18730
    },
    {
      "epoch": 0.9779900061320078,
      "grad_norm": 3.8557515144348145,
      "learning_rate": 3.370387766818016e-05,
      "loss": 0.858,
      "step": 18740
    },
    {
      "epoch": 0.9785118791342127,
      "grad_norm": 4.472252368927002,
      "learning_rate": 3.369517944435747e-05,
      "loss": 1.0622,
      "step": 18750
    },
    {
      "epoch": 0.9790337521364176,
      "grad_norm": 4.715199947357178,
      "learning_rate": 3.3686481220534763e-05,
      "loss": 0.954,
      "step": 18760
    },
    {
      "epoch": 0.9795556251386225,
      "grad_norm": 3.975177526473999,
      "learning_rate": 3.3677782996712074e-05,
      "loss": 0.9839,
      "step": 18770
    },
    {
      "epoch": 0.9800774981408275,
      "grad_norm": 4.083940029144287,
      "learning_rate": 3.366908477288938e-05,
      "loss": 0.9547,
      "step": 18780
    },
    {
      "epoch": 0.9805993711430323,
      "grad_norm": 4.314713478088379,
      "learning_rate": 3.366038654906668e-05,
      "loss": 1.0108,
      "step": 18790
    },
    {
      "epoch": 0.9811212441452373,
      "grad_norm": 4.417299747467041,
      "learning_rate": 3.3651688325243984e-05,
      "loss": 1.0056,
      "step": 18800
    },
    {
      "epoch": 0.9816431171474421,
      "grad_norm": 5.069392204284668,
      "learning_rate": 3.364299010142129e-05,
      "loss": 0.9995,
      "step": 18810
    },
    {
      "epoch": 0.9821649901496471,
      "grad_norm": 5.27446985244751,
      "learning_rate": 3.36342918775986e-05,
      "loss": 0.9307,
      "step": 18820
    },
    {
      "epoch": 0.9826868631518519,
      "grad_norm": 4.179201126098633,
      "learning_rate": 3.36255936537759e-05,
      "loss": 0.9519,
      "step": 18830
    },
    {
      "epoch": 0.9832087361540569,
      "grad_norm": 4.463870048522949,
      "learning_rate": 3.3616895429953205e-05,
      "loss": 0.8852,
      "step": 18840
    },
    {
      "epoch": 0.9837306091562619,
      "grad_norm": 4.526422023773193,
      "learning_rate": 3.360819720613051e-05,
      "loss": 0.9196,
      "step": 18850
    },
    {
      "epoch": 0.9842524821584667,
      "grad_norm": 4.7277374267578125,
      "learning_rate": 3.359949898230781e-05,
      "loss": 1.047,
      "step": 18860
    },
    {
      "epoch": 0.9847743551606717,
      "grad_norm": 4.613319396972656,
      "learning_rate": 3.3590800758485116e-05,
      "loss": 0.964,
      "step": 18870
    },
    {
      "epoch": 0.9852962281628765,
      "grad_norm": 4.151922702789307,
      "learning_rate": 3.3582102534662426e-05,
      "loss": 0.9668,
      "step": 18880
    },
    {
      "epoch": 0.9858181011650815,
      "grad_norm": 3.8367862701416016,
      "learning_rate": 3.357340431083973e-05,
      "loss": 1.0486,
      "step": 18890
    },
    {
      "epoch": 0.9863399741672864,
      "grad_norm": 4.8987016677856445,
      "learning_rate": 3.3564706087017033e-05,
      "loss": 0.8735,
      "step": 18900
    },
    {
      "epoch": 0.9868618471694913,
      "grad_norm": 4.8716840744018555,
      "learning_rate": 3.355600786319434e-05,
      "loss": 0.9929,
      "step": 18910
    },
    {
      "epoch": 0.9873837201716962,
      "grad_norm": 4.330658435821533,
      "learning_rate": 3.354730963937164e-05,
      "loss": 1.0219,
      "step": 18920
    },
    {
      "epoch": 0.9879055931739011,
      "grad_norm": 4.022839069366455,
      "learning_rate": 3.353861141554895e-05,
      "loss": 0.9916,
      "step": 18930
    },
    {
      "epoch": 0.988427466176106,
      "grad_norm": 4.460480690002441,
      "learning_rate": 3.3529913191726254e-05,
      "loss": 0.9551,
      "step": 18940
    },
    {
      "epoch": 0.988949339178311,
      "grad_norm": 4.749549865722656,
      "learning_rate": 3.352121496790356e-05,
      "loss": 0.9171,
      "step": 18950
    },
    {
      "epoch": 0.9894712121805159,
      "grad_norm": 4.003354549407959,
      "learning_rate": 3.351251674408086e-05,
      "loss": 0.9361,
      "step": 18960
    },
    {
      "epoch": 0.9899930851827208,
      "grad_norm": 4.857061386108398,
      "learning_rate": 3.3503818520258165e-05,
      "loss": 0.8704,
      "step": 18970
    },
    {
      "epoch": 0.9905149581849257,
      "grad_norm": 4.552832126617432,
      "learning_rate": 3.349512029643547e-05,
      "loss": 0.9568,
      "step": 18980
    },
    {
      "epoch": 0.9910368311871306,
      "grad_norm": 3.5037167072296143,
      "learning_rate": 3.348642207261277e-05,
      "loss": 0.9041,
      "step": 18990
    },
    {
      "epoch": 0.9915587041893356,
      "grad_norm": 3.3745484352111816,
      "learning_rate": 3.3477723848790076e-05,
      "loss": 0.9016,
      "step": 19000
    },
    {
      "epoch": 0.9920805771915404,
      "grad_norm": 4.613024711608887,
      "learning_rate": 3.346902562496738e-05,
      "loss": 0.8749,
      "step": 19010
    },
    {
      "epoch": 0.9926024501937454,
      "grad_norm": 4.085357666015625,
      "learning_rate": 3.346032740114469e-05,
      "loss": 0.9612,
      "step": 19020
    },
    {
      "epoch": 0.9931243231959502,
      "grad_norm": 4.818084716796875,
      "learning_rate": 3.345162917732199e-05,
      "loss": 0.8856,
      "step": 19030
    },
    {
      "epoch": 0.9936461961981552,
      "grad_norm": 5.700112819671631,
      "learning_rate": 3.34429309534993e-05,
      "loss": 0.8749,
      "step": 19040
    },
    {
      "epoch": 0.99416806920036,
      "grad_norm": 4.127659797668457,
      "learning_rate": 3.34342327296766e-05,
      "loss": 0.8617,
      "step": 19050
    },
    {
      "epoch": 0.994689942202565,
      "grad_norm": 4.706568241119385,
      "learning_rate": 3.3425534505853904e-05,
      "loss": 0.9357,
      "step": 19060
    },
    {
      "epoch": 0.9952118152047699,
      "grad_norm": 3.7569003105163574,
      "learning_rate": 3.3416836282031214e-05,
      "loss": 0.846,
      "step": 19070
    },
    {
      "epoch": 0.9957336882069748,
      "grad_norm": 5.150728702545166,
      "learning_rate": 3.340813805820852e-05,
      "loss": 0.9889,
      "step": 19080
    },
    {
      "epoch": 0.9962555612091798,
      "grad_norm": 3.7418832778930664,
      "learning_rate": 3.339943983438582e-05,
      "loss": 0.9075,
      "step": 19090
    },
    {
      "epoch": 0.9967774342113846,
      "grad_norm": 5.2511162757873535,
      "learning_rate": 3.3390741610563125e-05,
      "loss": 0.9511,
      "step": 19100
    },
    {
      "epoch": 0.9972993072135896,
      "grad_norm": 4.077248573303223,
      "learning_rate": 3.338204338674043e-05,
      "loss": 0.9637,
      "step": 19110
    },
    {
      "epoch": 0.9978211802157945,
      "grad_norm": 4.6683526039123535,
      "learning_rate": 3.337334516291773e-05,
      "loss": 0.9987,
      "step": 19120
    },
    {
      "epoch": 0.9983430532179994,
      "grad_norm": 4.5808587074279785,
      "learning_rate": 3.336464693909504e-05,
      "loss": 0.8658,
      "step": 19130
    },
    {
      "epoch": 0.9988649262202043,
      "grad_norm": 3.956022262573242,
      "learning_rate": 3.3355948715272346e-05,
      "loss": 0.9188,
      "step": 19140
    },
    {
      "epoch": 0.9993867992224093,
      "grad_norm": 4.806246757507324,
      "learning_rate": 3.334725049144965e-05,
      "loss": 0.927,
      "step": 19150
    },
    {
      "epoch": 0.9999086722246141,
      "grad_norm": 4.676771640777588,
      "learning_rate": 3.333855226762695e-05,
      "loss": 0.8665,
      "step": 19160
    },
    {
      "epoch": 0.9999608595248346,
      "eval_loss": 0.9421966671943665,
      "eval_runtime": 1020.0973,
      "eval_samples_per_second": 75.137,
      "eval_steps_per_second": 18.784,
      "step": 19161
    },
    {
      "epoch": 1.0004696857019844,
      "grad_norm": 4.572789192199707,
      "learning_rate": 3.3329854043804256e-05,
      "loss": 0.9486,
      "step": 19170
    },
    {
      "epoch": 1.0009915587041893,
      "grad_norm": 5.035638332366943,
      "learning_rate": 3.332115581998156e-05,
      "loss": 0.9296,
      "step": 19180
    },
    {
      "epoch": 1.0015134317063943,
      "grad_norm": 4.532062530517578,
      "learning_rate": 3.3312457596158863e-05,
      "loss": 0.9879,
      "step": 19190
    },
    {
      "epoch": 1.0020353047085993,
      "grad_norm": 4.092541694641113,
      "learning_rate": 3.330375937233617e-05,
      "loss": 0.8331,
      "step": 19200
    },
    {
      "epoch": 1.002557177710804,
      "grad_norm": 4.769407272338867,
      "learning_rate": 3.329506114851347e-05,
      "loss": 0.9303,
      "step": 19210
    },
    {
      "epoch": 1.003079050713009,
      "grad_norm": 4.157574653625488,
      "learning_rate": 3.328636292469078e-05,
      "loss": 1.0028,
      "step": 19220
    },
    {
      "epoch": 1.003600923715214,
      "grad_norm": 5.006597995758057,
      "learning_rate": 3.3277664700868084e-05,
      "loss": 0.9674,
      "step": 19230
    },
    {
      "epoch": 1.0041227967174189,
      "grad_norm": 3.19675874710083,
      "learning_rate": 3.326896647704539e-05,
      "loss": 0.9495,
      "step": 19240
    },
    {
      "epoch": 1.0046446697196236,
      "grad_norm": 4.264176368713379,
      "learning_rate": 3.326026825322269e-05,
      "loss": 0.9608,
      "step": 19250
    },
    {
      "epoch": 1.0051665427218286,
      "grad_norm": 4.065164566040039,
      "learning_rate": 3.3251570029399995e-05,
      "loss": 0.9109,
      "step": 19260
    },
    {
      "epoch": 1.0056884157240336,
      "grad_norm": 3.957453727722168,
      "learning_rate": 3.3242871805577305e-05,
      "loss": 0.9548,
      "step": 19270
    },
    {
      "epoch": 1.0062102887262385,
      "grad_norm": 4.382964611053467,
      "learning_rate": 3.323417358175461e-05,
      "loss": 1.0129,
      "step": 19280
    },
    {
      "epoch": 1.0067321617284435,
      "grad_norm": 4.511965274810791,
      "learning_rate": 3.322547535793191e-05,
      "loss": 0.9677,
      "step": 19290
    },
    {
      "epoch": 1.0072540347306482,
      "grad_norm": 4.801154136657715,
      "learning_rate": 3.3216777134109216e-05,
      "loss": 0.9348,
      "step": 19300
    },
    {
      "epoch": 1.0077759077328532,
      "grad_norm": 4.477494716644287,
      "learning_rate": 3.320807891028652e-05,
      "loss": 0.9521,
      "step": 19310
    },
    {
      "epoch": 1.0082977807350582,
      "grad_norm": 3.4178104400634766,
      "learning_rate": 3.319938068646383e-05,
      "loss": 0.8576,
      "step": 19320
    },
    {
      "epoch": 1.0088196537372631,
      "grad_norm": 4.043185234069824,
      "learning_rate": 3.3190682462641133e-05,
      "loss": 0.9717,
      "step": 19330
    },
    {
      "epoch": 1.0093415267394679,
      "grad_norm": 3.9733657836914062,
      "learning_rate": 3.318198423881844e-05,
      "loss": 0.8698,
      "step": 19340
    },
    {
      "epoch": 1.0098633997416728,
      "grad_norm": 4.95849609375,
      "learning_rate": 3.317328601499574e-05,
      "loss": 0.9638,
      "step": 19350
    },
    {
      "epoch": 1.0103852727438778,
      "grad_norm": 4.115656852722168,
      "learning_rate": 3.3164587791173044e-05,
      "loss": 0.9285,
      "step": 19360
    },
    {
      "epoch": 1.0109071457460828,
      "grad_norm": 3.584181070327759,
      "learning_rate": 3.3155889567350354e-05,
      "loss": 0.9167,
      "step": 19370
    },
    {
      "epoch": 1.0114290187482875,
      "grad_norm": 4.379214763641357,
      "learning_rate": 3.314719134352766e-05,
      "loss": 0.8988,
      "step": 19380
    },
    {
      "epoch": 1.0119508917504925,
      "grad_norm": 4.132543087005615,
      "learning_rate": 3.3138493119704955e-05,
      "loss": 0.8319,
      "step": 19390
    },
    {
      "epoch": 1.0124727647526974,
      "grad_norm": 5.0241546630859375,
      "learning_rate": 3.312979489588226e-05,
      "loss": 0.9244,
      "step": 19400
    },
    {
      "epoch": 1.0129946377549024,
      "grad_norm": 4.376433849334717,
      "learning_rate": 3.312109667205956e-05,
      "loss": 0.9018,
      "step": 19410
    },
    {
      "epoch": 1.0135165107571074,
      "grad_norm": 4.275363922119141,
      "learning_rate": 3.311239844823687e-05,
      "loss": 0.9262,
      "step": 19420
    },
    {
      "epoch": 1.014038383759312,
      "grad_norm": 5.165622234344482,
      "learning_rate": 3.3103700224414176e-05,
      "loss": 1.0313,
      "step": 19430
    },
    {
      "epoch": 1.014560256761517,
      "grad_norm": 4.307079792022705,
      "learning_rate": 3.309500200059148e-05,
      "loss": 0.9421,
      "step": 19440
    },
    {
      "epoch": 1.015082129763722,
      "grad_norm": 5.036000728607178,
      "learning_rate": 3.308630377676878e-05,
      "loss": 0.9355,
      "step": 19450
    },
    {
      "epoch": 1.015604002765927,
      "grad_norm": 4.290534019470215,
      "learning_rate": 3.3077605552946086e-05,
      "loss": 0.794,
      "step": 19460
    },
    {
      "epoch": 1.0161258757681317,
      "grad_norm": 4.102389812469482,
      "learning_rate": 3.30689073291234e-05,
      "loss": 0.8276,
      "step": 19470
    },
    {
      "epoch": 1.0166477487703367,
      "grad_norm": 4.888453960418701,
      "learning_rate": 3.30602091053007e-05,
      "loss": 0.8929,
      "step": 19480
    },
    {
      "epoch": 1.0171696217725417,
      "grad_norm": 4.797729015350342,
      "learning_rate": 3.3051510881478004e-05,
      "loss": 0.8578,
      "step": 19490
    },
    {
      "epoch": 1.0176914947747466,
      "grad_norm": 3.951063632965088,
      "learning_rate": 3.304281265765531e-05,
      "loss": 0.981,
      "step": 19500
    },
    {
      "epoch": 1.0182133677769514,
      "grad_norm": 4.109825611114502,
      "learning_rate": 3.303411443383261e-05,
      "loss": 0.8946,
      "step": 19510
    },
    {
      "epoch": 1.0187352407791563,
      "grad_norm": 4.74231481552124,
      "learning_rate": 3.302541621000992e-05,
      "loss": 0.9979,
      "step": 19520
    },
    {
      "epoch": 1.0192571137813613,
      "grad_norm": 4.238951683044434,
      "learning_rate": 3.3016717986187225e-05,
      "loss": 0.924,
      "step": 19530
    },
    {
      "epoch": 1.0197789867835663,
      "grad_norm": 4.724996089935303,
      "learning_rate": 3.300801976236453e-05,
      "loss": 0.9715,
      "step": 19540
    },
    {
      "epoch": 1.0203008597857712,
      "grad_norm": 4.801077842712402,
      "learning_rate": 3.299932153854183e-05,
      "loss": 0.9263,
      "step": 19550
    },
    {
      "epoch": 1.020822732787976,
      "grad_norm": 5.443404197692871,
      "learning_rate": 3.2990623314719135e-05,
      "loss": 0.9153,
      "step": 19560
    },
    {
      "epoch": 1.021344605790181,
      "grad_norm": 4.762052536010742,
      "learning_rate": 3.2981925090896446e-05,
      "loss": 0.9291,
      "step": 19570
    },
    {
      "epoch": 1.021866478792386,
      "grad_norm": 5.29847526550293,
      "learning_rate": 3.297322686707375e-05,
      "loss": 0.9478,
      "step": 19580
    },
    {
      "epoch": 1.0223883517945909,
      "grad_norm": 4.774055480957031,
      "learning_rate": 3.2964528643251046e-05,
      "loss": 0.8525,
      "step": 19590
    },
    {
      "epoch": 1.0229102247967956,
      "grad_norm": 3.8620026111602783,
      "learning_rate": 3.295583041942835e-05,
      "loss": 0.8901,
      "step": 19600
    },
    {
      "epoch": 1.0234320977990006,
      "grad_norm": 4.279909610748291,
      "learning_rate": 3.294713219560566e-05,
      "loss": 0.9547,
      "step": 19610
    },
    {
      "epoch": 1.0239539708012055,
      "grad_norm": 4.486428260803223,
      "learning_rate": 3.2938433971782963e-05,
      "loss": 0.9101,
      "step": 19620
    },
    {
      "epoch": 1.0244758438034105,
      "grad_norm": 4.401727676391602,
      "learning_rate": 3.292973574796027e-05,
      "loss": 0.9023,
      "step": 19630
    },
    {
      "epoch": 1.0249977168056152,
      "grad_norm": 3.965146064758301,
      "learning_rate": 3.292103752413757e-05,
      "loss": 0.9283,
      "step": 19640
    },
    {
      "epoch": 1.0255195898078202,
      "grad_norm": 4.0114054679870605,
      "learning_rate": 3.2912339300314874e-05,
      "loss": 0.892,
      "step": 19650
    },
    {
      "epoch": 1.0260414628100252,
      "grad_norm": 3.2450766563415527,
      "learning_rate": 3.290364107649218e-05,
      "loss": 0.8505,
      "step": 19660
    },
    {
      "epoch": 1.0265633358122301,
      "grad_norm": 4.9357147216796875,
      "learning_rate": 3.289494285266949e-05,
      "loss": 0.9679,
      "step": 19670
    },
    {
      "epoch": 1.027085208814435,
      "grad_norm": 4.983401775360107,
      "learning_rate": 3.288624462884679e-05,
      "loss": 0.8706,
      "step": 19680
    },
    {
      "epoch": 1.0276070818166398,
      "grad_norm": 4.687330722808838,
      "learning_rate": 3.2877546405024095e-05,
      "loss": 0.9523,
      "step": 19690
    },
    {
      "epoch": 1.0281289548188448,
      "grad_norm": 3.7442619800567627,
      "learning_rate": 3.28688481812014e-05,
      "loss": 0.9257,
      "step": 19700
    },
    {
      "epoch": 1.0286508278210498,
      "grad_norm": 4.521451473236084,
      "learning_rate": 3.28601499573787e-05,
      "loss": 0.9679,
      "step": 19710
    },
    {
      "epoch": 1.0291727008232547,
      "grad_norm": 3.336653470993042,
      "learning_rate": 3.285145173355601e-05,
      "loss": 0.8695,
      "step": 19720
    },
    {
      "epoch": 1.0296945738254595,
      "grad_norm": 4.577229022979736,
      "learning_rate": 3.2842753509733316e-05,
      "loss": 0.9649,
      "step": 19730
    },
    {
      "epoch": 1.0302164468276644,
      "grad_norm": 4.657800674438477,
      "learning_rate": 3.283405528591062e-05,
      "loss": 0.9834,
      "step": 19740
    },
    {
      "epoch": 1.0307383198298694,
      "grad_norm": 4.175501346588135,
      "learning_rate": 3.282535706208792e-05,
      "loss": 0.8959,
      "step": 19750
    },
    {
      "epoch": 1.0312601928320744,
      "grad_norm": 4.4130120277404785,
      "learning_rate": 3.281665883826523e-05,
      "loss": 0.8317,
      "step": 19760
    },
    {
      "epoch": 1.0317820658342791,
      "grad_norm": 3.47659969329834,
      "learning_rate": 3.280796061444254e-05,
      "loss": 0.9089,
      "step": 19770
    },
    {
      "epoch": 1.032303938836484,
      "grad_norm": 5.0303730964660645,
      "learning_rate": 3.279926239061984e-05,
      "loss": 0.9592,
      "step": 19780
    },
    {
      "epoch": 1.032825811838689,
      "grad_norm": 3.5928866863250732,
      "learning_rate": 3.2790564166797144e-05,
      "loss": 0.901,
      "step": 19790
    },
    {
      "epoch": 1.033347684840894,
      "grad_norm": 3.9180757999420166,
      "learning_rate": 3.278186594297444e-05,
      "loss": 0.9778,
      "step": 19800
    },
    {
      "epoch": 1.033869557843099,
      "grad_norm": 4.7522735595703125,
      "learning_rate": 3.277316771915175e-05,
      "loss": 0.8747,
      "step": 19810
    },
    {
      "epoch": 1.0343914308453037,
      "grad_norm": 3.7562572956085205,
      "learning_rate": 3.2764469495329055e-05,
      "loss": 0.9958,
      "step": 19820
    },
    {
      "epoch": 1.0349133038475087,
      "grad_norm": 4.486487865447998,
      "learning_rate": 3.275577127150636e-05,
      "loss": 0.8608,
      "step": 19830
    },
    {
      "epoch": 1.0354351768497136,
      "grad_norm": 3.951578140258789,
      "learning_rate": 3.274707304768366e-05,
      "loss": 0.9028,
      "step": 19840
    },
    {
      "epoch": 1.0359570498519186,
      "grad_norm": 4.419041156768799,
      "learning_rate": 3.2738374823860965e-05,
      "loss": 0.9374,
      "step": 19850
    },
    {
      "epoch": 1.0364789228541234,
      "grad_norm": 4.234618186950684,
      "learning_rate": 3.2729676600038276e-05,
      "loss": 0.9643,
      "step": 19860
    },
    {
      "epoch": 1.0370007958563283,
      "grad_norm": 4.131852626800537,
      "learning_rate": 3.272097837621558e-05,
      "loss": 0.9033,
      "step": 19870
    },
    {
      "epoch": 1.0375226688585333,
      "grad_norm": 3.7430877685546875,
      "learning_rate": 3.271228015239288e-05,
      "loss": 0.9522,
      "step": 19880
    },
    {
      "epoch": 1.0380445418607382,
      "grad_norm": 5.296824932098389,
      "learning_rate": 3.2703581928570186e-05,
      "loss": 0.9758,
      "step": 19890
    },
    {
      "epoch": 1.0385664148629432,
      "grad_norm": 4.086090087890625,
      "learning_rate": 3.269488370474749e-05,
      "loss": 0.9049,
      "step": 19900
    },
    {
      "epoch": 1.039088287865148,
      "grad_norm": 3.697965145111084,
      "learning_rate": 3.26861854809248e-05,
      "loss": 0.8925,
      "step": 19910
    },
    {
      "epoch": 1.039610160867353,
      "grad_norm": 3.7488045692443848,
      "learning_rate": 3.2677487257102104e-05,
      "loss": 0.9034,
      "step": 19920
    },
    {
      "epoch": 1.0401320338695579,
      "grad_norm": 4.992125034332275,
      "learning_rate": 3.266878903327941e-05,
      "loss": 0.9605,
      "step": 19930
    },
    {
      "epoch": 1.0406539068717628,
      "grad_norm": 4.903231143951416,
      "learning_rate": 3.266009080945671e-05,
      "loss": 0.9347,
      "step": 19940
    },
    {
      "epoch": 1.0411757798739676,
      "grad_norm": 3.6840078830718994,
      "learning_rate": 3.2651392585634014e-05,
      "loss": 0.8725,
      "step": 19950
    },
    {
      "epoch": 1.0416976528761726,
      "grad_norm": 3.8638429641723633,
      "learning_rate": 3.2643564184193584e-05,
      "loss": 0.9067,
      "step": 19960
    },
    {
      "epoch": 1.0422195258783775,
      "grad_norm": 5.214405059814453,
      "learning_rate": 3.2634865960370895e-05,
      "loss": 0.859,
      "step": 19970
    },
    {
      "epoch": 1.0427413988805825,
      "grad_norm": 5.398360252380371,
      "learning_rate": 3.26261677365482e-05,
      "loss": 0.9696,
      "step": 19980
    },
    {
      "epoch": 1.0432632718827872,
      "grad_norm": 5.127519607543945,
      "learning_rate": 3.26174695127255e-05,
      "loss": 0.8595,
      "step": 19990
    },
    {
      "epoch": 1.0437851448849922,
      "grad_norm": 4.062005043029785,
      "learning_rate": 3.2608771288902805e-05,
      "loss": 0.9378,
      "step": 20000
    },
    {
      "epoch": 1.0443070178871972,
      "grad_norm": 3.990126848220825,
      "learning_rate": 3.260007306508011e-05,
      "loss": 0.9161,
      "step": 20010
    },
    {
      "epoch": 1.0448288908894021,
      "grad_norm": 4.351926803588867,
      "learning_rate": 3.259137484125742e-05,
      "loss": 0.9088,
      "step": 20020
    },
    {
      "epoch": 1.045350763891607,
      "grad_norm": 4.4155497550964355,
      "learning_rate": 3.258267661743472e-05,
      "loss": 0.853,
      "step": 20030
    },
    {
      "epoch": 1.0458726368938118,
      "grad_norm": 3.537846088409424,
      "learning_rate": 3.2573978393612026e-05,
      "loss": 0.843,
      "step": 20040
    },
    {
      "epoch": 1.0463945098960168,
      "grad_norm": 4.105753421783447,
      "learning_rate": 3.256528016978933e-05,
      "loss": 0.9209,
      "step": 20050
    },
    {
      "epoch": 1.0469163828982218,
      "grad_norm": 4.568850517272949,
      "learning_rate": 3.255658194596663e-05,
      "loss": 0.8135,
      "step": 20060
    },
    {
      "epoch": 1.0474382559004267,
      "grad_norm": 3.6520192623138428,
      "learning_rate": 3.2547883722143944e-05,
      "loss": 0.8749,
      "step": 20070
    },
    {
      "epoch": 1.0479601289026315,
      "grad_norm": 3.867276430130005,
      "learning_rate": 3.253918549832125e-05,
      "loss": 0.8574,
      "step": 20080
    },
    {
      "epoch": 1.0484820019048364,
      "grad_norm": 4.784599781036377,
      "learning_rate": 3.253048727449855e-05,
      "loss": 0.9673,
      "step": 20090
    },
    {
      "epoch": 1.0490038749070414,
      "grad_norm": 3.993044137954712,
      "learning_rate": 3.2521789050675854e-05,
      "loss": 0.9628,
      "step": 20100
    },
    {
      "epoch": 1.0495257479092464,
      "grad_norm": 5.383264064788818,
      "learning_rate": 3.251309082685316e-05,
      "loss": 0.897,
      "step": 20110
    },
    {
      "epoch": 1.050047620911451,
      "grad_norm": 4.041265964508057,
      "learning_rate": 3.250439260303046e-05,
      "loss": 0.8232,
      "step": 20120
    },
    {
      "epoch": 1.050569493913656,
      "grad_norm": 5.363142967224121,
      "learning_rate": 3.249569437920777e-05,
      "loss": 1.0484,
      "step": 20130
    },
    {
      "epoch": 1.051091366915861,
      "grad_norm": 4.388626575469971,
      "learning_rate": 3.2486996155385075e-05,
      "loss": 0.9209,
      "step": 20140
    },
    {
      "epoch": 1.051613239918066,
      "grad_norm": 4.128162860870361,
      "learning_rate": 3.247829793156238e-05,
      "loss": 0.9284,
      "step": 20150
    },
    {
      "epoch": 1.052135112920271,
      "grad_norm": 5.033400535583496,
      "learning_rate": 3.2469599707739676e-05,
      "loss": 0.9502,
      "step": 20160
    },
    {
      "epoch": 1.0526569859224757,
      "grad_norm": 4.1257829666137695,
      "learning_rate": 3.2460901483916986e-05,
      "loss": 0.925,
      "step": 20170
    },
    {
      "epoch": 1.0531788589246807,
      "grad_norm": 4.0081682205200195,
      "learning_rate": 3.245220326009429e-05,
      "loss": 0.89,
      "step": 20180
    },
    {
      "epoch": 1.0537007319268856,
      "grad_norm": 4.049612522125244,
      "learning_rate": 3.244350503627159e-05,
      "loss": 1.0576,
      "step": 20190
    },
    {
      "epoch": 1.0542226049290906,
      "grad_norm": 4.268924713134766,
      "learning_rate": 3.2434806812448897e-05,
      "loss": 0.8276,
      "step": 20200
    },
    {
      "epoch": 1.0547444779312953,
      "grad_norm": 4.840218544006348,
      "learning_rate": 3.24261085886262e-05,
      "loss": 0.9133,
      "step": 20210
    },
    {
      "epoch": 1.0552663509335003,
      "grad_norm": 4.778117656707764,
      "learning_rate": 3.241741036480351e-05,
      "loss": 0.8556,
      "step": 20220
    },
    {
      "epoch": 1.0557882239357053,
      "grad_norm": 4.213569641113281,
      "learning_rate": 3.2408712140980814e-05,
      "loss": 0.9066,
      "step": 20230
    },
    {
      "epoch": 1.0563100969379102,
      "grad_norm": 4.093928813934326,
      "learning_rate": 3.240001391715812e-05,
      "loss": 1.0252,
      "step": 20240
    },
    {
      "epoch": 1.056831969940115,
      "grad_norm": 4.465456485748291,
      "learning_rate": 3.239131569333542e-05,
      "loss": 0.8089,
      "step": 20250
    },
    {
      "epoch": 1.05735384294232,
      "grad_norm": 4.4445600509643555,
      "learning_rate": 3.2382617469512725e-05,
      "loss": 1.0173,
      "step": 20260
    },
    {
      "epoch": 1.057875715944525,
      "grad_norm": 5.412769794464111,
      "learning_rate": 3.2373919245690035e-05,
      "loss": 0.8984,
      "step": 20270
    },
    {
      "epoch": 1.0583975889467299,
      "grad_norm": 4.395097255706787,
      "learning_rate": 3.236522102186734e-05,
      "loss": 0.8392,
      "step": 20280
    },
    {
      "epoch": 1.0589194619489348,
      "grad_norm": 4.686988830566406,
      "learning_rate": 3.235652279804464e-05,
      "loss": 1.0178,
      "step": 20290
    },
    {
      "epoch": 1.0594413349511396,
      "grad_norm": 5.013243675231934,
      "learning_rate": 3.2347824574221946e-05,
      "loss": 0.9414,
      "step": 20300
    },
    {
      "epoch": 1.0599632079533445,
      "grad_norm": 4.1367669105529785,
      "learning_rate": 3.233912635039925e-05,
      "loss": 0.9855,
      "step": 20310
    },
    {
      "epoch": 1.0604850809555495,
      "grad_norm": 4.767741680145264,
      "learning_rate": 3.233042812657656e-05,
      "loss": 0.9306,
      "step": 20320
    },
    {
      "epoch": 1.0610069539577545,
      "grad_norm": 4.153706073760986,
      "learning_rate": 3.232172990275386e-05,
      "loss": 0.9341,
      "step": 20330
    },
    {
      "epoch": 1.0615288269599592,
      "grad_norm": 4.772757530212402,
      "learning_rate": 3.2313031678931167e-05,
      "loss": 0.9742,
      "step": 20340
    },
    {
      "epoch": 1.0620506999621642,
      "grad_norm": 4.436487674713135,
      "learning_rate": 3.230433345510847e-05,
      "loss": 0.9122,
      "step": 20350
    },
    {
      "epoch": 1.0625725729643691,
      "grad_norm": 3.855581760406494,
      "learning_rate": 3.2295635231285774e-05,
      "loss": 0.8667,
      "step": 20360
    },
    {
      "epoch": 1.063094445966574,
      "grad_norm": 4.24049711227417,
      "learning_rate": 3.228693700746308e-05,
      "loss": 0.91,
      "step": 20370
    },
    {
      "epoch": 1.063616318968779,
      "grad_norm": 3.8566086292266846,
      "learning_rate": 3.227823878364038e-05,
      "loss": 0.8419,
      "step": 20380
    },
    {
      "epoch": 1.0641381919709838,
      "grad_norm": 4.381486892700195,
      "learning_rate": 3.2269540559817684e-05,
      "loss": 0.9314,
      "step": 20390
    },
    {
      "epoch": 1.0646600649731888,
      "grad_norm": 5.141751289367676,
      "learning_rate": 3.226084233599499e-05,
      "loss": 0.894,
      "step": 20400
    },
    {
      "epoch": 1.0651819379753937,
      "grad_norm": 3.7484757900238037,
      "learning_rate": 3.225214411217229e-05,
      "loss": 0.889,
      "step": 20410
    },
    {
      "epoch": 1.0657038109775987,
      "grad_norm": 4.6777753829956055,
      "learning_rate": 3.22434458883496e-05,
      "loss": 0.9349,
      "step": 20420
    },
    {
      "epoch": 1.0662256839798034,
      "grad_norm": 4.144761085510254,
      "learning_rate": 3.2234747664526905e-05,
      "loss": 0.9492,
      "step": 20430
    },
    {
      "epoch": 1.0667475569820084,
      "grad_norm": 4.514299392700195,
      "learning_rate": 3.222604944070421e-05,
      "loss": 0.9465,
      "step": 20440
    },
    {
      "epoch": 1.0672694299842134,
      "grad_norm": 4.621492385864258,
      "learning_rate": 3.221735121688151e-05,
      "loss": 0.8751,
      "step": 20450
    },
    {
      "epoch": 1.0677913029864183,
      "grad_norm": 3.695905923843384,
      "learning_rate": 3.2208652993058816e-05,
      "loss": 0.912,
      "step": 20460
    },
    {
      "epoch": 1.068313175988623,
      "grad_norm": 4.896453857421875,
      "learning_rate": 3.2199954769236126e-05,
      "loss": 0.892,
      "step": 20470
    },
    {
      "epoch": 1.068835048990828,
      "grad_norm": 3.7271132469177246,
      "learning_rate": 3.219125654541343e-05,
      "loss": 0.9684,
      "step": 20480
    },
    {
      "epoch": 1.069356921993033,
      "grad_norm": 4.47481632232666,
      "learning_rate": 3.218255832159073e-05,
      "loss": 0.9291,
      "step": 20490
    },
    {
      "epoch": 1.069878794995238,
      "grad_norm": 4.1892170906066895,
      "learning_rate": 3.217386009776804e-05,
      "loss": 0.8798,
      "step": 20500
    },
    {
      "epoch": 1.070400667997443,
      "grad_norm": 3.8808326721191406,
      "learning_rate": 3.216516187394534e-05,
      "loss": 0.9146,
      "step": 20510
    },
    {
      "epoch": 1.0709225409996477,
      "grad_norm": 4.6553449630737305,
      "learning_rate": 3.215646365012265e-05,
      "loss": 1.0824,
      "step": 20520
    },
    {
      "epoch": 1.0714444140018526,
      "grad_norm": 4.026949882507324,
      "learning_rate": 3.2147765426299954e-05,
      "loss": 0.8346,
      "step": 20530
    },
    {
      "epoch": 1.0719662870040576,
      "grad_norm": 4.346516132354736,
      "learning_rate": 3.213906720247726e-05,
      "loss": 1.0249,
      "step": 20540
    },
    {
      "epoch": 1.0724881600062626,
      "grad_norm": 4.083127975463867,
      "learning_rate": 3.213036897865456e-05,
      "loss": 0.9299,
      "step": 20550
    },
    {
      "epoch": 1.0730100330084673,
      "grad_norm": 4.054990768432617,
      "learning_rate": 3.2121670754831865e-05,
      "loss": 0.9605,
      "step": 20560
    },
    {
      "epoch": 1.0735319060106723,
      "grad_norm": 3.716794729232788,
      "learning_rate": 3.211297253100917e-05,
      "loss": 1.0069,
      "step": 20570
    },
    {
      "epoch": 1.0740537790128772,
      "grad_norm": 3.6183483600616455,
      "learning_rate": 3.210427430718647e-05,
      "loss": 0.9735,
      "step": 20580
    },
    {
      "epoch": 1.0745756520150822,
      "grad_norm": 4.040185928344727,
      "learning_rate": 3.2095576083363776e-05,
      "loss": 0.7903,
      "step": 20590
    },
    {
      "epoch": 1.075097525017287,
      "grad_norm": 4.1364970207214355,
      "learning_rate": 3.208687785954108e-05,
      "loss": 0.9521,
      "step": 20600
    },
    {
      "epoch": 1.075619398019492,
      "grad_norm": 4.669419765472412,
      "learning_rate": 3.207817963571839e-05,
      "loss": 0.8424,
      "step": 20610
    },
    {
      "epoch": 1.0761412710216969,
      "grad_norm": 4.4708757400512695,
      "learning_rate": 3.206948141189569e-05,
      "loss": 0.9767,
      "step": 20620
    },
    {
      "epoch": 1.0766631440239018,
      "grad_norm": 3.9691052436828613,
      "learning_rate": 3.2060783188072997e-05,
      "loss": 0.9169,
      "step": 20630
    },
    {
      "epoch": 1.0771850170261068,
      "grad_norm": 4.607908725738525,
      "learning_rate": 3.20520849642503e-05,
      "loss": 0.9403,
      "step": 20640
    },
    {
      "epoch": 1.0777068900283115,
      "grad_norm": 4.743311882019043,
      "learning_rate": 3.2043386740427604e-05,
      "loss": 0.963,
      "step": 20650
    },
    {
      "epoch": 1.0782287630305165,
      "grad_norm": 4.336082458496094,
      "learning_rate": 3.203468851660491e-05,
      "loss": 0.9305,
      "step": 20660
    },
    {
      "epoch": 1.0787506360327215,
      "grad_norm": 4.194534778594971,
      "learning_rate": 3.202599029278222e-05,
      "loss": 0.8972,
      "step": 20670
    },
    {
      "epoch": 1.0792725090349264,
      "grad_norm": 3.8368539810180664,
      "learning_rate": 3.201729206895952e-05,
      "loss": 0.9556,
      "step": 20680
    },
    {
      "epoch": 1.0797943820371312,
      "grad_norm": 4.259963035583496,
      "learning_rate": 3.2008593845136825e-05,
      "loss": 0.8861,
      "step": 20690
    },
    {
      "epoch": 1.0803162550393361,
      "grad_norm": 4.6202311515808105,
      "learning_rate": 3.199989562131413e-05,
      "loss": 0.9825,
      "step": 20700
    },
    {
      "epoch": 1.0808381280415411,
      "grad_norm": 4.53098201751709,
      "learning_rate": 3.199119739749143e-05,
      "loss": 0.9439,
      "step": 20710
    },
    {
      "epoch": 1.081360001043746,
      "grad_norm": 5.0307111740112305,
      "learning_rate": 3.198249917366874e-05,
      "loss": 0.9746,
      "step": 20720
    },
    {
      "epoch": 1.0818818740459508,
      "grad_norm": 4.498754978179932,
      "learning_rate": 3.1973800949846046e-05,
      "loss": 0.9362,
      "step": 20730
    },
    {
      "epoch": 1.0824037470481558,
      "grad_norm": 4.251705646514893,
      "learning_rate": 3.196510272602335e-05,
      "loss": 0.9615,
      "step": 20740
    },
    {
      "epoch": 1.0829256200503607,
      "grad_norm": 3.7837679386138916,
      "learning_rate": 3.195640450220065e-05,
      "loss": 0.8937,
      "step": 20750
    },
    {
      "epoch": 1.0834474930525657,
      "grad_norm": 4.170478343963623,
      "learning_rate": 3.1947706278377956e-05,
      "loss": 0.8196,
      "step": 20760
    },
    {
      "epoch": 1.0839693660547707,
      "grad_norm": 4.256983280181885,
      "learning_rate": 3.193900805455527e-05,
      "loss": 0.8637,
      "step": 20770
    },
    {
      "epoch": 1.0844912390569754,
      "grad_norm": 4.893668174743652,
      "learning_rate": 3.1930309830732563e-05,
      "loss": 0.9523,
      "step": 20780
    },
    {
      "epoch": 1.0850131120591804,
      "grad_norm": 4.481866836547852,
      "learning_rate": 3.192161160690987e-05,
      "loss": 0.909,
      "step": 20790
    },
    {
      "epoch": 1.0855349850613853,
      "grad_norm": 4.172964096069336,
      "learning_rate": 3.191291338308717e-05,
      "loss": 0.9381,
      "step": 20800
    },
    {
      "epoch": 1.0860568580635903,
      "grad_norm": 4.144299507141113,
      "learning_rate": 3.190421515926448e-05,
      "loss": 0.9618,
      "step": 20810
    },
    {
      "epoch": 1.086578731065795,
      "grad_norm": 3.8001654148101807,
      "learning_rate": 3.1895516935441784e-05,
      "loss": 0.9456,
      "step": 20820
    },
    {
      "epoch": 1.087100604068,
      "grad_norm": 5.135472297668457,
      "learning_rate": 3.188681871161909e-05,
      "loss": 0.9217,
      "step": 20830
    },
    {
      "epoch": 1.087622477070205,
      "grad_norm": 4.211520671844482,
      "learning_rate": 3.187812048779639e-05,
      "loss": 0.9181,
      "step": 20840
    },
    {
      "epoch": 1.08814435007241,
      "grad_norm": 5.067038536071777,
      "learning_rate": 3.1869422263973695e-05,
      "loss": 1.0081,
      "step": 20850
    },
    {
      "epoch": 1.0886662230746147,
      "grad_norm": 4.175296306610107,
      "learning_rate": 3.1860724040151005e-05,
      "loss": 0.8741,
      "step": 20860
    },
    {
      "epoch": 1.0891880960768197,
      "grad_norm": 4.885464668273926,
      "learning_rate": 3.185202581632831e-05,
      "loss": 0.8875,
      "step": 20870
    },
    {
      "epoch": 1.0897099690790246,
      "grad_norm": 3.822244167327881,
      "learning_rate": 3.184332759250561e-05,
      "loss": 0.8967,
      "step": 20880
    },
    {
      "epoch": 1.0902318420812296,
      "grad_norm": 4.222369194030762,
      "learning_rate": 3.1834629368682916e-05,
      "loss": 0.9794,
      "step": 20890
    },
    {
      "epoch": 1.0907537150834345,
      "grad_norm": 3.9220824241638184,
      "learning_rate": 3.182593114486022e-05,
      "loss": 0.8123,
      "step": 20900
    },
    {
      "epoch": 1.0912755880856393,
      "grad_norm": 4.217059135437012,
      "learning_rate": 3.181723292103752e-05,
      "loss": 0.7893,
      "step": 20910
    },
    {
      "epoch": 1.0917974610878443,
      "grad_norm": 4.095812797546387,
      "learning_rate": 3.1808534697214833e-05,
      "loss": 0.8651,
      "step": 20920
    },
    {
      "epoch": 1.0923193340900492,
      "grad_norm": 5.248235702514648,
      "learning_rate": 3.179983647339214e-05,
      "loss": 0.9079,
      "step": 20930
    },
    {
      "epoch": 1.0928412070922542,
      "grad_norm": 4.5708417892456055,
      "learning_rate": 3.179113824956944e-05,
      "loss": 0.8506,
      "step": 20940
    },
    {
      "epoch": 1.093363080094459,
      "grad_norm": 3.542968988418579,
      "learning_rate": 3.1782440025746744e-05,
      "loss": 0.9002,
      "step": 20950
    },
    {
      "epoch": 1.093884953096664,
      "grad_norm": 4.655092716217041,
      "learning_rate": 3.177374180192405e-05,
      "loss": 0.8683,
      "step": 20960
    },
    {
      "epoch": 1.0944068260988689,
      "grad_norm": 4.7518510818481445,
      "learning_rate": 3.176504357810136e-05,
      "loss": 0.9098,
      "step": 20970
    },
    {
      "epoch": 1.0949286991010738,
      "grad_norm": 4.228362560272217,
      "learning_rate": 3.175634535427866e-05,
      "loss": 0.9343,
      "step": 20980
    },
    {
      "epoch": 1.0954505721032786,
      "grad_norm": 3.9188761711120605,
      "learning_rate": 3.174764713045596e-05,
      "loss": 0.9433,
      "step": 20990
    },
    {
      "epoch": 1.0959724451054835,
      "grad_norm": 3.64827036857605,
      "learning_rate": 3.173894890663326e-05,
      "loss": 0.9525,
      "step": 21000
    },
    {
      "epoch": 1.0964943181076885,
      "grad_norm": 4.262205123901367,
      "learning_rate": 3.173025068281057e-05,
      "loss": 0.9542,
      "step": 21010
    },
    {
      "epoch": 1.0970161911098935,
      "grad_norm": 4.19510555267334,
      "learning_rate": 3.1721552458987876e-05,
      "loss": 1.0149,
      "step": 21020
    },
    {
      "epoch": 1.0975380641120984,
      "grad_norm": 4.183012008666992,
      "learning_rate": 3.171285423516518e-05,
      "loss": 0.9841,
      "step": 21030
    },
    {
      "epoch": 1.0980599371143032,
      "grad_norm": 4.823692321777344,
      "learning_rate": 3.170415601134248e-05,
      "loss": 0.9222,
      "step": 21040
    },
    {
      "epoch": 1.0985818101165081,
      "grad_norm": 3.9728341102600098,
      "learning_rate": 3.1695457787519786e-05,
      "loss": 0.9664,
      "step": 21050
    },
    {
      "epoch": 1.099103683118713,
      "grad_norm": 4.430090427398682,
      "learning_rate": 3.16867595636971e-05,
      "loss": 0.9527,
      "step": 21060
    },
    {
      "epoch": 1.099625556120918,
      "grad_norm": 4.315768718719482,
      "learning_rate": 3.16780613398744e-05,
      "loss": 0.9846,
      "step": 21070
    },
    {
      "epoch": 1.1001474291231228,
      "grad_norm": 4.456768035888672,
      "learning_rate": 3.1669363116051704e-05,
      "loss": 0.9099,
      "step": 21080
    },
    {
      "epoch": 1.1006693021253278,
      "grad_norm": 3.8968756198883057,
      "learning_rate": 3.166066489222901e-05,
      "loss": 0.7887,
      "step": 21090
    },
    {
      "epoch": 1.1011911751275327,
      "grad_norm": 3.5477747917175293,
      "learning_rate": 3.165196666840631e-05,
      "loss": 0.841,
      "step": 21100
    },
    {
      "epoch": 1.1017130481297377,
      "grad_norm": 5.987664699554443,
      "learning_rate": 3.164326844458362e-05,
      "loss": 0.9895,
      "step": 21110
    },
    {
      "epoch": 1.1022349211319424,
      "grad_norm": 4.366005897521973,
      "learning_rate": 3.1634570220760925e-05,
      "loss": 0.8907,
      "step": 21120
    },
    {
      "epoch": 1.1027567941341474,
      "grad_norm": 4.2358717918396,
      "learning_rate": 3.162587199693823e-05,
      "loss": 0.902,
      "step": 21130
    },
    {
      "epoch": 1.1032786671363524,
      "grad_norm": 4.114589691162109,
      "learning_rate": 3.161717377311553e-05,
      "loss": 0.9388,
      "step": 21140
    },
    {
      "epoch": 1.1038005401385573,
      "grad_norm": 4.95801305770874,
      "learning_rate": 3.1608475549292835e-05,
      "loss": 1.0285,
      "step": 21150
    },
    {
      "epoch": 1.1043224131407623,
      "grad_norm": 4.78315544128418,
      "learning_rate": 3.1599777325470146e-05,
      "loss": 0.8657,
      "step": 21160
    },
    {
      "epoch": 1.104844286142967,
      "grad_norm": 3.4718658924102783,
      "learning_rate": 3.159107910164745e-05,
      "loss": 0.9317,
      "step": 21170
    },
    {
      "epoch": 1.105366159145172,
      "grad_norm": 3.864002227783203,
      "learning_rate": 3.158238087782475e-05,
      "loss": 0.8684,
      "step": 21180
    },
    {
      "epoch": 1.105888032147377,
      "grad_norm": 3.9545693397521973,
      "learning_rate": 3.1573682654002056e-05,
      "loss": 0.8775,
      "step": 21190
    },
    {
      "epoch": 1.106409905149582,
      "grad_norm": 4.475011348724365,
      "learning_rate": 3.1565854252561626e-05,
      "loss": 1.0032,
      "step": 21200
    },
    {
      "epoch": 1.1069317781517867,
      "grad_norm": 4.366229057312012,
      "learning_rate": 3.155715602873893e-05,
      "loss": 0.9994,
      "step": 21210
    },
    {
      "epoch": 1.1074536511539916,
      "grad_norm": 4.329071521759033,
      "learning_rate": 3.154845780491624e-05,
      "loss": 0.8894,
      "step": 21220
    },
    {
      "epoch": 1.1079755241561966,
      "grad_norm": 4.394219398498535,
      "learning_rate": 3.1539759581093544e-05,
      "loss": 0.9346,
      "step": 21230
    },
    {
      "epoch": 1.1084973971584016,
      "grad_norm": 4.0009894371032715,
      "learning_rate": 3.153106135727085e-05,
      "loss": 0.9221,
      "step": 21240
    },
    {
      "epoch": 1.1090192701606063,
      "grad_norm": 4.401872634887695,
      "learning_rate": 3.152236313344815e-05,
      "loss": 1.0269,
      "step": 21250
    },
    {
      "epoch": 1.1095411431628113,
      "grad_norm": 4.378276348114014,
      "learning_rate": 3.1513664909625454e-05,
      "loss": 1.0203,
      "step": 21260
    },
    {
      "epoch": 1.1100630161650162,
      "grad_norm": 3.5626139640808105,
      "learning_rate": 3.1504966685802765e-05,
      "loss": 0.8544,
      "step": 21270
    },
    {
      "epoch": 1.1105848891672212,
      "grad_norm": 3.791947603225708,
      "learning_rate": 3.149626846198007e-05,
      "loss": 0.9085,
      "step": 21280
    },
    {
      "epoch": 1.1111067621694262,
      "grad_norm": 3.9593355655670166,
      "learning_rate": 3.148757023815737e-05,
      "loss": 0.978,
      "step": 21290
    },
    {
      "epoch": 1.111628635171631,
      "grad_norm": 4.5296807289123535,
      "learning_rate": 3.1478872014334675e-05,
      "loss": 0.8676,
      "step": 21300
    },
    {
      "epoch": 1.1121505081738359,
      "grad_norm": 4.562293529510498,
      "learning_rate": 3.147017379051198e-05,
      "loss": 0.8867,
      "step": 21310
    },
    {
      "epoch": 1.1126723811760408,
      "grad_norm": 4.119987964630127,
      "learning_rate": 3.146147556668929e-05,
      "loss": 0.9221,
      "step": 21320
    },
    {
      "epoch": 1.1131942541782458,
      "grad_norm": 3.579740524291992,
      "learning_rate": 3.145277734286659e-05,
      "loss": 0.8836,
      "step": 21330
    },
    {
      "epoch": 1.1137161271804505,
      "grad_norm": 4.291860103607178,
      "learning_rate": 3.1444079119043896e-05,
      "loss": 0.9047,
      "step": 21340
    },
    {
      "epoch": 1.1142380001826555,
      "grad_norm": 5.214565753936768,
      "learning_rate": 3.143538089522119e-05,
      "loss": 0.9656,
      "step": 21350
    },
    {
      "epoch": 1.1147598731848605,
      "grad_norm": 5.206131458282471,
      "learning_rate": 3.1426682671398496e-05,
      "loss": 0.9061,
      "step": 21360
    },
    {
      "epoch": 1.1152817461870654,
      "grad_norm": 4.377364635467529,
      "learning_rate": 3.141798444757581e-05,
      "loss": 0.8728,
      "step": 21370
    },
    {
      "epoch": 1.1158036191892702,
      "grad_norm": 4.3371124267578125,
      "learning_rate": 3.140928622375311e-05,
      "loss": 1.0863,
      "step": 21380
    },
    {
      "epoch": 1.1163254921914751,
      "grad_norm": 4.612508773803711,
      "learning_rate": 3.1400587999930414e-05,
      "loss": 1.0525,
      "step": 21390
    },
    {
      "epoch": 1.11684736519368,
      "grad_norm": 4.325464248657227,
      "learning_rate": 3.139188977610772e-05,
      "loss": 0.8474,
      "step": 21400
    },
    {
      "epoch": 1.117369238195885,
      "grad_norm": 4.60832405090332,
      "learning_rate": 3.138319155228502e-05,
      "loss": 0.8727,
      "step": 21410
    },
    {
      "epoch": 1.11789111119809,
      "grad_norm": 3.573117256164551,
      "learning_rate": 3.137449332846233e-05,
      "loss": 0.8469,
      "step": 21420
    },
    {
      "epoch": 1.1184129842002948,
      "grad_norm": 4.525783538818359,
      "learning_rate": 3.1365795104639635e-05,
      "loss": 0.9888,
      "step": 21430
    },
    {
      "epoch": 1.1189348572024997,
      "grad_norm": 4.610989570617676,
      "learning_rate": 3.135709688081694e-05,
      "loss": 0.9034,
      "step": 21440
    },
    {
      "epoch": 1.1194567302047047,
      "grad_norm": 3.6221907138824463,
      "learning_rate": 3.134839865699424e-05,
      "loss": 0.8235,
      "step": 21450
    },
    {
      "epoch": 1.1199786032069097,
      "grad_norm": 3.8885202407836914,
      "learning_rate": 3.1339700433171546e-05,
      "loss": 0.91,
      "step": 21460
    },
    {
      "epoch": 1.1205004762091144,
      "grad_norm": 3.529552698135376,
      "learning_rate": 3.1331002209348856e-05,
      "loss": 0.9642,
      "step": 21470
    },
    {
      "epoch": 1.1210223492113194,
      "grad_norm": 4.644674777984619,
      "learning_rate": 3.132230398552616e-05,
      "loss": 0.8792,
      "step": 21480
    },
    {
      "epoch": 1.1215442222135243,
      "grad_norm": 4.988848686218262,
      "learning_rate": 3.131360576170346e-05,
      "loss": 0.9523,
      "step": 21490
    },
    {
      "epoch": 1.1220660952157293,
      "grad_norm": 4.363224983215332,
      "learning_rate": 3.1304907537880766e-05,
      "loss": 0.8785,
      "step": 21500
    },
    {
      "epoch": 1.122587968217934,
      "grad_norm": 5.218181610107422,
      "learning_rate": 3.129620931405807e-05,
      "loss": 0.9115,
      "step": 21510
    },
    {
      "epoch": 1.123109841220139,
      "grad_norm": 4.257721900939941,
      "learning_rate": 3.128751109023538e-05,
      "loss": 0.9507,
      "step": 21520
    },
    {
      "epoch": 1.123631714222344,
      "grad_norm": 4.656651020050049,
      "learning_rate": 3.1278812866412684e-05,
      "loss": 0.8604,
      "step": 21530
    },
    {
      "epoch": 1.124153587224549,
      "grad_norm": 4.789514541625977,
      "learning_rate": 3.127011464258999e-05,
      "loss": 0.8869,
      "step": 21540
    },
    {
      "epoch": 1.124675460226754,
      "grad_norm": 4.1086225509643555,
      "learning_rate": 3.126141641876729e-05,
      "loss": 1.0508,
      "step": 21550
    },
    {
      "epoch": 1.1251973332289587,
      "grad_norm": 5.147448539733887,
      "learning_rate": 3.1252718194944595e-05,
      "loss": 0.922,
      "step": 21560
    },
    {
      "epoch": 1.1257192062311636,
      "grad_norm": 4.737381458282471,
      "learning_rate": 3.12440199711219e-05,
      "loss": 0.9307,
      "step": 21570
    },
    {
      "epoch": 1.1262410792333686,
      "grad_norm": 2.9072206020355225,
      "learning_rate": 3.12353217472992e-05,
      "loss": 0.8622,
      "step": 21580
    },
    {
      "epoch": 1.1267629522355735,
      "grad_norm": 4.217147350311279,
      "learning_rate": 3.1226623523476505e-05,
      "loss": 0.9322,
      "step": 21590
    },
    {
      "epoch": 1.1272848252377785,
      "grad_norm": 4.810601234436035,
      "learning_rate": 3.121792529965381e-05,
      "loss": 0.8711,
      "step": 21600
    },
    {
      "epoch": 1.1278066982399833,
      "grad_norm": 4.48790168762207,
      "learning_rate": 3.120922707583111e-05,
      "loss": 0.8227,
      "step": 21610
    },
    {
      "epoch": 1.1283285712421882,
      "grad_norm": 5.092573165893555,
      "learning_rate": 3.120052885200842e-05,
      "loss": 0.9222,
      "step": 21620
    },
    {
      "epoch": 1.1288504442443932,
      "grad_norm": 5.411190032958984,
      "learning_rate": 3.1191830628185726e-05,
      "loss": 0.9865,
      "step": 21630
    },
    {
      "epoch": 1.129372317246598,
      "grad_norm": 3.88069748878479,
      "learning_rate": 3.118313240436303e-05,
      "loss": 0.9346,
      "step": 21640
    },
    {
      "epoch": 1.1298941902488029,
      "grad_norm": 4.290396213531494,
      "learning_rate": 3.117443418054033e-05,
      "loss": 0.9426,
      "step": 21650
    },
    {
      "epoch": 1.1304160632510079,
      "grad_norm": 4.071400165557861,
      "learning_rate": 3.116573595671764e-05,
      "loss": 0.9196,
      "step": 21660
    },
    {
      "epoch": 1.1309379362532128,
      "grad_norm": 4.565057754516602,
      "learning_rate": 3.115703773289495e-05,
      "loss": 0.9592,
      "step": 21670
    },
    {
      "epoch": 1.1314598092554178,
      "grad_norm": 3.954906463623047,
      "learning_rate": 3.114833950907225e-05,
      "loss": 0.8999,
      "step": 21680
    },
    {
      "epoch": 1.1319816822576225,
      "grad_norm": 4.235265731811523,
      "learning_rate": 3.1139641285249554e-05,
      "loss": 0.8632,
      "step": 21690
    },
    {
      "epoch": 1.1325035552598275,
      "grad_norm": 4.152727127075195,
      "learning_rate": 3.113094306142686e-05,
      "loss": 0.9101,
      "step": 21700
    },
    {
      "epoch": 1.1330254282620325,
      "grad_norm": 4.141358375549316,
      "learning_rate": 3.112224483760416e-05,
      "loss": 0.9293,
      "step": 21710
    },
    {
      "epoch": 1.1335473012642374,
      "grad_norm": 4.241705417633057,
      "learning_rate": 3.111354661378147e-05,
      "loss": 0.8151,
      "step": 21720
    },
    {
      "epoch": 1.1340691742664424,
      "grad_norm": 4.517250061035156,
      "learning_rate": 3.1104848389958775e-05,
      "loss": 0.9375,
      "step": 21730
    },
    {
      "epoch": 1.1345910472686471,
      "grad_norm": 4.551843643188477,
      "learning_rate": 3.109615016613608e-05,
      "loss": 0.9916,
      "step": 21740
    },
    {
      "epoch": 1.135112920270852,
      "grad_norm": 5.018312454223633,
      "learning_rate": 3.108745194231338e-05,
      "loss": 0.9623,
      "step": 21750
    },
    {
      "epoch": 1.135634793273057,
      "grad_norm": 3.9458060264587402,
      "learning_rate": 3.1078753718490686e-05,
      "loss": 0.9383,
      "step": 21760
    },
    {
      "epoch": 1.1361566662752618,
      "grad_norm": 4.806337356567383,
      "learning_rate": 3.107005549466799e-05,
      "loss": 1.0409,
      "step": 21770
    },
    {
      "epoch": 1.1366785392774668,
      "grad_norm": 4.787307262420654,
      "learning_rate": 3.106135727084529e-05,
      "loss": 1.028,
      "step": 21780
    },
    {
      "epoch": 1.1372004122796717,
      "grad_norm": 4.510919570922852,
      "learning_rate": 3.1052659047022597e-05,
      "loss": 0.9345,
      "step": 21790
    },
    {
      "epoch": 1.1377222852818767,
      "grad_norm": 5.3183512687683105,
      "learning_rate": 3.10439608231999e-05,
      "loss": 0.997,
      "step": 21800
    },
    {
      "epoch": 1.1382441582840817,
      "grad_norm": 3.436565399169922,
      "learning_rate": 3.103526259937721e-05,
      "loss": 0.8365,
      "step": 21810
    },
    {
      "epoch": 1.1387660312862864,
      "grad_norm": 4.41291618347168,
      "learning_rate": 3.1026564375554514e-05,
      "loss": 0.9602,
      "step": 21820
    },
    {
      "epoch": 1.1392879042884914,
      "grad_norm": 4.92413330078125,
      "learning_rate": 3.101786615173182e-05,
      "loss": 0.9329,
      "step": 21830
    },
    {
      "epoch": 1.1398097772906963,
      "grad_norm": 5.17400598526001,
      "learning_rate": 3.100916792790912e-05,
      "loss": 0.9604,
      "step": 21840
    },
    {
      "epoch": 1.1403316502929013,
      "grad_norm": 4.000039100646973,
      "learning_rate": 3.1000469704086425e-05,
      "loss": 0.9342,
      "step": 21850
    },
    {
      "epoch": 1.1408535232951063,
      "grad_norm": 4.874218463897705,
      "learning_rate": 3.0991771480263735e-05,
      "loss": 0.9651,
      "step": 21860
    },
    {
      "epoch": 1.141375396297311,
      "grad_norm": 4.958019256591797,
      "learning_rate": 3.098307325644104e-05,
      "loss": 0.9699,
      "step": 21870
    },
    {
      "epoch": 1.141897269299516,
      "grad_norm": 3.895763397216797,
      "learning_rate": 3.097437503261834e-05,
      "loss": 0.8481,
      "step": 21880
    },
    {
      "epoch": 1.142419142301721,
      "grad_norm": 4.637277126312256,
      "learning_rate": 3.0965676808795646e-05,
      "loss": 0.9869,
      "step": 21890
    },
    {
      "epoch": 1.1429410153039257,
      "grad_norm": 5.069997310638428,
      "learning_rate": 3.095697858497295e-05,
      "loss": 0.8998,
      "step": 21900
    },
    {
      "epoch": 1.1434628883061306,
      "grad_norm": 3.985403299331665,
      "learning_rate": 3.094828036115025e-05,
      "loss": 0.8603,
      "step": 21910
    },
    {
      "epoch": 1.1439847613083356,
      "grad_norm": 4.209633827209473,
      "learning_rate": 3.093958213732756e-05,
      "loss": 0.9043,
      "step": 21920
    },
    {
      "epoch": 1.1445066343105406,
      "grad_norm": 4.7280707359313965,
      "learning_rate": 3.0930883913504867e-05,
      "loss": 0.9519,
      "step": 21930
    },
    {
      "epoch": 1.1450285073127455,
      "grad_norm": 4.842629432678223,
      "learning_rate": 3.092218568968217e-05,
      "loss": 0.9936,
      "step": 21940
    },
    {
      "epoch": 1.1455503803149503,
      "grad_norm": 3.677990198135376,
      "learning_rate": 3.0913487465859474e-05,
      "loss": 0.9443,
      "step": 21950
    },
    {
      "epoch": 1.1460722533171552,
      "grad_norm": 4.848618507385254,
      "learning_rate": 3.090478924203678e-05,
      "loss": 0.9348,
      "step": 21960
    },
    {
      "epoch": 1.1465941263193602,
      "grad_norm": 4.073791980743408,
      "learning_rate": 3.089609101821408e-05,
      "loss": 1.0338,
      "step": 21970
    },
    {
      "epoch": 1.1471159993215652,
      "grad_norm": 4.440752983093262,
      "learning_rate": 3.0887392794391384e-05,
      "loss": 0.9127,
      "step": 21980
    },
    {
      "epoch": 1.1476378723237701,
      "grad_norm": 4.348321437835693,
      "learning_rate": 3.087869457056869e-05,
      "loss": 0.9487,
      "step": 21990
    },
    {
      "epoch": 1.1481597453259749,
      "grad_norm": 4.734807014465332,
      "learning_rate": 3.086999634674599e-05,
      "loss": 0.881,
      "step": 22000
    },
    {
      "epoch": 1.1486816183281798,
      "grad_norm": 4.185235023498535,
      "learning_rate": 3.08612981229233e-05,
      "loss": 0.8882,
      "step": 22010
    },
    {
      "epoch": 1.1492034913303848,
      "grad_norm": 4.817497253417969,
      "learning_rate": 3.0852599899100605e-05,
      "loss": 0.8299,
      "step": 22020
    },
    {
      "epoch": 1.1497253643325898,
      "grad_norm": 4.692025661468506,
      "learning_rate": 3.084390167527791e-05,
      "loss": 0.8864,
      "step": 22030
    },
    {
      "epoch": 1.1502472373347945,
      "grad_norm": 4.735044956207275,
      "learning_rate": 3.083520345145521e-05,
      "loss": 0.932,
      "step": 22040
    },
    {
      "epoch": 1.1507691103369995,
      "grad_norm": 4.329746246337891,
      "learning_rate": 3.0826505227632516e-05,
      "loss": 0.9816,
      "step": 22050
    },
    {
      "epoch": 1.1512909833392044,
      "grad_norm": 4.268415927886963,
      "learning_rate": 3.0817807003809826e-05,
      "loss": 0.9274,
      "step": 22060
    },
    {
      "epoch": 1.1518128563414094,
      "grad_norm": 4.767779350280762,
      "learning_rate": 3.080910877998713e-05,
      "loss": 0.9481,
      "step": 22070
    },
    {
      "epoch": 1.1523347293436141,
      "grad_norm": 5.185668468475342,
      "learning_rate": 3.080041055616443e-05,
      "loss": 0.8888,
      "step": 22080
    },
    {
      "epoch": 1.152856602345819,
      "grad_norm": 4.22519063949585,
      "learning_rate": 3.079171233234174e-05,
      "loss": 0.9824,
      "step": 22090
    },
    {
      "epoch": 1.153378475348024,
      "grad_norm": 4.339265823364258,
      "learning_rate": 3.078301410851904e-05,
      "loss": 0.9925,
      "step": 22100
    },
    {
      "epoch": 1.153900348350229,
      "grad_norm": 2.999880313873291,
      "learning_rate": 3.077431588469635e-05,
      "loss": 0.8469,
      "step": 22110
    },
    {
      "epoch": 1.154422221352434,
      "grad_norm": 3.6719136238098145,
      "learning_rate": 3.0765617660873654e-05,
      "loss": 0.8472,
      "step": 22120
    },
    {
      "epoch": 1.1549440943546387,
      "grad_norm": 4.194770812988281,
      "learning_rate": 3.075691943705096e-05,
      "loss": 0.8997,
      "step": 22130
    },
    {
      "epoch": 1.1554659673568437,
      "grad_norm": 5.348006725311279,
      "learning_rate": 3.074822121322826e-05,
      "loss": 0.8841,
      "step": 22140
    },
    {
      "epoch": 1.1559878403590487,
      "grad_norm": 4.557192325592041,
      "learning_rate": 3.0739522989405565e-05,
      "loss": 0.8814,
      "step": 22150
    },
    {
      "epoch": 1.1565097133612536,
      "grad_norm": 4.891847133636475,
      "learning_rate": 3.073082476558287e-05,
      "loss": 1.0166,
      "step": 22160
    },
    {
      "epoch": 1.1570315863634584,
      "grad_norm": 4.371206283569336,
      "learning_rate": 3.072212654176018e-05,
      "loss": 1.0169,
      "step": 22170
    },
    {
      "epoch": 1.1575534593656633,
      "grad_norm": 4.483578681945801,
      "learning_rate": 3.0713428317937476e-05,
      "loss": 0.9481,
      "step": 22180
    },
    {
      "epoch": 1.1580753323678683,
      "grad_norm": 4.011980056762695,
      "learning_rate": 3.070473009411478e-05,
      "loss": 0.9607,
      "step": 22190
    },
    {
      "epoch": 1.1585972053700733,
      "grad_norm": 4.5771284103393555,
      "learning_rate": 3.069603187029208e-05,
      "loss": 0.8816,
      "step": 22200
    },
    {
      "epoch": 1.159119078372278,
      "grad_norm": 4.00177526473999,
      "learning_rate": 3.068733364646939e-05,
      "loss": 0.9138,
      "step": 22210
    },
    {
      "epoch": 1.159640951374483,
      "grad_norm": 5.151308059692383,
      "learning_rate": 3.0678635422646697e-05,
      "loss": 0.9412,
      "step": 22220
    },
    {
      "epoch": 1.160162824376688,
      "grad_norm": 4.87066650390625,
      "learning_rate": 3.0669937198824e-05,
      "loss": 0.8613,
      "step": 22230
    },
    {
      "epoch": 1.160684697378893,
      "grad_norm": 4.589944839477539,
      "learning_rate": 3.0661238975001304e-05,
      "loss": 0.9346,
      "step": 22240
    },
    {
      "epoch": 1.1612065703810979,
      "grad_norm": 3.96250319480896,
      "learning_rate": 3.065254075117861e-05,
      "loss": 0.9437,
      "step": 22250
    },
    {
      "epoch": 1.1617284433833026,
      "grad_norm": 3.3495476245880127,
      "learning_rate": 3.064384252735592e-05,
      "loss": 0.9615,
      "step": 22260
    },
    {
      "epoch": 1.1622503163855076,
      "grad_norm": 4.314621448516846,
      "learning_rate": 3.063514430353322e-05,
      "loss": 0.8475,
      "step": 22270
    },
    {
      "epoch": 1.1627721893877125,
      "grad_norm": 4.675779819488525,
      "learning_rate": 3.0626446079710525e-05,
      "loss": 0.8164,
      "step": 22280
    },
    {
      "epoch": 1.1632940623899175,
      "grad_norm": 3.886964797973633,
      "learning_rate": 3.061774785588783e-05,
      "loss": 0.9062,
      "step": 22290
    },
    {
      "epoch": 1.1638159353921222,
      "grad_norm": 5.243348598480225,
      "learning_rate": 3.060904963206513e-05,
      "loss": 0.8969,
      "step": 22300
    },
    {
      "epoch": 1.1643378083943272,
      "grad_norm": 3.776235580444336,
      "learning_rate": 3.060035140824244e-05,
      "loss": 0.9545,
      "step": 22310
    },
    {
      "epoch": 1.1648596813965322,
      "grad_norm": 4.6505327224731445,
      "learning_rate": 3.0591653184419746e-05,
      "loss": 0.97,
      "step": 22320
    },
    {
      "epoch": 1.1653815543987371,
      "grad_norm": 4.563047409057617,
      "learning_rate": 3.058295496059705e-05,
      "loss": 0.9352,
      "step": 22330
    },
    {
      "epoch": 1.1659034274009419,
      "grad_norm": 3.7742252349853516,
      "learning_rate": 3.057425673677435e-05,
      "loss": 0.8898,
      "step": 22340
    },
    {
      "epoch": 1.1664253004031468,
      "grad_norm": 4.232722282409668,
      "learning_rate": 3.0565558512951656e-05,
      "loss": 0.9254,
      "step": 22350
    },
    {
      "epoch": 1.1669471734053518,
      "grad_norm": 4.476244926452637,
      "learning_rate": 3.0556860289128967e-05,
      "loss": 0.9429,
      "step": 22360
    },
    {
      "epoch": 1.1674690464075568,
      "grad_norm": 4.160396099090576,
      "learning_rate": 3.054816206530627e-05,
      "loss": 0.8822,
      "step": 22370
    },
    {
      "epoch": 1.1679909194097617,
      "grad_norm": 3.524831771850586,
      "learning_rate": 3.0539463841483574e-05,
      "loss": 0.9204,
      "step": 22380
    },
    {
      "epoch": 1.1685127924119665,
      "grad_norm": 5.919838905334473,
      "learning_rate": 3.053076561766087e-05,
      "loss": 0.8671,
      "step": 22390
    },
    {
      "epoch": 1.1690346654141714,
      "grad_norm": 4.429348945617676,
      "learning_rate": 3.0522067393838174e-05,
      "loss": 0.8526,
      "step": 22400
    },
    {
      "epoch": 1.1695565384163764,
      "grad_norm": 4.2584123611450195,
      "learning_rate": 3.051336917001548e-05,
      "loss": 0.956,
      "step": 22410
    },
    {
      "epoch": 1.1700784114185814,
      "grad_norm": 3.8281474113464355,
      "learning_rate": 3.0504670946192788e-05,
      "loss": 0.862,
      "step": 22420
    },
    {
      "epoch": 1.1706002844207861,
      "grad_norm": 4.981677532196045,
      "learning_rate": 3.049597272237009e-05,
      "loss": 0.871,
      "step": 22430
    },
    {
      "epoch": 1.171122157422991,
      "grad_norm": 4.328861713409424,
      "learning_rate": 3.04872744985474e-05,
      "loss": 0.982,
      "step": 22440
    },
    {
      "epoch": 1.171644030425196,
      "grad_norm": 4.3440070152282715,
      "learning_rate": 3.0478576274724702e-05,
      "loss": 0.964,
      "step": 22450
    },
    {
      "epoch": 1.172165903427401,
      "grad_norm": 3.563624858856201,
      "learning_rate": 3.0469878050902005e-05,
      "loss": 0.8451,
      "step": 22460
    },
    {
      "epoch": 1.1726877764296058,
      "grad_norm": 4.247690677642822,
      "learning_rate": 3.0461179827079312e-05,
      "loss": 0.9065,
      "step": 22470
    },
    {
      "epoch": 1.1732096494318107,
      "grad_norm": 3.9670512676239014,
      "learning_rate": 3.0452481603256616e-05,
      "loss": 0.9438,
      "step": 22480
    },
    {
      "epoch": 1.1737315224340157,
      "grad_norm": 3.917483329772949,
      "learning_rate": 3.044378337943392e-05,
      "loss": 0.9254,
      "step": 22490
    },
    {
      "epoch": 1.1742533954362206,
      "grad_norm": 3.9944002628326416,
      "learning_rate": 3.0435085155611226e-05,
      "loss": 0.9161,
      "step": 22500
    },
    {
      "epoch": 1.1747752684384256,
      "grad_norm": 4.1227521896362305,
      "learning_rate": 3.042638693178853e-05,
      "loss": 0.9239,
      "step": 22510
    },
    {
      "epoch": 1.1752971414406304,
      "grad_norm": 3.8179233074188232,
      "learning_rate": 3.0417688707965837e-05,
      "loss": 0.9352,
      "step": 22520
    },
    {
      "epoch": 1.1758190144428353,
      "grad_norm": 4.0265212059021,
      "learning_rate": 3.040899048414314e-05,
      "loss": 0.9126,
      "step": 22530
    },
    {
      "epoch": 1.1763408874450403,
      "grad_norm": 3.9516851902008057,
      "learning_rate": 3.0400292260320444e-05,
      "loss": 0.8848,
      "step": 22540
    },
    {
      "epoch": 1.1768627604472452,
      "grad_norm": 4.6800360679626465,
      "learning_rate": 3.039159403649775e-05,
      "loss": 0.996,
      "step": 22550
    },
    {
      "epoch": 1.17738463344945,
      "grad_norm": 5.220006942749023,
      "learning_rate": 3.0382895812675054e-05,
      "loss": 0.9376,
      "step": 22560
    },
    {
      "epoch": 1.177906506451655,
      "grad_norm": 3.88671612739563,
      "learning_rate": 3.037419758885236e-05,
      "loss": 0.881,
      "step": 22570
    },
    {
      "epoch": 1.17842837945386,
      "grad_norm": 4.07728385925293,
      "learning_rate": 3.0365499365029665e-05,
      "loss": 0.9313,
      "step": 22580
    },
    {
      "epoch": 1.1789502524560649,
      "grad_norm": 4.42210054397583,
      "learning_rate": 3.035680114120697e-05,
      "loss": 0.9685,
      "step": 22590
    },
    {
      "epoch": 1.1794721254582696,
      "grad_norm": 4.235950469970703,
      "learning_rate": 3.034810291738427e-05,
      "loss": 0.8251,
      "step": 22600
    },
    {
      "epoch": 1.1799939984604746,
      "grad_norm": 4.840875148773193,
      "learning_rate": 3.0339404693561572e-05,
      "loss": 0.8235,
      "step": 22610
    },
    {
      "epoch": 1.1805158714626796,
      "grad_norm": 3.7522120475769043,
      "learning_rate": 3.033070646973888e-05,
      "loss": 0.9631,
      "step": 22620
    },
    {
      "epoch": 1.1810377444648845,
      "grad_norm": 4.499439716339111,
      "learning_rate": 3.0322008245916183e-05,
      "loss": 0.9833,
      "step": 22630
    },
    {
      "epoch": 1.1815596174670895,
      "grad_norm": 4.06912088394165,
      "learning_rate": 3.031331002209349e-05,
      "loss": 0.9084,
      "step": 22640
    },
    {
      "epoch": 1.1820814904692942,
      "grad_norm": 4.897435665130615,
      "learning_rate": 3.0304611798270793e-05,
      "loss": 0.8673,
      "step": 22650
    },
    {
      "epoch": 1.1826033634714992,
      "grad_norm": 4.212364196777344,
      "learning_rate": 3.0295913574448097e-05,
      "loss": 0.9038,
      "step": 22660
    },
    {
      "epoch": 1.1831252364737042,
      "grad_norm": 4.236741542816162,
      "learning_rate": 3.0287215350625404e-05,
      "loss": 0.8827,
      "step": 22670
    },
    {
      "epoch": 1.1836471094759091,
      "grad_norm": 4.226964950561523,
      "learning_rate": 3.0278517126802707e-05,
      "loss": 0.9825,
      "step": 22680
    },
    {
      "epoch": 1.1841689824781139,
      "grad_norm": 3.053539752960205,
      "learning_rate": 3.0269818902980014e-05,
      "loss": 0.8319,
      "step": 22690
    },
    {
      "epoch": 1.1846908554803188,
      "grad_norm": 4.191862106323242,
      "learning_rate": 3.0261120679157318e-05,
      "loss": 0.9193,
      "step": 22700
    },
    {
      "epoch": 1.1852127284825238,
      "grad_norm": 3.6549956798553467,
      "learning_rate": 3.025242245533462e-05,
      "loss": 0.929,
      "step": 22710
    },
    {
      "epoch": 1.1857346014847288,
      "grad_norm": 3.3463361263275146,
      "learning_rate": 3.0243724231511928e-05,
      "loss": 0.8517,
      "step": 22720
    },
    {
      "epoch": 1.1862564744869335,
      "grad_norm": 3.7875616550445557,
      "learning_rate": 3.0235026007689232e-05,
      "loss": 0.8449,
      "step": 22730
    },
    {
      "epoch": 1.1867783474891385,
      "grad_norm": 4.11353063583374,
      "learning_rate": 3.022632778386654e-05,
      "loss": 0.8857,
      "step": 22740
    },
    {
      "epoch": 1.1873002204913434,
      "grad_norm": 4.035567283630371,
      "learning_rate": 3.0217629560043842e-05,
      "loss": 0.8793,
      "step": 22750
    },
    {
      "epoch": 1.1878220934935484,
      "grad_norm": 4.745903015136719,
      "learning_rate": 3.0208931336221146e-05,
      "loss": 1.0176,
      "step": 22760
    },
    {
      "epoch": 1.1883439664957534,
      "grad_norm": 3.870138645172119,
      "learning_rate": 3.0200233112398453e-05,
      "loss": 0.9421,
      "step": 22770
    },
    {
      "epoch": 1.188865839497958,
      "grad_norm": 3.4347546100616455,
      "learning_rate": 3.0191534888575756e-05,
      "loss": 0.9222,
      "step": 22780
    },
    {
      "epoch": 1.189387712500163,
      "grad_norm": 4.629244327545166,
      "learning_rate": 3.018283666475306e-05,
      "loss": 1.0213,
      "step": 22790
    },
    {
      "epoch": 1.189909585502368,
      "grad_norm": 4.2523627281188965,
      "learning_rate": 3.0174138440930367e-05,
      "loss": 0.9412,
      "step": 22800
    },
    {
      "epoch": 1.190431458504573,
      "grad_norm": 4.040578365325928,
      "learning_rate": 3.0165440217107667e-05,
      "loss": 0.9451,
      "step": 22810
    },
    {
      "epoch": 1.1909533315067777,
      "grad_norm": 4.818055152893066,
      "learning_rate": 3.015674199328497e-05,
      "loss": 0.9129,
      "step": 22820
    },
    {
      "epoch": 1.1914752045089827,
      "grad_norm": 4.141822338104248,
      "learning_rate": 3.0148043769462274e-05,
      "loss": 0.9053,
      "step": 22830
    },
    {
      "epoch": 1.1919970775111877,
      "grad_norm": 5.074376106262207,
      "learning_rate": 3.013934554563958e-05,
      "loss": 0.9614,
      "step": 22840
    },
    {
      "epoch": 1.1925189505133926,
      "grad_norm": 3.7144339084625244,
      "learning_rate": 3.0130647321816885e-05,
      "loss": 0.926,
      "step": 22850
    },
    {
      "epoch": 1.1930408235155974,
      "grad_norm": 3.957568407058716,
      "learning_rate": 3.012194909799419e-05,
      "loss": 0.8582,
      "step": 22860
    },
    {
      "epoch": 1.1935626965178023,
      "grad_norm": 3.333991050720215,
      "learning_rate": 3.0113250874171495e-05,
      "loss": 0.9188,
      "step": 22870
    },
    {
      "epoch": 1.1940845695200073,
      "grad_norm": 4.139097690582275,
      "learning_rate": 3.01045526503488e-05,
      "loss": 0.9336,
      "step": 22880
    },
    {
      "epoch": 1.1946064425222123,
      "grad_norm": 3.501709222793579,
      "learning_rate": 3.0095854426526105e-05,
      "loss": 0.9094,
      "step": 22890
    },
    {
      "epoch": 1.1951283155244172,
      "grad_norm": 5.054731845855713,
      "learning_rate": 3.008715620270341e-05,
      "loss": 0.9703,
      "step": 22900
    },
    {
      "epoch": 1.195650188526622,
      "grad_norm": 3.636629343032837,
      "learning_rate": 3.0078457978880713e-05,
      "loss": 0.9002,
      "step": 22910
    },
    {
      "epoch": 1.196172061528827,
      "grad_norm": 4.147922515869141,
      "learning_rate": 3.006975975505802e-05,
      "loss": 0.8818,
      "step": 22920
    },
    {
      "epoch": 1.196693934531032,
      "grad_norm": 4.076362609863281,
      "learning_rate": 3.0061061531235323e-05,
      "loss": 0.7998,
      "step": 22930
    },
    {
      "epoch": 1.1972158075332369,
      "grad_norm": 4.414876937866211,
      "learning_rate": 3.005236330741263e-05,
      "loss": 0.8203,
      "step": 22940
    },
    {
      "epoch": 1.1977376805354416,
      "grad_norm": 4.1149702072143555,
      "learning_rate": 3.0043665083589934e-05,
      "loss": 0.9115,
      "step": 22950
    },
    {
      "epoch": 1.1982595535376466,
      "grad_norm": 4.579132080078125,
      "learning_rate": 3.0034966859767237e-05,
      "loss": 0.8207,
      "step": 22960
    },
    {
      "epoch": 1.1987814265398515,
      "grad_norm": 3.485758066177368,
      "learning_rate": 3.0026268635944544e-05,
      "loss": 0.8969,
      "step": 22970
    },
    {
      "epoch": 1.1993032995420565,
      "grad_norm": 4.282761096954346,
      "learning_rate": 3.0017570412121848e-05,
      "loss": 0.9694,
      "step": 22980
    },
    {
      "epoch": 1.1998251725442612,
      "grad_norm": 3.9712071418762207,
      "learning_rate": 3.0008872188299155e-05,
      "loss": 0.8501,
      "step": 22990
    },
    {
      "epoch": 1.2003470455464662,
      "grad_norm": 4.6673359870910645,
      "learning_rate": 3.0000173964476458e-05,
      "loss": 0.9654,
      "step": 23000
    },
    {
      "epoch": 1.2008689185486712,
      "grad_norm": 4.010831832885742,
      "learning_rate": 2.9991475740653758e-05,
      "loss": 0.8608,
      "step": 23010
    },
    {
      "epoch": 1.2013907915508761,
      "grad_norm": 5.1245269775390625,
      "learning_rate": 2.9982777516831062e-05,
      "loss": 0.9152,
      "step": 23020
    },
    {
      "epoch": 1.201912664553081,
      "grad_norm": 4.190815448760986,
      "learning_rate": 2.9974079293008365e-05,
      "loss": 0.8932,
      "step": 23030
    },
    {
      "epoch": 1.2024345375552858,
      "grad_norm": 5.090660095214844,
      "learning_rate": 2.9965381069185672e-05,
      "loss": 0.9065,
      "step": 23040
    },
    {
      "epoch": 1.2029564105574908,
      "grad_norm": 4.130352020263672,
      "learning_rate": 2.9956682845362976e-05,
      "loss": 0.8444,
      "step": 23050
    },
    {
      "epoch": 1.2034782835596958,
      "grad_norm": 4.691790580749512,
      "learning_rate": 2.9947984621540283e-05,
      "loss": 0.9042,
      "step": 23060
    },
    {
      "epoch": 1.2040001565619007,
      "grad_norm": 4.260890960693359,
      "learning_rate": 2.9939286397717586e-05,
      "loss": 0.8888,
      "step": 23070
    },
    {
      "epoch": 1.2045220295641055,
      "grad_norm": 4.40352725982666,
      "learning_rate": 2.993058817389489e-05,
      "loss": 0.9915,
      "step": 23080
    },
    {
      "epoch": 1.2050439025663104,
      "grad_norm": 4.437662601470947,
      "learning_rate": 2.9921889950072197e-05,
      "loss": 0.8795,
      "step": 23090
    },
    {
      "epoch": 1.2055657755685154,
      "grad_norm": 4.626595497131348,
      "learning_rate": 2.99131917262495e-05,
      "loss": 0.8072,
      "step": 23100
    },
    {
      "epoch": 1.2060876485707204,
      "grad_norm": 4.506986141204834,
      "learning_rate": 2.9904493502426807e-05,
      "loss": 0.8268,
      "step": 23110
    },
    {
      "epoch": 1.2066095215729251,
      "grad_norm": 4.141473293304443,
      "learning_rate": 2.989579527860411e-05,
      "loss": 0.8835,
      "step": 23120
    },
    {
      "epoch": 1.20713139457513,
      "grad_norm": 4.275319576263428,
      "learning_rate": 2.9887097054781414e-05,
      "loss": 0.8792,
      "step": 23130
    },
    {
      "epoch": 1.207653267577335,
      "grad_norm": 4.231922149658203,
      "learning_rate": 2.987839883095872e-05,
      "loss": 0.8405,
      "step": 23140
    },
    {
      "epoch": 1.20817514057954,
      "grad_norm": 4.415327548980713,
      "learning_rate": 2.9869700607136025e-05,
      "loss": 0.9376,
      "step": 23150
    },
    {
      "epoch": 1.208697013581745,
      "grad_norm": 4.144058704376221,
      "learning_rate": 2.9861002383313332e-05,
      "loss": 0.8621,
      "step": 23160
    },
    {
      "epoch": 1.2092188865839497,
      "grad_norm": 4.086302280426025,
      "learning_rate": 2.9852304159490635e-05,
      "loss": 0.9069,
      "step": 23170
    },
    {
      "epoch": 1.2097407595861547,
      "grad_norm": 5.017307758331299,
      "learning_rate": 2.984360593566794e-05,
      "loss": 0.8899,
      "step": 23180
    },
    {
      "epoch": 1.2102626325883596,
      "grad_norm": 4.651699066162109,
      "learning_rate": 2.9834907711845246e-05,
      "loss": 0.9346,
      "step": 23190
    },
    {
      "epoch": 1.2107845055905646,
      "grad_norm": 4.495687484741211,
      "learning_rate": 2.982620948802255e-05,
      "loss": 1.0023,
      "step": 23200
    },
    {
      "epoch": 1.2113063785927696,
      "grad_norm": 4.41084623336792,
      "learning_rate": 2.9817511264199853e-05,
      "loss": 0.9552,
      "step": 23210
    },
    {
      "epoch": 1.2118282515949743,
      "grad_norm": 5.152996063232422,
      "learning_rate": 2.9808813040377153e-05,
      "loss": 0.9308,
      "step": 23220
    },
    {
      "epoch": 1.2123501245971793,
      "grad_norm": 4.631107807159424,
      "learning_rate": 2.980011481655446e-05,
      "loss": 0.9262,
      "step": 23230
    },
    {
      "epoch": 1.2128719975993842,
      "grad_norm": 3.413480043411255,
      "learning_rate": 2.9791416592731764e-05,
      "loss": 0.9823,
      "step": 23240
    },
    {
      "epoch": 1.213393870601589,
      "grad_norm": 4.6113786697387695,
      "learning_rate": 2.9782718368909067e-05,
      "loss": 1.0013,
      "step": 23250
    },
    {
      "epoch": 1.213915743603794,
      "grad_norm": 4.156499862670898,
      "learning_rate": 2.9774020145086374e-05,
      "loss": 1.0328,
      "step": 23260
    },
    {
      "epoch": 1.214437616605999,
      "grad_norm": 4.508455276489258,
      "learning_rate": 2.9765321921263678e-05,
      "loss": 0.8515,
      "step": 23270
    },
    {
      "epoch": 1.2149594896082039,
      "grad_norm": 4.580816745758057,
      "learning_rate": 2.9756623697440985e-05,
      "loss": 0.9419,
      "step": 23280
    },
    {
      "epoch": 1.2154813626104088,
      "grad_norm": 4.001652717590332,
      "learning_rate": 2.9747925473618288e-05,
      "loss": 0.9665,
      "step": 23290
    },
    {
      "epoch": 1.2160032356126136,
      "grad_norm": 4.577293395996094,
      "learning_rate": 2.973922724979559e-05,
      "loss": 0.9266,
      "step": 23300
    },
    {
      "epoch": 1.2165251086148186,
      "grad_norm": 4.24061393737793,
      "learning_rate": 2.97305290259729e-05,
      "loss": 0.8722,
      "step": 23310
    },
    {
      "epoch": 1.2170469816170235,
      "grad_norm": 3.898242950439453,
      "learning_rate": 2.9721830802150202e-05,
      "loss": 0.9374,
      "step": 23320
    },
    {
      "epoch": 1.2175688546192285,
      "grad_norm": 4.40089750289917,
      "learning_rate": 2.9713132578327506e-05,
      "loss": 0.8967,
      "step": 23330
    },
    {
      "epoch": 1.2180907276214334,
      "grad_norm": 4.590658664703369,
      "learning_rate": 2.9704434354504813e-05,
      "loss": 0.9638,
      "step": 23340
    },
    {
      "epoch": 1.2186126006236382,
      "grad_norm": 4.603707790374756,
      "learning_rate": 2.9695736130682116e-05,
      "loss": 0.9336,
      "step": 23350
    },
    {
      "epoch": 1.2191344736258432,
      "grad_norm": 4.704098701477051,
      "learning_rate": 2.9687037906859423e-05,
      "loss": 0.9758,
      "step": 23360
    },
    {
      "epoch": 1.2196563466280481,
      "grad_norm": 4.697600364685059,
      "learning_rate": 2.9678339683036727e-05,
      "loss": 0.9088,
      "step": 23370
    },
    {
      "epoch": 1.2201782196302529,
      "grad_norm": 4.093345642089844,
      "learning_rate": 2.966964145921403e-05,
      "loss": 0.8823,
      "step": 23380
    },
    {
      "epoch": 1.2207000926324578,
      "grad_norm": 4.610570907592773,
      "learning_rate": 2.9660943235391337e-05,
      "loss": 0.8497,
      "step": 23390
    },
    {
      "epoch": 1.2212219656346628,
      "grad_norm": 4.5830464363098145,
      "learning_rate": 2.965224501156864e-05,
      "loss": 0.9454,
      "step": 23400
    },
    {
      "epoch": 1.2217438386368678,
      "grad_norm": 3.593280553817749,
      "learning_rate": 2.9643546787745948e-05,
      "loss": 1.0022,
      "step": 23410
    },
    {
      "epoch": 1.2222657116390727,
      "grad_norm": 4.548890590667725,
      "learning_rate": 2.963484856392325e-05,
      "loss": 0.8874,
      "step": 23420
    },
    {
      "epoch": 1.2227875846412775,
      "grad_norm": 4.894193172454834,
      "learning_rate": 2.962615034010055e-05,
      "loss": 0.9513,
      "step": 23430
    },
    {
      "epoch": 1.2233094576434824,
      "grad_norm": 4.161768436431885,
      "learning_rate": 2.9617452116277855e-05,
      "loss": 0.9416,
      "step": 23440
    },
    {
      "epoch": 1.2238313306456874,
      "grad_norm": 4.8226423263549805,
      "learning_rate": 2.960875389245516e-05,
      "loss": 0.8757,
      "step": 23450
    },
    {
      "epoch": 1.2243532036478924,
      "grad_norm": 5.634993076324463,
      "learning_rate": 2.9600055668632465e-05,
      "loss": 0.8707,
      "step": 23460
    },
    {
      "epoch": 1.2248750766500973,
      "grad_norm": 3.849285364151001,
      "learning_rate": 2.959135744480977e-05,
      "loss": 0.8239,
      "step": 23470
    },
    {
      "epoch": 1.225396949652302,
      "grad_norm": 4.151854038238525,
      "learning_rate": 2.9582659220987076e-05,
      "loss": 0.8728,
      "step": 23480
    },
    {
      "epoch": 1.225918822654507,
      "grad_norm": 4.06364631652832,
      "learning_rate": 2.957396099716438e-05,
      "loss": 0.9555,
      "step": 23490
    },
    {
      "epoch": 1.226440695656712,
      "grad_norm": 4.115001678466797,
      "learning_rate": 2.9565262773341683e-05,
      "loss": 0.9457,
      "step": 23500
    },
    {
      "epoch": 1.2269625686589167,
      "grad_norm": 3.640880823135376,
      "learning_rate": 2.955656454951899e-05,
      "loss": 0.9112,
      "step": 23510
    },
    {
      "epoch": 1.2274844416611217,
      "grad_norm": 4.366461753845215,
      "learning_rate": 2.9547866325696293e-05,
      "loss": 0.9208,
      "step": 23520
    },
    {
      "epoch": 1.2280063146633267,
      "grad_norm": 4.043226718902588,
      "learning_rate": 2.95391681018736e-05,
      "loss": 0.8556,
      "step": 23530
    },
    {
      "epoch": 1.2285281876655316,
      "grad_norm": 4.3039350509643555,
      "learning_rate": 2.9530469878050904e-05,
      "loss": 0.9755,
      "step": 23540
    },
    {
      "epoch": 1.2290500606677366,
      "grad_norm": 3.8758933544158936,
      "learning_rate": 2.9521771654228207e-05,
      "loss": 0.9089,
      "step": 23550
    },
    {
      "epoch": 1.2295719336699413,
      "grad_norm": 3.90238094329834,
      "learning_rate": 2.9513073430405514e-05,
      "loss": 0.9706,
      "step": 23560
    },
    {
      "epoch": 1.2300938066721463,
      "grad_norm": 4.797757148742676,
      "learning_rate": 2.9504375206582818e-05,
      "loss": 0.9509,
      "step": 23570
    },
    {
      "epoch": 1.2306156796743513,
      "grad_norm": 4.800642013549805,
      "learning_rate": 2.949567698276012e-05,
      "loss": 0.9965,
      "step": 23580
    },
    {
      "epoch": 1.2311375526765562,
      "grad_norm": 4.240925312042236,
      "learning_rate": 2.948697875893743e-05,
      "loss": 0.8145,
      "step": 23590
    },
    {
      "epoch": 1.2316594256787612,
      "grad_norm": 4.850058555603027,
      "learning_rate": 2.9478280535114732e-05,
      "loss": 0.9206,
      "step": 23600
    },
    {
      "epoch": 1.232181298680966,
      "grad_norm": 4.629554271697998,
      "learning_rate": 2.946958231129204e-05,
      "loss": 0.9203,
      "step": 23610
    },
    {
      "epoch": 1.232703171683171,
      "grad_norm": 4.769324779510498,
      "learning_rate": 2.9460884087469342e-05,
      "loss": 0.9334,
      "step": 23620
    },
    {
      "epoch": 1.2332250446853759,
      "grad_norm": 3.5250251293182373,
      "learning_rate": 2.9452185863646646e-05,
      "loss": 0.8897,
      "step": 23630
    },
    {
      "epoch": 1.2337469176875808,
      "grad_norm": 4.59556770324707,
      "learning_rate": 2.9443487639823946e-05,
      "loss": 0.8867,
      "step": 23640
    },
    {
      "epoch": 1.2342687906897856,
      "grad_norm": 4.743692874908447,
      "learning_rate": 2.9434789416001253e-05,
      "loss": 0.9064,
      "step": 23650
    },
    {
      "epoch": 1.2347906636919905,
      "grad_norm": 4.5638628005981445,
      "learning_rate": 2.9426091192178557e-05,
      "loss": 0.8547,
      "step": 23660
    },
    {
      "epoch": 1.2353125366941955,
      "grad_norm": 4.622069358825684,
      "learning_rate": 2.941739296835586e-05,
      "loss": 0.9439,
      "step": 23670
    },
    {
      "epoch": 1.2358344096964005,
      "grad_norm": 4.499711513519287,
      "learning_rate": 2.9408694744533167e-05,
      "loss": 0.8296,
      "step": 23680
    },
    {
      "epoch": 1.2363562826986052,
      "grad_norm": 4.396122932434082,
      "learning_rate": 2.939999652071047e-05,
      "loss": 0.9969,
      "step": 23690
    },
    {
      "epoch": 1.2368781557008102,
      "grad_norm": 3.9984750747680664,
      "learning_rate": 2.9391298296887774e-05,
      "loss": 0.9082,
      "step": 23700
    },
    {
      "epoch": 1.2374000287030151,
      "grad_norm": 3.8340797424316406,
      "learning_rate": 2.938260007306508e-05,
      "loss": 0.9281,
      "step": 23710
    },
    {
      "epoch": 1.23792190170522,
      "grad_norm": 3.6734588146209717,
      "learning_rate": 2.9373901849242385e-05,
      "loss": 0.8763,
      "step": 23720
    },
    {
      "epoch": 1.238443774707425,
      "grad_norm": 4.854093074798584,
      "learning_rate": 2.936520362541969e-05,
      "loss": 0.8245,
      "step": 23730
    },
    {
      "epoch": 1.2389656477096298,
      "grad_norm": 4.337430477142334,
      "learning_rate": 2.9356505401596995e-05,
      "loss": 0.8662,
      "step": 23740
    },
    {
      "epoch": 1.2394875207118348,
      "grad_norm": 4.139665603637695,
      "learning_rate": 2.93478071777743e-05,
      "loss": 0.8421,
      "step": 23750
    },
    {
      "epoch": 1.2400093937140397,
      "grad_norm": 4.712093353271484,
      "learning_rate": 2.9339108953951606e-05,
      "loss": 0.9429,
      "step": 23760
    },
    {
      "epoch": 1.2405312667162447,
      "grad_norm": 4.710353851318359,
      "learning_rate": 2.933041073012891e-05,
      "loss": 0.884,
      "step": 23770
    },
    {
      "epoch": 1.2410531397184494,
      "grad_norm": 4.6663007736206055,
      "learning_rate": 2.9321712506306216e-05,
      "loss": 1.0158,
      "step": 23780
    },
    {
      "epoch": 1.2415750127206544,
      "grad_norm": 4.314326286315918,
      "learning_rate": 2.931301428248352e-05,
      "loss": 0.9179,
      "step": 23790
    },
    {
      "epoch": 1.2420968857228594,
      "grad_norm": 3.713505506515503,
      "learning_rate": 2.9304316058660823e-05,
      "loss": 0.9396,
      "step": 23800
    },
    {
      "epoch": 1.2426187587250643,
      "grad_norm": 5.28725004196167,
      "learning_rate": 2.929561783483813e-05,
      "loss": 0.8885,
      "step": 23810
    },
    {
      "epoch": 1.243140631727269,
      "grad_norm": 4.046226501464844,
      "learning_rate": 2.9286919611015434e-05,
      "loss": 0.8378,
      "step": 23820
    },
    {
      "epoch": 1.243662504729474,
      "grad_norm": 4.41099214553833,
      "learning_rate": 2.927822138719274e-05,
      "loss": 0.8768,
      "step": 23830
    },
    {
      "epoch": 1.244184377731679,
      "grad_norm": 3.926668405532837,
      "learning_rate": 2.9269523163370044e-05,
      "loss": 0.8291,
      "step": 23840
    },
    {
      "epoch": 1.244706250733884,
      "grad_norm": 4.7218708992004395,
      "learning_rate": 2.9260824939547344e-05,
      "loss": 0.9369,
      "step": 23850
    },
    {
      "epoch": 1.245228123736089,
      "grad_norm": 4.145586013793945,
      "learning_rate": 2.9252126715724648e-05,
      "loss": 0.9673,
      "step": 23860
    },
    {
      "epoch": 1.2457499967382937,
      "grad_norm": 4.552575588226318,
      "learning_rate": 2.924342849190195e-05,
      "loss": 0.9399,
      "step": 23870
    },
    {
      "epoch": 1.2462718697404986,
      "grad_norm": 4.7897186279296875,
      "learning_rate": 2.923473026807926e-05,
      "loss": 0.9713,
      "step": 23880
    },
    {
      "epoch": 1.2467937427427036,
      "grad_norm": 3.815704107284546,
      "learning_rate": 2.9226032044256562e-05,
      "loss": 0.8979,
      "step": 23890
    },
    {
      "epoch": 1.2473156157449086,
      "grad_norm": 4.7239909172058105,
      "learning_rate": 2.921733382043387e-05,
      "loss": 0.8312,
      "step": 23900
    },
    {
      "epoch": 1.2478374887471133,
      "grad_norm": 3.845447063446045,
      "learning_rate": 2.9208635596611173e-05,
      "loss": 0.9191,
      "step": 23910
    },
    {
      "epoch": 1.2483593617493183,
      "grad_norm": 4.3278584480285645,
      "learning_rate": 2.9199937372788476e-05,
      "loss": 0.7717,
      "step": 23920
    },
    {
      "epoch": 1.2488812347515232,
      "grad_norm": 4.979239463806152,
      "learning_rate": 2.9191239148965783e-05,
      "loss": 0.9008,
      "step": 23930
    },
    {
      "epoch": 1.2494031077537282,
      "grad_norm": 4.6194167137146,
      "learning_rate": 2.9182540925143087e-05,
      "loss": 0.9418,
      "step": 23940
    },
    {
      "epoch": 1.249924980755933,
      "grad_norm": 4.451831817626953,
      "learning_rate": 2.9173842701320393e-05,
      "loss": 0.8909,
      "step": 23950
    },
    {
      "epoch": 1.250446853758138,
      "grad_norm": 3.9140076637268066,
      "learning_rate": 2.9165144477497697e-05,
      "loss": 0.823,
      "step": 23960
    },
    {
      "epoch": 1.2509687267603429,
      "grad_norm": 5.4921555519104,
      "learning_rate": 2.9156446253675e-05,
      "loss": 0.986,
      "step": 23970
    },
    {
      "epoch": 1.2514905997625478,
      "grad_norm": 3.9418375492095947,
      "learning_rate": 2.9147748029852308e-05,
      "loss": 0.9264,
      "step": 23980
    },
    {
      "epoch": 1.2520124727647528,
      "grad_norm": 4.202447414398193,
      "learning_rate": 2.913904980602961e-05,
      "loss": 0.8158,
      "step": 23990
    },
    {
      "epoch": 1.2525343457669575,
      "grad_norm": 4.319001197814941,
      "learning_rate": 2.9130351582206915e-05,
      "loss": 0.9109,
      "step": 24000
    },
    {
      "epoch": 1.2530562187691625,
      "grad_norm": 4.68379545211792,
      "learning_rate": 2.912165335838422e-05,
      "loss": 0.9214,
      "step": 24010
    },
    {
      "epoch": 1.2535780917713675,
      "grad_norm": 4.711191177368164,
      "learning_rate": 2.9112955134561525e-05,
      "loss": 0.9103,
      "step": 24020
    },
    {
      "epoch": 1.2540999647735722,
      "grad_norm": 3.9497742652893066,
      "learning_rate": 2.9104256910738832e-05,
      "loss": 0.9323,
      "step": 24030
    },
    {
      "epoch": 1.2546218377757772,
      "grad_norm": 4.187644004821777,
      "learning_rate": 2.9095558686916136e-05,
      "loss": 0.9228,
      "step": 24040
    },
    {
      "epoch": 1.2551437107779821,
      "grad_norm": 5.341514587402344,
      "learning_rate": 2.908686046309344e-05,
      "loss": 0.9447,
      "step": 24050
    },
    {
      "epoch": 1.2556655837801871,
      "grad_norm": 4.507972717285156,
      "learning_rate": 2.907816223927074e-05,
      "loss": 0.9888,
      "step": 24060
    },
    {
      "epoch": 1.256187456782392,
      "grad_norm": 4.004724025726318,
      "learning_rate": 2.9069464015448046e-05,
      "loss": 0.9359,
      "step": 24070
    },
    {
      "epoch": 1.2567093297845968,
      "grad_norm": 3.7446506023406982,
      "learning_rate": 2.906076579162535e-05,
      "loss": 0.867,
      "step": 24080
    },
    {
      "epoch": 1.2572312027868018,
      "grad_norm": 4.087561130523682,
      "learning_rate": 2.9052067567802653e-05,
      "loss": 0.9128,
      "step": 24090
    },
    {
      "epoch": 1.2577530757890067,
      "grad_norm": 4.880438327789307,
      "learning_rate": 2.904336934397996e-05,
      "loss": 0.963,
      "step": 24100
    },
    {
      "epoch": 1.2582749487912117,
      "grad_norm": 4.578061580657959,
      "learning_rate": 2.9034671120157264e-05,
      "loss": 0.8884,
      "step": 24110
    },
    {
      "epoch": 1.2587968217934167,
      "grad_norm": 4.173280239105225,
      "learning_rate": 2.9025972896334567e-05,
      "loss": 0.9245,
      "step": 24120
    },
    {
      "epoch": 1.2593186947956214,
      "grad_norm": 4.743886470794678,
      "learning_rate": 2.9017274672511874e-05,
      "loss": 0.9448,
      "step": 24130
    },
    {
      "epoch": 1.2598405677978264,
      "grad_norm": 3.6473379135131836,
      "learning_rate": 2.9008576448689178e-05,
      "loss": 0.8724,
      "step": 24140
    },
    {
      "epoch": 1.2603624408000313,
      "grad_norm": 3.8636176586151123,
      "learning_rate": 2.8999878224866485e-05,
      "loss": 0.9666,
      "step": 24150
    },
    {
      "epoch": 1.260884313802236,
      "grad_norm": 4.171045780181885,
      "learning_rate": 2.899118000104379e-05,
      "loss": 0.9448,
      "step": 24160
    },
    {
      "epoch": 1.2614061868044413,
      "grad_norm": 4.28814172744751,
      "learning_rate": 2.8982481777221092e-05,
      "loss": 0.9836,
      "step": 24170
    },
    {
      "epoch": 1.261928059806646,
      "grad_norm": 4.280492305755615,
      "learning_rate": 2.89737835533984e-05,
      "loss": 0.9837,
      "step": 24180
    },
    {
      "epoch": 1.262449932808851,
      "grad_norm": 4.21783447265625,
      "learning_rate": 2.8965085329575702e-05,
      "loss": 0.9342,
      "step": 24190
    },
    {
      "epoch": 1.262971805811056,
      "grad_norm": 5.353865146636963,
      "learning_rate": 2.895638710575301e-05,
      "loss": 0.9608,
      "step": 24200
    },
    {
      "epoch": 1.2634936788132607,
      "grad_norm": 4.990716457366943,
      "learning_rate": 2.8947688881930313e-05,
      "loss": 0.9742,
      "step": 24210
    },
    {
      "epoch": 1.2640155518154657,
      "grad_norm": 4.555704116821289,
      "learning_rate": 2.8938990658107616e-05,
      "loss": 0.7725,
      "step": 24220
    },
    {
      "epoch": 1.2645374248176706,
      "grad_norm": 4.669400691986084,
      "learning_rate": 2.8930292434284923e-05,
      "loss": 0.8507,
      "step": 24230
    },
    {
      "epoch": 1.2650592978198756,
      "grad_norm": 4.849408149719238,
      "learning_rate": 2.8921594210462227e-05,
      "loss": 0.8888,
      "step": 24240
    },
    {
      "epoch": 1.2655811708220805,
      "grad_norm": 4.751853942871094,
      "learning_rate": 2.8912895986639534e-05,
      "loss": 0.8841,
      "step": 24250
    },
    {
      "epoch": 1.2661030438242853,
      "grad_norm": 3.9972846508026123,
      "learning_rate": 2.890419776281683e-05,
      "loss": 0.801,
      "step": 24260
    },
    {
      "epoch": 1.2666249168264903,
      "grad_norm": 4.2227911949157715,
      "learning_rate": 2.8895499538994138e-05,
      "loss": 0.94,
      "step": 24270
    },
    {
      "epoch": 1.2671467898286952,
      "grad_norm": 4.928227424621582,
      "learning_rate": 2.888680131517144e-05,
      "loss": 0.8653,
      "step": 24280
    },
    {
      "epoch": 1.2676686628309002,
      "grad_norm": 4.7848968505859375,
      "learning_rate": 2.8878103091348745e-05,
      "loss": 1.0343,
      "step": 24290
    },
    {
      "epoch": 1.2681905358331051,
      "grad_norm": 4.396775245666504,
      "learning_rate": 2.886940486752605e-05,
      "loss": 0.8918,
      "step": 24300
    },
    {
      "epoch": 1.26871240883531,
      "grad_norm": 5.210907936096191,
      "learning_rate": 2.8860706643703355e-05,
      "loss": 0.9151,
      "step": 24310
    },
    {
      "epoch": 1.2692342818375149,
      "grad_norm": 4.410383701324463,
      "learning_rate": 2.8852008419880662e-05,
      "loss": 0.9739,
      "step": 24320
    },
    {
      "epoch": 1.2697561548397198,
      "grad_norm": 3.726450204849243,
      "learning_rate": 2.8843310196057966e-05,
      "loss": 0.8167,
      "step": 24330
    },
    {
      "epoch": 1.2702780278419246,
      "grad_norm": 3.886416435241699,
      "learning_rate": 2.883461197223527e-05,
      "loss": 0.9173,
      "step": 24340
    },
    {
      "epoch": 1.2707999008441295,
      "grad_norm": 4.220252990722656,
      "learning_rate": 2.8825913748412576e-05,
      "loss": 0.9044,
      "step": 24350
    },
    {
      "epoch": 1.2713217738463345,
      "grad_norm": 4.89557409286499,
      "learning_rate": 2.881721552458988e-05,
      "loss": 0.8474,
      "step": 24360
    },
    {
      "epoch": 1.2718436468485395,
      "grad_norm": 4.5902018547058105,
      "learning_rate": 2.8808517300767187e-05,
      "loss": 0.9721,
      "step": 24370
    },
    {
      "epoch": 1.2723655198507444,
      "grad_norm": 4.232043743133545,
      "learning_rate": 2.879981907694449e-05,
      "loss": 0.9519,
      "step": 24380
    },
    {
      "epoch": 1.2728873928529492,
      "grad_norm": 4.153280735015869,
      "learning_rate": 2.8791120853121794e-05,
      "loss": 0.9319,
      "step": 24390
    },
    {
      "epoch": 1.2734092658551541,
      "grad_norm": 4.8031325340271,
      "learning_rate": 2.87824226292991e-05,
      "loss": 0.975,
      "step": 24400
    },
    {
      "epoch": 1.273931138857359,
      "grad_norm": 3.9511687755584717,
      "learning_rate": 2.8773724405476404e-05,
      "loss": 0.9292,
      "step": 24410
    },
    {
      "epoch": 1.274453011859564,
      "grad_norm": 4.46301794052124,
      "learning_rate": 2.8765026181653708e-05,
      "loss": 0.9305,
      "step": 24420
    },
    {
      "epoch": 1.274974884861769,
      "grad_norm": 4.901420593261719,
      "learning_rate": 2.8756327957831015e-05,
      "loss": 0.9619,
      "step": 24430
    },
    {
      "epoch": 1.2754967578639738,
      "grad_norm": 5.414133548736572,
      "learning_rate": 2.8747629734008318e-05,
      "loss": 0.9207,
      "step": 24440
    },
    {
      "epoch": 1.2760186308661787,
      "grad_norm": 4.588951587677002,
      "learning_rate": 2.8738931510185625e-05,
      "loss": 0.9114,
      "step": 24450
    },
    {
      "epoch": 1.2765405038683837,
      "grad_norm": 4.875421524047852,
      "learning_rate": 2.873023328636293e-05,
      "loss": 0.9244,
      "step": 24460
    },
    {
      "epoch": 1.2770623768705884,
      "grad_norm": 5.115414619445801,
      "learning_rate": 2.872153506254023e-05,
      "loss": 0.8412,
      "step": 24470
    },
    {
      "epoch": 1.2775842498727934,
      "grad_norm": 5.724788665771484,
      "learning_rate": 2.8712836838717532e-05,
      "loss": 0.8901,
      "step": 24480
    },
    {
      "epoch": 1.2781061228749984,
      "grad_norm": 5.478124141693115,
      "learning_rate": 2.870413861489484e-05,
      "loss": 0.9332,
      "step": 24490
    },
    {
      "epoch": 1.2786279958772033,
      "grad_norm": 4.696909427642822,
      "learning_rate": 2.8695440391072143e-05,
      "loss": 1.0182,
      "step": 24500
    },
    {
      "epoch": 1.2791498688794083,
      "grad_norm": 4.33417272567749,
      "learning_rate": 2.8686742167249446e-05,
      "loss": 0.8727,
      "step": 24510
    },
    {
      "epoch": 1.279671741881613,
      "grad_norm": 4.43773889541626,
      "learning_rate": 2.8678043943426753e-05,
      "loss": 0.955,
      "step": 24520
    },
    {
      "epoch": 1.280193614883818,
      "grad_norm": 4.610671520233154,
      "learning_rate": 2.8669345719604057e-05,
      "loss": 0.9392,
      "step": 24530
    },
    {
      "epoch": 1.280715487886023,
      "grad_norm": 4.590407371520996,
      "learning_rate": 2.866064749578136e-05,
      "loss": 0.8686,
      "step": 24540
    },
    {
      "epoch": 1.281237360888228,
      "grad_norm": 4.698857307434082,
      "learning_rate": 2.8651949271958667e-05,
      "loss": 0.8442,
      "step": 24550
    },
    {
      "epoch": 1.281759233890433,
      "grad_norm": 4.215048789978027,
      "learning_rate": 2.864325104813597e-05,
      "loss": 0.9049,
      "step": 24560
    },
    {
      "epoch": 1.2822811068926376,
      "grad_norm": 4.9163947105407715,
      "learning_rate": 2.8635422646695548e-05,
      "loss": 0.9054,
      "step": 24570
    },
    {
      "epoch": 1.2828029798948426,
      "grad_norm": 4.353578090667725,
      "learning_rate": 2.862672442287285e-05,
      "loss": 0.9228,
      "step": 24580
    },
    {
      "epoch": 1.2833248528970476,
      "grad_norm": 3.8305647373199463,
      "learning_rate": 2.8618026199050158e-05,
      "loss": 0.9622,
      "step": 24590
    },
    {
      "epoch": 1.2838467258992523,
      "grad_norm": 4.856352806091309,
      "learning_rate": 2.860932797522746e-05,
      "loss": 0.884,
      "step": 24600
    },
    {
      "epoch": 1.2843685989014573,
      "grad_norm": 4.079742431640625,
      "learning_rate": 2.860062975140477e-05,
      "loss": 0.8709,
      "step": 24610
    },
    {
      "epoch": 1.2848904719036622,
      "grad_norm": 4.154860019683838,
      "learning_rate": 2.8591931527582065e-05,
      "loss": 0.8371,
      "step": 24620
    },
    {
      "epoch": 1.2854123449058672,
      "grad_norm": 4.891490936279297,
      "learning_rate": 2.8583233303759372e-05,
      "loss": 0.9688,
      "step": 24630
    },
    {
      "epoch": 1.2859342179080722,
      "grad_norm": 4.236595153808594,
      "learning_rate": 2.8574535079936676e-05,
      "loss": 0.8895,
      "step": 24640
    },
    {
      "epoch": 1.286456090910277,
      "grad_norm": 4.738125801086426,
      "learning_rate": 2.8565836856113983e-05,
      "loss": 0.9161,
      "step": 24650
    },
    {
      "epoch": 1.2869779639124819,
      "grad_norm": 4.378057956695557,
      "learning_rate": 2.8557138632291286e-05,
      "loss": 0.891,
      "step": 24660
    },
    {
      "epoch": 1.2874998369146868,
      "grad_norm": 4.343336582183838,
      "learning_rate": 2.854844040846859e-05,
      "loss": 0.8284,
      "step": 24670
    },
    {
      "epoch": 1.2880217099168918,
      "grad_norm": 4.551950931549072,
      "learning_rate": 2.8539742184645897e-05,
      "loss": 0.7774,
      "step": 24680
    },
    {
      "epoch": 1.2885435829190968,
      "grad_norm": 3.751154661178589,
      "learning_rate": 2.85310439608232e-05,
      "loss": 0.9532,
      "step": 24690
    },
    {
      "epoch": 1.2890654559213015,
      "grad_norm": 4.243073463439941,
      "learning_rate": 2.8522345737000504e-05,
      "loss": 0.8519,
      "step": 24700
    },
    {
      "epoch": 1.2895873289235065,
      "grad_norm": 4.513702392578125,
      "learning_rate": 2.851364751317781e-05,
      "loss": 0.9665,
      "step": 24710
    },
    {
      "epoch": 1.2901092019257114,
      "grad_norm": 3.691627025604248,
      "learning_rate": 2.8504949289355114e-05,
      "loss": 0.8089,
      "step": 24720
    },
    {
      "epoch": 1.2906310749279162,
      "grad_norm": 3.8293163776397705,
      "learning_rate": 2.849625106553242e-05,
      "loss": 0.8977,
      "step": 24730
    },
    {
      "epoch": 1.2911529479301211,
      "grad_norm": 4.829126358032227,
      "learning_rate": 2.8487552841709725e-05,
      "loss": 0.8984,
      "step": 24740
    },
    {
      "epoch": 1.291674820932326,
      "grad_norm": 5.0109686851501465,
      "learning_rate": 2.847885461788703e-05,
      "loss": 0.8954,
      "step": 24750
    },
    {
      "epoch": 1.292196693934531,
      "grad_norm": 4.052231311798096,
      "learning_rate": 2.8470156394064335e-05,
      "loss": 0.8654,
      "step": 24760
    },
    {
      "epoch": 1.292718566936736,
      "grad_norm": 4.17534065246582,
      "learning_rate": 2.846145817024164e-05,
      "loss": 0.9255,
      "step": 24770
    },
    {
      "epoch": 1.2932404399389408,
      "grad_norm": 5.0220489501953125,
      "learning_rate": 2.8452759946418946e-05,
      "loss": 0.9074,
      "step": 24780
    },
    {
      "epoch": 1.2937623129411457,
      "grad_norm": 4.588744640350342,
      "learning_rate": 2.844406172259625e-05,
      "loss": 0.9353,
      "step": 24790
    },
    {
      "epoch": 1.2942841859433507,
      "grad_norm": 4.531062126159668,
      "learning_rate": 2.8435363498773553e-05,
      "loss": 0.9653,
      "step": 24800
    },
    {
      "epoch": 1.2948060589455557,
      "grad_norm": 4.706002712249756,
      "learning_rate": 2.842666527495086e-05,
      "loss": 0.8662,
      "step": 24810
    },
    {
      "epoch": 1.2953279319477606,
      "grad_norm": 4.461428642272949,
      "learning_rate": 2.8417967051128163e-05,
      "loss": 0.883,
      "step": 24820
    },
    {
      "epoch": 1.2958498049499654,
      "grad_norm": 5.271054267883301,
      "learning_rate": 2.8409268827305464e-05,
      "loss": 0.8946,
      "step": 24830
    },
    {
      "epoch": 1.2963716779521703,
      "grad_norm": 4.018074989318848,
      "learning_rate": 2.8400570603482767e-05,
      "loss": 0.8075,
      "step": 24840
    },
    {
      "epoch": 1.2968935509543753,
      "grad_norm": 4.291164398193359,
      "learning_rate": 2.8391872379660074e-05,
      "loss": 0.9588,
      "step": 24850
    },
    {
      "epoch": 1.29741542395658,
      "grad_norm": 5.231220722198486,
      "learning_rate": 2.8383174155837378e-05,
      "loss": 0.8748,
      "step": 24860
    },
    {
      "epoch": 1.297937296958785,
      "grad_norm": 5.191959381103516,
      "learning_rate": 2.837447593201468e-05,
      "loss": 0.9381,
      "step": 24870
    },
    {
      "epoch": 1.29845916996099,
      "grad_norm": 3.9551382064819336,
      "learning_rate": 2.8365777708191988e-05,
      "loss": 0.8843,
      "step": 24880
    },
    {
      "epoch": 1.298981042963195,
      "grad_norm": 4.485442161560059,
      "learning_rate": 2.835707948436929e-05,
      "loss": 0.9169,
      "step": 24890
    },
    {
      "epoch": 1.2995029159654,
      "grad_norm": 4.444712162017822,
      "learning_rate": 2.83483812605466e-05,
      "loss": 0.8846,
      "step": 24900
    },
    {
      "epoch": 1.3000247889676046,
      "grad_norm": 5.536259651184082,
      "learning_rate": 2.8339683036723902e-05,
      "loss": 0.9998,
      "step": 24910
    },
    {
      "epoch": 1.3005466619698096,
      "grad_norm": 4.382264137268066,
      "learning_rate": 2.8330984812901206e-05,
      "loss": 0.8226,
      "step": 24920
    },
    {
      "epoch": 1.3010685349720146,
      "grad_norm": 4.436713218688965,
      "learning_rate": 2.8322286589078513e-05,
      "loss": 0.8648,
      "step": 24930
    },
    {
      "epoch": 1.3015904079742195,
      "grad_norm": 4.192939758300781,
      "learning_rate": 2.8313588365255816e-05,
      "loss": 0.9113,
      "step": 24940
    },
    {
      "epoch": 1.3021122809764245,
      "grad_norm": 4.781384468078613,
      "learning_rate": 2.830489014143312e-05,
      "loss": 1.0152,
      "step": 24950
    },
    {
      "epoch": 1.3026341539786293,
      "grad_norm": 4.508979320526123,
      "learning_rate": 2.8296191917610427e-05,
      "loss": 0.9689,
      "step": 24960
    },
    {
      "epoch": 1.3031560269808342,
      "grad_norm": 4.74002742767334,
      "learning_rate": 2.828749369378773e-05,
      "loss": 0.9254,
      "step": 24970
    },
    {
      "epoch": 1.3036778999830392,
      "grad_norm": 4.244317531585693,
      "learning_rate": 2.8278795469965037e-05,
      "loss": 0.8007,
      "step": 24980
    },
    {
      "epoch": 1.304199772985244,
      "grad_norm": 3.595564365386963,
      "learning_rate": 2.827009724614234e-05,
      "loss": 0.9174,
      "step": 24990
    },
    {
      "epoch": 1.3047216459874489,
      "grad_norm": 4.642885208129883,
      "learning_rate": 2.8261399022319644e-05,
      "loss": 0.9259,
      "step": 25000
    },
    {
      "epoch": 1.3052435189896539,
      "grad_norm": 5.045854091644287,
      "learning_rate": 2.825270079849695e-05,
      "loss": 0.9123,
      "step": 25010
    },
    {
      "epoch": 1.3057653919918588,
      "grad_norm": 4.959795951843262,
      "learning_rate": 2.8244002574674255e-05,
      "loss": 0.9303,
      "step": 25020
    },
    {
      "epoch": 1.3062872649940638,
      "grad_norm": 4.3432416915893555,
      "learning_rate": 2.823530435085156e-05,
      "loss": 0.9927,
      "step": 25030
    },
    {
      "epoch": 1.3068091379962685,
      "grad_norm": 4.473991870880127,
      "learning_rate": 2.822660612702886e-05,
      "loss": 0.8551,
      "step": 25040
    },
    {
      "epoch": 1.3073310109984735,
      "grad_norm": 3.9328880310058594,
      "learning_rate": 2.8217907903206165e-05,
      "loss": 0.8926,
      "step": 25050
    },
    {
      "epoch": 1.3078528840006785,
      "grad_norm": 3.7178361415863037,
      "learning_rate": 2.820920967938347e-05,
      "loss": 0.8163,
      "step": 25060
    },
    {
      "epoch": 1.3083747570028834,
      "grad_norm": 4.0907745361328125,
      "learning_rate": 2.8200511455560776e-05,
      "loss": 0.8974,
      "step": 25070
    },
    {
      "epoch": 1.3088966300050884,
      "grad_norm": 4.674025058746338,
      "learning_rate": 2.819181323173808e-05,
      "loss": 1.017,
      "step": 25080
    },
    {
      "epoch": 1.3094185030072931,
      "grad_norm": 3.787876605987549,
      "learning_rate": 2.8183115007915383e-05,
      "loss": 0.861,
      "step": 25090
    },
    {
      "epoch": 1.309940376009498,
      "grad_norm": 4.076206207275391,
      "learning_rate": 2.817441678409269e-05,
      "loss": 0.8575,
      "step": 25100
    },
    {
      "epoch": 1.310462249011703,
      "grad_norm": 4.914007663726807,
      "learning_rate": 2.8165718560269993e-05,
      "loss": 0.8502,
      "step": 25110
    },
    {
      "epoch": 1.3109841220139078,
      "grad_norm": 4.402491569519043,
      "learning_rate": 2.8157020336447297e-05,
      "loss": 0.931,
      "step": 25120
    },
    {
      "epoch": 1.3115059950161128,
      "grad_norm": 3.4855706691741943,
      "learning_rate": 2.8148322112624604e-05,
      "loss": 0.8019,
      "step": 25130
    },
    {
      "epoch": 1.3120278680183177,
      "grad_norm": 3.9659125804901123,
      "learning_rate": 2.8139623888801907e-05,
      "loss": 0.8982,
      "step": 25140
    },
    {
      "epoch": 1.3125497410205227,
      "grad_norm": 4.807580947875977,
      "learning_rate": 2.8130925664979214e-05,
      "loss": 0.9175,
      "step": 25150
    },
    {
      "epoch": 1.3130716140227277,
      "grad_norm": 4.496165752410889,
      "learning_rate": 2.8122227441156518e-05,
      "loss": 0.8674,
      "step": 25160
    },
    {
      "epoch": 1.3135934870249324,
      "grad_norm": 4.376605033874512,
      "learning_rate": 2.811352921733382e-05,
      "loss": 0.9254,
      "step": 25170
    },
    {
      "epoch": 1.3141153600271374,
      "grad_norm": 4.085859298706055,
      "learning_rate": 2.810483099351113e-05,
      "loss": 0.9267,
      "step": 25180
    },
    {
      "epoch": 1.3146372330293423,
      "grad_norm": 4.645833492279053,
      "learning_rate": 2.8096132769688432e-05,
      "loss": 0.9116,
      "step": 25190
    },
    {
      "epoch": 1.3151591060315473,
      "grad_norm": 3.6719799041748047,
      "learning_rate": 2.808743454586574e-05,
      "loss": 0.8626,
      "step": 25200
    },
    {
      "epoch": 1.3156809790337523,
      "grad_norm": 4.54748010635376,
      "learning_rate": 2.8078736322043042e-05,
      "loss": 0.9177,
      "step": 25210
    },
    {
      "epoch": 1.316202852035957,
      "grad_norm": 3.554969310760498,
      "learning_rate": 2.8070038098220346e-05,
      "loss": 0.8097,
      "step": 25220
    },
    {
      "epoch": 1.316724725038162,
      "grad_norm": 4.986962795257568,
      "learning_rate": 2.8061339874397653e-05,
      "loss": 0.8653,
      "step": 25230
    },
    {
      "epoch": 1.317246598040367,
      "grad_norm": 4.276953220367432,
      "learning_rate": 2.805264165057495e-05,
      "loss": 0.9583,
      "step": 25240
    },
    {
      "epoch": 1.3177684710425717,
      "grad_norm": 4.474531650543213,
      "learning_rate": 2.8043943426752257e-05,
      "loss": 0.9362,
      "step": 25250
    },
    {
      "epoch": 1.3182903440447766,
      "grad_norm": 4.667130470275879,
      "learning_rate": 2.803524520292956e-05,
      "loss": 1.0062,
      "step": 25260
    },
    {
      "epoch": 1.3188122170469816,
      "grad_norm": 3.69785475730896,
      "learning_rate": 2.8026546979106867e-05,
      "loss": 0.8796,
      "step": 25270
    },
    {
      "epoch": 1.3193340900491866,
      "grad_norm": 4.794766426086426,
      "learning_rate": 2.801784875528417e-05,
      "loss": 0.9574,
      "step": 25280
    },
    {
      "epoch": 1.3198559630513915,
      "grad_norm": 4.567374229431152,
      "learning_rate": 2.8009150531461474e-05,
      "loss": 0.8232,
      "step": 25290
    },
    {
      "epoch": 1.3203778360535963,
      "grad_norm": 4.269191265106201,
      "learning_rate": 2.800045230763878e-05,
      "loss": 0.9425,
      "step": 25300
    },
    {
      "epoch": 1.3208997090558012,
      "grad_norm": 4.584222316741943,
      "learning_rate": 2.7991754083816085e-05,
      "loss": 1.0334,
      "step": 25310
    },
    {
      "epoch": 1.3214215820580062,
      "grad_norm": 4.7524285316467285,
      "learning_rate": 2.798305585999339e-05,
      "loss": 0.9526,
      "step": 25320
    },
    {
      "epoch": 1.3219434550602112,
      "grad_norm": 4.266702175140381,
      "learning_rate": 2.7974357636170695e-05,
      "loss": 0.8186,
      "step": 25330
    },
    {
      "epoch": 1.3224653280624161,
      "grad_norm": 4.754350662231445,
      "learning_rate": 2.7965659412348e-05,
      "loss": 0.9471,
      "step": 25340
    },
    {
      "epoch": 1.3229872010646209,
      "grad_norm": 4.268664360046387,
      "learning_rate": 2.7956961188525306e-05,
      "loss": 0.8744,
      "step": 25350
    },
    {
      "epoch": 1.3235090740668258,
      "grad_norm": 4.4362688064575195,
      "learning_rate": 2.794826296470261e-05,
      "loss": 0.9898,
      "step": 25360
    },
    {
      "epoch": 1.3240309470690308,
      "grad_norm": 4.003083229064941,
      "learning_rate": 2.7939564740879913e-05,
      "loss": 0.9786,
      "step": 25370
    },
    {
      "epoch": 1.3245528200712355,
      "grad_norm": 3.6769845485687256,
      "learning_rate": 2.793086651705722e-05,
      "loss": 0.8519,
      "step": 25380
    },
    {
      "epoch": 1.3250746930734405,
      "grad_norm": 4.191720008850098,
      "learning_rate": 2.7922168293234523e-05,
      "loss": 0.9457,
      "step": 25390
    },
    {
      "epoch": 1.3255965660756455,
      "grad_norm": 4.690851211547852,
      "learning_rate": 2.791347006941183e-05,
      "loss": 0.9892,
      "step": 25400
    },
    {
      "epoch": 1.3261184390778504,
      "grad_norm": 4.021285057067871,
      "learning_rate": 2.7904771845589134e-05,
      "loss": 0.872,
      "step": 25410
    },
    {
      "epoch": 1.3266403120800554,
      "grad_norm": 5.290771484375,
      "learning_rate": 2.7896073621766437e-05,
      "loss": 0.9202,
      "step": 25420
    },
    {
      "epoch": 1.3271621850822601,
      "grad_norm": 4.294698238372803,
      "learning_rate": 2.7887375397943744e-05,
      "loss": 0.9855,
      "step": 25430
    },
    {
      "epoch": 1.327684058084465,
      "grad_norm": 3.616097927093506,
      "learning_rate": 2.7878677174121048e-05,
      "loss": 0.8429,
      "step": 25440
    },
    {
      "epoch": 1.32820593108667,
      "grad_norm": 4.457461357116699,
      "learning_rate": 2.7869978950298348e-05,
      "loss": 0.9358,
      "step": 25450
    },
    {
      "epoch": 1.328727804088875,
      "grad_norm": 4.1497578620910645,
      "learning_rate": 2.786128072647565e-05,
      "loss": 0.9473,
      "step": 25460
    },
    {
      "epoch": 1.32924967709108,
      "grad_norm": 5.032433032989502,
      "learning_rate": 2.785258250265296e-05,
      "loss": 0.9616,
      "step": 25470
    },
    {
      "epoch": 1.3297715500932847,
      "grad_norm": 4.361518859863281,
      "learning_rate": 2.7843884278830262e-05,
      "loss": 0.9865,
      "step": 25480
    },
    {
      "epoch": 1.3302934230954897,
      "grad_norm": 4.113657474517822,
      "learning_rate": 2.7835186055007566e-05,
      "loss": 0.9437,
      "step": 25490
    },
    {
      "epoch": 1.3308152960976947,
      "grad_norm": 3.7637786865234375,
      "learning_rate": 2.7826487831184872e-05,
      "loss": 0.7997,
      "step": 25500
    },
    {
      "epoch": 1.3313371690998994,
      "grad_norm": 4.551170825958252,
      "learning_rate": 2.7817789607362176e-05,
      "loss": 0.9588,
      "step": 25510
    },
    {
      "epoch": 1.3318590421021044,
      "grad_norm": 4.506586074829102,
      "learning_rate": 2.7809091383539483e-05,
      "loss": 0.9924,
      "step": 25520
    },
    {
      "epoch": 1.3323809151043093,
      "grad_norm": 4.8014020919799805,
      "learning_rate": 2.7800393159716786e-05,
      "loss": 0.8578,
      "step": 25530
    },
    {
      "epoch": 1.3329027881065143,
      "grad_norm": 4.407503128051758,
      "learning_rate": 2.779169493589409e-05,
      "loss": 0.8889,
      "step": 25540
    },
    {
      "epoch": 1.3334246611087193,
      "grad_norm": 4.522455215454102,
      "learning_rate": 2.7782996712071397e-05,
      "loss": 0.9124,
      "step": 25550
    },
    {
      "epoch": 1.333946534110924,
      "grad_norm": 4.528085708618164,
      "learning_rate": 2.77742984882487e-05,
      "loss": 0.9132,
      "step": 25560
    },
    {
      "epoch": 1.334468407113129,
      "grad_norm": 4.700680732727051,
      "learning_rate": 2.7765600264426007e-05,
      "loss": 0.8924,
      "step": 25570
    },
    {
      "epoch": 1.334990280115334,
      "grad_norm": 5.137636661529541,
      "learning_rate": 2.775690204060331e-05,
      "loss": 0.8753,
      "step": 25580
    },
    {
      "epoch": 1.335512153117539,
      "grad_norm": 3.5156593322753906,
      "learning_rate": 2.7748203816780615e-05,
      "loss": 0.8763,
      "step": 25590
    },
    {
      "epoch": 1.3360340261197439,
      "grad_norm": 4.583376884460449,
      "learning_rate": 2.773950559295792e-05,
      "loss": 0.8946,
      "step": 25600
    },
    {
      "epoch": 1.3365558991219486,
      "grad_norm": 3.5512540340423584,
      "learning_rate": 2.7730807369135225e-05,
      "loss": 0.9325,
      "step": 25610
    },
    {
      "epoch": 1.3370777721241536,
      "grad_norm": 4.862934589385986,
      "learning_rate": 2.7722109145312532e-05,
      "loss": 0.8895,
      "step": 25620
    },
    {
      "epoch": 1.3375996451263585,
      "grad_norm": 4.245427131652832,
      "learning_rate": 2.7713410921489836e-05,
      "loss": 0.9661,
      "step": 25630
    },
    {
      "epoch": 1.3381215181285633,
      "grad_norm": 5.442684173583984,
      "learning_rate": 2.770471269766714e-05,
      "loss": 1.0007,
      "step": 25640
    },
    {
      "epoch": 1.3386433911307682,
      "grad_norm": 4.4695329666137695,
      "learning_rate": 2.7696014473844446e-05,
      "loss": 0.9222,
      "step": 25650
    },
    {
      "epoch": 1.3391652641329732,
      "grad_norm": 5.013552665710449,
      "learning_rate": 2.7687316250021743e-05,
      "loss": 0.837,
      "step": 25660
    },
    {
      "epoch": 1.3396871371351782,
      "grad_norm": 5.098879337310791,
      "learning_rate": 2.767861802619905e-05,
      "loss": 0.9257,
      "step": 25670
    },
    {
      "epoch": 1.3402090101373831,
      "grad_norm": 4.560725688934326,
      "learning_rate": 2.7669919802376353e-05,
      "loss": 0.8575,
      "step": 25680
    },
    {
      "epoch": 1.3407308831395879,
      "grad_norm": 4.093288898468018,
      "learning_rate": 2.766122157855366e-05,
      "loss": 0.8702,
      "step": 25690
    },
    {
      "epoch": 1.3412527561417928,
      "grad_norm": 4.107720851898193,
      "learning_rate": 2.7652523354730964e-05,
      "loss": 0.9779,
      "step": 25700
    },
    {
      "epoch": 1.3417746291439978,
      "grad_norm": 4.120087623596191,
      "learning_rate": 2.7643825130908267e-05,
      "loss": 0.9548,
      "step": 25710
    },
    {
      "epoch": 1.3422965021462028,
      "grad_norm": 4.173940658569336,
      "learning_rate": 2.7635126907085574e-05,
      "loss": 0.9301,
      "step": 25720
    },
    {
      "epoch": 1.3428183751484077,
      "grad_norm": 4.113811492919922,
      "learning_rate": 2.7626428683262878e-05,
      "loss": 0.8936,
      "step": 25730
    },
    {
      "epoch": 1.3433402481506125,
      "grad_norm": 3.611642360687256,
      "learning_rate": 2.7617730459440185e-05,
      "loss": 0.9774,
      "step": 25740
    },
    {
      "epoch": 1.3438621211528174,
      "grad_norm": 5.363929271697998,
      "learning_rate": 2.7609032235617488e-05,
      "loss": 0.9948,
      "step": 25750
    },
    {
      "epoch": 1.3443839941550224,
      "grad_norm": 4.20024299621582,
      "learning_rate": 2.7600334011794792e-05,
      "loss": 0.8801,
      "step": 25760
    },
    {
      "epoch": 1.3449058671572272,
      "grad_norm": 4.435460090637207,
      "learning_rate": 2.75916357879721e-05,
      "loss": 0.9887,
      "step": 25770
    },
    {
      "epoch": 1.3454277401594323,
      "grad_norm": 5.299761772155762,
      "learning_rate": 2.7582937564149402e-05,
      "loss": 0.8287,
      "step": 25780
    },
    {
      "epoch": 1.345949613161637,
      "grad_norm": 3.7686173915863037,
      "learning_rate": 2.7574239340326706e-05,
      "loss": 0.848,
      "step": 25790
    },
    {
      "epoch": 1.346471486163842,
      "grad_norm": 3.6520118713378906,
      "learning_rate": 2.7565541116504013e-05,
      "loss": 0.8073,
      "step": 25800
    },
    {
      "epoch": 1.346993359166047,
      "grad_norm": 5.367930889129639,
      "learning_rate": 2.7556842892681316e-05,
      "loss": 0.8948,
      "step": 25810
    },
    {
      "epoch": 1.3475152321682518,
      "grad_norm": 4.9612860679626465,
      "learning_rate": 2.7548144668858623e-05,
      "loss": 0.9698,
      "step": 25820
    },
    {
      "epoch": 1.3480371051704567,
      "grad_norm": 4.347167491912842,
      "learning_rate": 2.7539446445035927e-05,
      "loss": 0.9533,
      "step": 25830
    },
    {
      "epoch": 1.3485589781726617,
      "grad_norm": 5.3118767738342285,
      "learning_rate": 2.753074822121323e-05,
      "loss": 0.8095,
      "step": 25840
    },
    {
      "epoch": 1.3490808511748666,
      "grad_norm": 4.274835586547852,
      "learning_rate": 2.7522049997390537e-05,
      "loss": 0.8712,
      "step": 25850
    },
    {
      "epoch": 1.3496027241770716,
      "grad_norm": 3.648836612701416,
      "learning_rate": 2.751335177356784e-05,
      "loss": 0.7959,
      "step": 25860
    },
    {
      "epoch": 1.3501245971792764,
      "grad_norm": 5.061746120452881,
      "learning_rate": 2.750465354974514e-05,
      "loss": 0.8394,
      "step": 25870
    },
    {
      "epoch": 1.3506464701814813,
      "grad_norm": 5.277658939361572,
      "learning_rate": 2.7495955325922445e-05,
      "loss": 0.9256,
      "step": 25880
    },
    {
      "epoch": 1.3511683431836863,
      "grad_norm": 4.571814060211182,
      "learning_rate": 2.748725710209975e-05,
      "loss": 0.9619,
      "step": 25890
    },
    {
      "epoch": 1.351690216185891,
      "grad_norm": 4.286448955535889,
      "learning_rate": 2.7478558878277055e-05,
      "loss": 0.9228,
      "step": 25900
    },
    {
      "epoch": 1.3522120891880962,
      "grad_norm": 4.550065040588379,
      "learning_rate": 2.746986065445436e-05,
      "loss": 0.8774,
      "step": 25910
    },
    {
      "epoch": 1.352733962190301,
      "grad_norm": 4.903728485107422,
      "learning_rate": 2.7461162430631666e-05,
      "loss": 0.8598,
      "step": 25920
    },
    {
      "epoch": 1.353255835192506,
      "grad_norm": 3.223409652709961,
      "learning_rate": 2.745246420680897e-05,
      "loss": 0.9203,
      "step": 25930
    },
    {
      "epoch": 1.3537777081947109,
      "grad_norm": 4.922258377075195,
      "learning_rate": 2.7443765982986276e-05,
      "loss": 0.9759,
      "step": 25940
    },
    {
      "epoch": 1.3542995811969156,
      "grad_norm": 5.547544002532959,
      "learning_rate": 2.743506775916358e-05,
      "loss": 0.9233,
      "step": 25950
    },
    {
      "epoch": 1.3548214541991206,
      "grad_norm": 4.049901962280273,
      "learning_rate": 2.7426369535340883e-05,
      "loss": 0.9044,
      "step": 25960
    },
    {
      "epoch": 1.3553433272013256,
      "grad_norm": 4.813560962677002,
      "learning_rate": 2.741767131151819e-05,
      "loss": 1.0283,
      "step": 25970
    },
    {
      "epoch": 1.3558652002035305,
      "grad_norm": 4.379031658172607,
      "learning_rate": 2.7408973087695494e-05,
      "loss": 0.941,
      "step": 25980
    },
    {
      "epoch": 1.3563870732057355,
      "grad_norm": 4.779653549194336,
      "learning_rate": 2.74002748638728e-05,
      "loss": 0.9687,
      "step": 25990
    },
    {
      "epoch": 1.3569089462079402,
      "grad_norm": 5.146736145019531,
      "learning_rate": 2.7391576640050104e-05,
      "loss": 0.9046,
      "step": 26000
    },
    {
      "epoch": 1.3574308192101452,
      "grad_norm": 4.714215278625488,
      "learning_rate": 2.7382878416227408e-05,
      "loss": 0.8782,
      "step": 26010
    },
    {
      "epoch": 1.3579526922123502,
      "grad_norm": 4.948359966278076,
      "learning_rate": 2.7374180192404715e-05,
      "loss": 0.8835,
      "step": 26020
    },
    {
      "epoch": 1.3584745652145551,
      "grad_norm": 5.277137756347656,
      "learning_rate": 2.7365481968582018e-05,
      "loss": 0.8937,
      "step": 26030
    },
    {
      "epoch": 1.35899643821676,
      "grad_norm": 4.358494758605957,
      "learning_rate": 2.7356783744759322e-05,
      "loss": 0.9666,
      "step": 26040
    },
    {
      "epoch": 1.3595183112189648,
      "grad_norm": 4.461777687072754,
      "learning_rate": 2.734808552093663e-05,
      "loss": 0.8438,
      "step": 26050
    },
    {
      "epoch": 1.3600401842211698,
      "grad_norm": 3.4160873889923096,
      "learning_rate": 2.7339387297113932e-05,
      "loss": 0.9113,
      "step": 26060
    },
    {
      "epoch": 1.3605620572233748,
      "grad_norm": 4.356808662414551,
      "learning_rate": 2.733068907329124e-05,
      "loss": 0.9027,
      "step": 26070
    },
    {
      "epoch": 1.3610839302255795,
      "grad_norm": 3.337340831756592,
      "learning_rate": 2.7321990849468536e-05,
      "loss": 0.7806,
      "step": 26080
    },
    {
      "epoch": 1.3616058032277845,
      "grad_norm": 4.316141605377197,
      "learning_rate": 2.7313292625645843e-05,
      "loss": 0.8012,
      "step": 26090
    },
    {
      "epoch": 1.3621276762299894,
      "grad_norm": 3.875211477279663,
      "learning_rate": 2.7304594401823146e-05,
      "loss": 0.9311,
      "step": 26100
    },
    {
      "epoch": 1.3626495492321944,
      "grad_norm": 5.027014255523682,
      "learning_rate": 2.7295896178000453e-05,
      "loss": 0.8354,
      "step": 26110
    },
    {
      "epoch": 1.3631714222343994,
      "grad_norm": 4.1350932121276855,
      "learning_rate": 2.7287197954177757e-05,
      "loss": 0.8389,
      "step": 26120
    },
    {
      "epoch": 1.363693295236604,
      "grad_norm": 4.547918796539307,
      "learning_rate": 2.727849973035506e-05,
      "loss": 1.0178,
      "step": 26130
    },
    {
      "epoch": 1.364215168238809,
      "grad_norm": 5.059506893157959,
      "learning_rate": 2.7269801506532367e-05,
      "loss": 0.9255,
      "step": 26140
    },
    {
      "epoch": 1.364737041241014,
      "grad_norm": 5.161921977996826,
      "learning_rate": 2.726110328270967e-05,
      "loss": 0.9165,
      "step": 26150
    },
    {
      "epoch": 1.365258914243219,
      "grad_norm": 3.791109085083008,
      "learning_rate": 2.7252405058886978e-05,
      "loss": 1.003,
      "step": 26160
    },
    {
      "epoch": 1.365780787245424,
      "grad_norm": 4.185595512390137,
      "learning_rate": 2.724370683506428e-05,
      "loss": 0.8624,
      "step": 26170
    },
    {
      "epoch": 1.3663026602476287,
      "grad_norm": 5.228285789489746,
      "learning_rate": 2.7235008611241585e-05,
      "loss": 0.8152,
      "step": 26180
    },
    {
      "epoch": 1.3668245332498337,
      "grad_norm": 4.280328750610352,
      "learning_rate": 2.7226310387418892e-05,
      "loss": 0.9491,
      "step": 26190
    },
    {
      "epoch": 1.3673464062520386,
      "grad_norm": 4.507463455200195,
      "learning_rate": 2.7217612163596195e-05,
      "loss": 0.8886,
      "step": 26200
    },
    {
      "epoch": 1.3678682792542434,
      "grad_norm": 5.060512542724609,
      "learning_rate": 2.72089139397735e-05,
      "loss": 0.8633,
      "step": 26210
    },
    {
      "epoch": 1.3683901522564483,
      "grad_norm": 4.725316047668457,
      "learning_rate": 2.7200215715950806e-05,
      "loss": 0.9994,
      "step": 26220
    },
    {
      "epoch": 1.3689120252586533,
      "grad_norm": 3.7493343353271484,
      "learning_rate": 2.719151749212811e-05,
      "loss": 0.9367,
      "step": 26230
    },
    {
      "epoch": 1.3694338982608583,
      "grad_norm": 4.197308540344238,
      "learning_rate": 2.7182819268305416e-05,
      "loss": 0.8524,
      "step": 26240
    },
    {
      "epoch": 1.3699557712630632,
      "grad_norm": 3.490764856338501,
      "learning_rate": 2.717412104448272e-05,
      "loss": 0.9299,
      "step": 26250
    },
    {
      "epoch": 1.370477644265268,
      "grad_norm": 3.717836380004883,
      "learning_rate": 2.7165422820660023e-05,
      "loss": 0.8755,
      "step": 26260
    },
    {
      "epoch": 1.370999517267473,
      "grad_norm": 3.6145012378692627,
      "learning_rate": 2.715672459683733e-05,
      "loss": 0.8797,
      "step": 26270
    },
    {
      "epoch": 1.371521390269678,
      "grad_norm": 4.856585502624512,
      "learning_rate": 2.7148026373014634e-05,
      "loss": 1.0069,
      "step": 26280
    },
    {
      "epoch": 1.3720432632718829,
      "grad_norm": 3.494274139404297,
      "learning_rate": 2.7139328149191934e-05,
      "loss": 0.8525,
      "step": 26290
    },
    {
      "epoch": 1.3725651362740878,
      "grad_norm": 4.051092147827148,
      "learning_rate": 2.7130629925369238e-05,
      "loss": 0.862,
      "step": 26300
    },
    {
      "epoch": 1.3730870092762926,
      "grad_norm": 4.580018997192383,
      "learning_rate": 2.7121931701546545e-05,
      "loss": 0.8124,
      "step": 26310
    },
    {
      "epoch": 1.3736088822784975,
      "grad_norm": 4.689774990081787,
      "learning_rate": 2.7113233477723848e-05,
      "loss": 0.8833,
      "step": 26320
    },
    {
      "epoch": 1.3741307552807025,
      "grad_norm": 3.9634850025177,
      "learning_rate": 2.7104535253901152e-05,
      "loss": 0.9007,
      "step": 26330
    },
    {
      "epoch": 1.3746526282829072,
      "grad_norm": 3.427213430404663,
      "learning_rate": 2.709583703007846e-05,
      "loss": 0.8956,
      "step": 26340
    },
    {
      "epoch": 1.3751745012851122,
      "grad_norm": 4.901822090148926,
      "learning_rate": 2.7087138806255762e-05,
      "loss": 0.9475,
      "step": 26350
    },
    {
      "epoch": 1.3756963742873172,
      "grad_norm": 3.6993954181671143,
      "learning_rate": 2.707844058243307e-05,
      "loss": 0.8409,
      "step": 26360
    },
    {
      "epoch": 1.3762182472895221,
      "grad_norm": 3.6514933109283447,
      "learning_rate": 2.7069742358610373e-05,
      "loss": 0.8076,
      "step": 26370
    },
    {
      "epoch": 1.376740120291727,
      "grad_norm": 3.8351259231567383,
      "learning_rate": 2.7061044134787676e-05,
      "loss": 0.8253,
      "step": 26380
    },
    {
      "epoch": 1.3772619932939318,
      "grad_norm": 4.057377815246582,
      "learning_rate": 2.7052345910964983e-05,
      "loss": 0.8634,
      "step": 26390
    },
    {
      "epoch": 1.3777838662961368,
      "grad_norm": 3.767824411392212,
      "learning_rate": 2.7043647687142287e-05,
      "loss": 0.9256,
      "step": 26400
    },
    {
      "epoch": 1.3783057392983418,
      "grad_norm": 5.0591325759887695,
      "learning_rate": 2.7034949463319594e-05,
      "loss": 0.7997,
      "step": 26410
    },
    {
      "epoch": 1.3788276123005467,
      "grad_norm": 4.5906758308410645,
      "learning_rate": 2.7026251239496897e-05,
      "loss": 0.8267,
      "step": 26420
    },
    {
      "epoch": 1.3793494853027517,
      "grad_norm": 4.5393195152282715,
      "learning_rate": 2.70175530156742e-05,
      "loss": 0.9468,
      "step": 26430
    },
    {
      "epoch": 1.3798713583049564,
      "grad_norm": 5.016193389892578,
      "learning_rate": 2.7008854791851508e-05,
      "loss": 0.8682,
      "step": 26440
    },
    {
      "epoch": 1.3803932313071614,
      "grad_norm": 4.102967739105225,
      "learning_rate": 2.700015656802881e-05,
      "loss": 0.9403,
      "step": 26450
    },
    {
      "epoch": 1.3809151043093664,
      "grad_norm": 4.428144454956055,
      "learning_rate": 2.6991458344206115e-05,
      "loss": 0.9366,
      "step": 26460
    },
    {
      "epoch": 1.3814369773115711,
      "grad_norm": 4.753683090209961,
      "learning_rate": 2.6982760120383422e-05,
      "loss": 0.9054,
      "step": 26470
    },
    {
      "epoch": 1.381958850313776,
      "grad_norm": 4.323885440826416,
      "learning_rate": 2.6974061896560725e-05,
      "loss": 0.8966,
      "step": 26480
    },
    {
      "epoch": 1.382480723315981,
      "grad_norm": 3.272675037384033,
      "learning_rate": 2.6965363672738025e-05,
      "loss": 0.897,
      "step": 26490
    },
    {
      "epoch": 1.383002596318186,
      "grad_norm": 5.030580997467041,
      "learning_rate": 2.695666544891533e-05,
      "loss": 0.9124,
      "step": 26500
    },
    {
      "epoch": 1.383524469320391,
      "grad_norm": 4.464049816131592,
      "learning_rate": 2.6947967225092636e-05,
      "loss": 0.9164,
      "step": 26510
    },
    {
      "epoch": 1.3840463423225957,
      "grad_norm": 4.630090713500977,
      "learning_rate": 2.693926900126994e-05,
      "loss": 0.9021,
      "step": 26520
    },
    {
      "epoch": 1.3845682153248007,
      "grad_norm": 4.2580246925354,
      "learning_rate": 2.6930570777447246e-05,
      "loss": 0.8993,
      "step": 26530
    },
    {
      "epoch": 1.3850900883270056,
      "grad_norm": 5.075238227844238,
      "learning_rate": 2.692187255362455e-05,
      "loss": 0.9191,
      "step": 26540
    },
    {
      "epoch": 1.3856119613292106,
      "grad_norm": 4.822109222412109,
      "learning_rate": 2.6913174329801854e-05,
      "loss": 0.8384,
      "step": 26550
    },
    {
      "epoch": 1.3861338343314156,
      "grad_norm": 3.6060707569122314,
      "learning_rate": 2.690447610597916e-05,
      "loss": 0.7808,
      "step": 26560
    },
    {
      "epoch": 1.3866557073336203,
      "grad_norm": 4.845661163330078,
      "learning_rate": 2.6895777882156464e-05,
      "loss": 1.0141,
      "step": 26570
    },
    {
      "epoch": 1.3871775803358253,
      "grad_norm": 4.63943338394165,
      "learning_rate": 2.6887079658333768e-05,
      "loss": 0.9187,
      "step": 26580
    },
    {
      "epoch": 1.3876994533380302,
      "grad_norm": 4.60002326965332,
      "learning_rate": 2.6878381434511074e-05,
      "loss": 1.0052,
      "step": 26590
    },
    {
      "epoch": 1.388221326340235,
      "grad_norm": 4.1409382820129395,
      "learning_rate": 2.6869683210688378e-05,
      "loss": 0.9405,
      "step": 26600
    },
    {
      "epoch": 1.38874319934244,
      "grad_norm": 4.522759437561035,
      "learning_rate": 2.6860984986865685e-05,
      "loss": 0.8481,
      "step": 26610
    },
    {
      "epoch": 1.389265072344645,
      "grad_norm": 3.5284414291381836,
      "learning_rate": 2.685228676304299e-05,
      "loss": 0.8064,
      "step": 26620
    },
    {
      "epoch": 1.3897869453468499,
      "grad_norm": 5.047085285186768,
      "learning_rate": 2.6843588539220292e-05,
      "loss": 1.0006,
      "step": 26630
    },
    {
      "epoch": 1.3903088183490548,
      "grad_norm": 4.496644973754883,
      "learning_rate": 2.68348903153976e-05,
      "loss": 0.8378,
      "step": 26640
    },
    {
      "epoch": 1.3908306913512596,
      "grad_norm": 4.306619167327881,
      "learning_rate": 2.6826192091574903e-05,
      "loss": 0.9253,
      "step": 26650
    },
    {
      "epoch": 1.3913525643534645,
      "grad_norm": 3.741790771484375,
      "learning_rate": 2.681749386775221e-05,
      "loss": 0.8444,
      "step": 26660
    },
    {
      "epoch": 1.3918744373556695,
      "grad_norm": 3.890312671661377,
      "learning_rate": 2.6808795643929513e-05,
      "loss": 0.9358,
      "step": 26670
    },
    {
      "epoch": 1.3923963103578745,
      "grad_norm": 3.5811169147491455,
      "learning_rate": 2.6800097420106817e-05,
      "loss": 0.8531,
      "step": 26680
    },
    {
      "epoch": 1.3929181833600794,
      "grad_norm": 4.453181266784668,
      "learning_rate": 2.6791399196284124e-05,
      "loss": 0.7949,
      "step": 26690
    },
    {
      "epoch": 1.3934400563622842,
      "grad_norm": 4.609302997589111,
      "learning_rate": 2.678270097246142e-05,
      "loss": 0.9192,
      "step": 26700
    },
    {
      "epoch": 1.3939619293644891,
      "grad_norm": 3.993479013442993,
      "learning_rate": 2.6774002748638727e-05,
      "loss": 0.9048,
      "step": 26710
    },
    {
      "epoch": 1.3944838023666941,
      "grad_norm": 4.701344013214111,
      "learning_rate": 2.676530452481603e-05,
      "loss": 0.7988,
      "step": 26720
    },
    {
      "epoch": 1.3950056753688989,
      "grad_norm": 3.57724666595459,
      "learning_rate": 2.6756606300993338e-05,
      "loss": 0.9166,
      "step": 26730
    },
    {
      "epoch": 1.3955275483711038,
      "grad_norm": 4.934535503387451,
      "learning_rate": 2.674790807717064e-05,
      "loss": 0.9824,
      "step": 26740
    },
    {
      "epoch": 1.3960494213733088,
      "grad_norm": 4.96039342880249,
      "learning_rate": 2.6739209853347945e-05,
      "loss": 1.0014,
      "step": 26750
    },
    {
      "epoch": 1.3965712943755137,
      "grad_norm": 3.4219934940338135,
      "learning_rate": 2.6730511629525252e-05,
      "loss": 0.8412,
      "step": 26760
    },
    {
      "epoch": 1.3970931673777187,
      "grad_norm": 4.710256576538086,
      "learning_rate": 2.6721813405702555e-05,
      "loss": 0.9285,
      "step": 26770
    },
    {
      "epoch": 1.3976150403799235,
      "grad_norm": 4.535994052886963,
      "learning_rate": 2.6713115181879862e-05,
      "loss": 0.9107,
      "step": 26780
    },
    {
      "epoch": 1.3981369133821284,
      "grad_norm": 4.780555248260498,
      "learning_rate": 2.6704416958057166e-05,
      "loss": 0.8861,
      "step": 26790
    },
    {
      "epoch": 1.3986587863843334,
      "grad_norm": 4.639938831329346,
      "learning_rate": 2.669571873423447e-05,
      "loss": 0.8997,
      "step": 26800
    },
    {
      "epoch": 1.3991806593865384,
      "grad_norm": 6.0046916007995605,
      "learning_rate": 2.6687020510411776e-05,
      "loss": 0.8763,
      "step": 26810
    },
    {
      "epoch": 1.3997025323887433,
      "grad_norm": 4.123882293701172,
      "learning_rate": 2.6679192108971353e-05,
      "loss": 0.9462,
      "step": 26820
    },
    {
      "epoch": 1.400224405390948,
      "grad_norm": 4.8545308113098145,
      "learning_rate": 2.6670493885148656e-05,
      "loss": 0.8861,
      "step": 26830
    },
    {
      "epoch": 1.400746278393153,
      "grad_norm": 5.04343318939209,
      "learning_rate": 2.666179566132596e-05,
      "loss": 0.9271,
      "step": 26840
    },
    {
      "epoch": 1.401268151395358,
      "grad_norm": 4.28737211227417,
      "learning_rate": 2.665309743750326e-05,
      "loss": 0.9064,
      "step": 26850
    },
    {
      "epoch": 1.4017900243975627,
      "grad_norm": 4.771765232086182,
      "learning_rate": 2.6644399213680564e-05,
      "loss": 0.8766,
      "step": 26860
    },
    {
      "epoch": 1.4023118973997677,
      "grad_norm": 3.796860694885254,
      "learning_rate": 2.663570098985787e-05,
      "loss": 0.8586,
      "step": 26870
    },
    {
      "epoch": 1.4028337704019727,
      "grad_norm": 5.1384687423706055,
      "learning_rate": 2.6627002766035174e-05,
      "loss": 0.8825,
      "step": 26880
    },
    {
      "epoch": 1.4033556434041776,
      "grad_norm": 4.595959186553955,
      "learning_rate": 2.661830454221248e-05,
      "loss": 0.8508,
      "step": 26890
    },
    {
      "epoch": 1.4038775164063826,
      "grad_norm": 3.91079044342041,
      "learning_rate": 2.6609606318389785e-05,
      "loss": 0.8242,
      "step": 26900
    },
    {
      "epoch": 1.4043993894085873,
      "grad_norm": 4.525021553039551,
      "learning_rate": 2.6600908094567088e-05,
      "loss": 0.9251,
      "step": 26910
    },
    {
      "epoch": 1.4049212624107923,
      "grad_norm": 4.888672828674316,
      "learning_rate": 2.6592209870744395e-05,
      "loss": 0.865,
      "step": 26920
    },
    {
      "epoch": 1.4054431354129973,
      "grad_norm": 5.021195888519287,
      "learning_rate": 2.65835116469217e-05,
      "loss": 0.9799,
      "step": 26930
    },
    {
      "epoch": 1.4059650084152022,
      "grad_norm": 3.3572049140930176,
      "learning_rate": 2.6574813423099006e-05,
      "loss": 0.8999,
      "step": 26940
    },
    {
      "epoch": 1.4064868814174072,
      "grad_norm": 4.9000020027160645,
      "learning_rate": 2.656611519927631e-05,
      "loss": 0.9437,
      "step": 26950
    },
    {
      "epoch": 1.407008754419612,
      "grad_norm": 4.178442001342773,
      "learning_rate": 2.6557416975453613e-05,
      "loss": 0.7852,
      "step": 26960
    },
    {
      "epoch": 1.407530627421817,
      "grad_norm": 4.098694801330566,
      "learning_rate": 2.654871875163092e-05,
      "loss": 0.8975,
      "step": 26970
    },
    {
      "epoch": 1.4080525004240219,
      "grad_norm": 3.8251893520355225,
      "learning_rate": 2.6540020527808223e-05,
      "loss": 0.8924,
      "step": 26980
    },
    {
      "epoch": 1.4085743734262266,
      "grad_norm": 4.711676597595215,
      "learning_rate": 2.653132230398553e-05,
      "loss": 0.9095,
      "step": 26990
    },
    {
      "epoch": 1.4090962464284316,
      "grad_norm": 3.4575490951538086,
      "learning_rate": 2.6522624080162834e-05,
      "loss": 0.8197,
      "step": 27000
    },
    {
      "epoch": 1.4096181194306365,
      "grad_norm": 5.075685024261475,
      "learning_rate": 2.6513925856340137e-05,
      "loss": 0.9292,
      "step": 27010
    },
    {
      "epoch": 1.4101399924328415,
      "grad_norm": 3.739419460296631,
      "learning_rate": 2.6505227632517444e-05,
      "loss": 0.8138,
      "step": 27020
    },
    {
      "epoch": 1.4106618654350465,
      "grad_norm": 5.098340034484863,
      "learning_rate": 2.6496529408694748e-05,
      "loss": 0.9892,
      "step": 27030
    },
    {
      "epoch": 1.4111837384372512,
      "grad_norm": 4.3321051597595215,
      "learning_rate": 2.648783118487205e-05,
      "loss": 0.8904,
      "step": 27040
    },
    {
      "epoch": 1.4117056114394562,
      "grad_norm": 4.03415584564209,
      "learning_rate": 2.6479132961049358e-05,
      "loss": 0.7605,
      "step": 27050
    },
    {
      "epoch": 1.4122274844416611,
      "grad_norm": 4.2400383949279785,
      "learning_rate": 2.647043473722666e-05,
      "loss": 0.982,
      "step": 27060
    },
    {
      "epoch": 1.412749357443866,
      "grad_norm": 4.70097017288208,
      "learning_rate": 2.6461736513403962e-05,
      "loss": 1.0365,
      "step": 27070
    },
    {
      "epoch": 1.413271230446071,
      "grad_norm": 4.011623382568359,
      "learning_rate": 2.6453038289581265e-05,
      "loss": 0.8414,
      "step": 27080
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 4.047351837158203,
      "learning_rate": 2.6444340065758572e-05,
      "loss": 1.0173,
      "step": 27090
    },
    {
      "epoch": 1.4143149764504808,
      "grad_norm": 3.8736488819122314,
      "learning_rate": 2.6435641841935876e-05,
      "loss": 0.8738,
      "step": 27100
    },
    {
      "epoch": 1.4148368494526857,
      "grad_norm": 4.774497985839844,
      "learning_rate": 2.6426943618113183e-05,
      "loss": 0.9165,
      "step": 27110
    },
    {
      "epoch": 1.4153587224548905,
      "grad_norm": 4.834358215332031,
      "learning_rate": 2.6418245394290486e-05,
      "loss": 0.8638,
      "step": 27120
    },
    {
      "epoch": 1.4158805954570954,
      "grad_norm": 5.012382507324219,
      "learning_rate": 2.640954717046779e-05,
      "loss": 0.87,
      "step": 27130
    },
    {
      "epoch": 1.4164024684593004,
      "grad_norm": 4.007693290710449,
      "learning_rate": 2.6400848946645097e-05,
      "loss": 0.8427,
      "step": 27140
    },
    {
      "epoch": 1.4169243414615054,
      "grad_norm": 4.403949737548828,
      "learning_rate": 2.63921507228224e-05,
      "loss": 0.9488,
      "step": 27150
    },
    {
      "epoch": 1.4174462144637103,
      "grad_norm": 4.606173515319824,
      "learning_rate": 2.6383452498999704e-05,
      "loss": 0.9095,
      "step": 27160
    },
    {
      "epoch": 1.417968087465915,
      "grad_norm": 4.681257247924805,
      "learning_rate": 2.637475427517701e-05,
      "loss": 0.9495,
      "step": 27170
    },
    {
      "epoch": 1.41848996046812,
      "grad_norm": 4.860471725463867,
      "learning_rate": 2.6366056051354315e-05,
      "loss": 0.9739,
      "step": 27180
    },
    {
      "epoch": 1.419011833470325,
      "grad_norm": 4.126245498657227,
      "learning_rate": 2.635735782753162e-05,
      "loss": 0.9077,
      "step": 27190
    },
    {
      "epoch": 1.41953370647253,
      "grad_norm": 4.102245330810547,
      "learning_rate": 2.6348659603708925e-05,
      "loss": 0.9466,
      "step": 27200
    },
    {
      "epoch": 1.420055579474735,
      "grad_norm": 4.865116596221924,
      "learning_rate": 2.633996137988623e-05,
      "loss": 0.8401,
      "step": 27210
    },
    {
      "epoch": 1.4205774524769397,
      "grad_norm": 4.344300746917725,
      "learning_rate": 2.6331263156063535e-05,
      "loss": 0.8559,
      "step": 27220
    },
    {
      "epoch": 1.4210993254791446,
      "grad_norm": 4.220940589904785,
      "learning_rate": 2.632256493224084e-05,
      "loss": 0.8206,
      "step": 27230
    },
    {
      "epoch": 1.4216211984813496,
      "grad_norm": 4.272971153259277,
      "learning_rate": 2.6313866708418146e-05,
      "loss": 0.8814,
      "step": 27240
    },
    {
      "epoch": 1.4221430714835543,
      "grad_norm": 4.089020252227783,
      "learning_rate": 2.630516848459545e-05,
      "loss": 0.8472,
      "step": 27250
    },
    {
      "epoch": 1.4226649444857593,
      "grad_norm": 4.886687755584717,
      "learning_rate": 2.6296470260772753e-05,
      "loss": 0.9525,
      "step": 27260
    },
    {
      "epoch": 1.4231868174879643,
      "grad_norm": 4.931058883666992,
      "learning_rate": 2.6287772036950053e-05,
      "loss": 0.9018,
      "step": 27270
    },
    {
      "epoch": 1.4237086904901692,
      "grad_norm": 4.255046367645264,
      "learning_rate": 2.6279073813127357e-05,
      "loss": 0.8328,
      "step": 27280
    },
    {
      "epoch": 1.4242305634923742,
      "grad_norm": 3.8739538192749023,
      "learning_rate": 2.6270375589304664e-05,
      "loss": 0.8491,
      "step": 27290
    },
    {
      "epoch": 1.424752436494579,
      "grad_norm": 4.5532145500183105,
      "learning_rate": 2.6261677365481967e-05,
      "loss": 0.8619,
      "step": 27300
    },
    {
      "epoch": 1.425274309496784,
      "grad_norm": 4.615657806396484,
      "learning_rate": 2.6252979141659274e-05,
      "loss": 0.9394,
      "step": 27310
    },
    {
      "epoch": 1.4257961824989889,
      "grad_norm": 3.7848737239837646,
      "learning_rate": 2.6244280917836578e-05,
      "loss": 0.8918,
      "step": 27320
    },
    {
      "epoch": 1.4263180555011938,
      "grad_norm": 4.398906230926514,
      "learning_rate": 2.623558269401388e-05,
      "loss": 0.8613,
      "step": 27330
    },
    {
      "epoch": 1.4268399285033988,
      "grad_norm": 4.32411527633667,
      "learning_rate": 2.6226884470191188e-05,
      "loss": 0.9132,
      "step": 27340
    },
    {
      "epoch": 1.4273618015056035,
      "grad_norm": 4.9945855140686035,
      "learning_rate": 2.6218186246368492e-05,
      "loss": 0.8768,
      "step": 27350
    },
    {
      "epoch": 1.4278836745078085,
      "grad_norm": 4.452817916870117,
      "learning_rate": 2.62094880225458e-05,
      "loss": 0.8611,
      "step": 27360
    },
    {
      "epoch": 1.4284055475100135,
      "grad_norm": 4.680656433105469,
      "learning_rate": 2.6200789798723102e-05,
      "loss": 0.9385,
      "step": 27370
    },
    {
      "epoch": 1.4289274205122182,
      "grad_norm": 5.080206871032715,
      "learning_rate": 2.6192091574900406e-05,
      "loss": 0.8703,
      "step": 27380
    },
    {
      "epoch": 1.4294492935144234,
      "grad_norm": 4.191443920135498,
      "learning_rate": 2.6183393351077713e-05,
      "loss": 0.8136,
      "step": 27390
    },
    {
      "epoch": 1.4299711665166281,
      "grad_norm": 4.1250481605529785,
      "learning_rate": 2.6174695127255016e-05,
      "loss": 0.9005,
      "step": 27400
    },
    {
      "epoch": 1.430493039518833,
      "grad_norm": 4.120203018188477,
      "learning_rate": 2.6165996903432323e-05,
      "loss": 0.9456,
      "step": 27410
    },
    {
      "epoch": 1.431014912521038,
      "grad_norm": 4.329132080078125,
      "learning_rate": 2.6157298679609627e-05,
      "loss": 0.8742,
      "step": 27420
    },
    {
      "epoch": 1.4315367855232428,
      "grad_norm": 4.647845268249512,
      "learning_rate": 2.614860045578693e-05,
      "loss": 0.9214,
      "step": 27430
    },
    {
      "epoch": 1.4320586585254478,
      "grad_norm": 4.295416831970215,
      "learning_rate": 2.6139902231964237e-05,
      "loss": 1.0422,
      "step": 27440
    },
    {
      "epoch": 1.4325805315276527,
      "grad_norm": 4.12346076965332,
      "learning_rate": 2.613120400814154e-05,
      "loss": 0.8355,
      "step": 27450
    },
    {
      "epoch": 1.4331024045298577,
      "grad_norm": 3.895113945007324,
      "learning_rate": 2.6122505784318844e-05,
      "loss": 0.852,
      "step": 27460
    },
    {
      "epoch": 1.4336242775320627,
      "grad_norm": 4.25316858291626,
      "learning_rate": 2.6113807560496145e-05,
      "loss": 0.8935,
      "step": 27470
    },
    {
      "epoch": 1.4341461505342674,
      "grad_norm": 3.928060531616211,
      "learning_rate": 2.610510933667345e-05,
      "loss": 0.8481,
      "step": 27480
    },
    {
      "epoch": 1.4346680235364724,
      "grad_norm": 4.523441791534424,
      "learning_rate": 2.6096411112850755e-05,
      "loss": 0.8767,
      "step": 27490
    },
    {
      "epoch": 1.4351898965386773,
      "grad_norm": 4.853462219238281,
      "learning_rate": 2.608771288902806e-05,
      "loss": 0.9774,
      "step": 27500
    },
    {
      "epoch": 1.435711769540882,
      "grad_norm": 3.9146077632904053,
      "learning_rate": 2.6079014665205366e-05,
      "loss": 0.8754,
      "step": 27510
    },
    {
      "epoch": 1.4362336425430873,
      "grad_norm": 4.152112007141113,
      "learning_rate": 2.607031644138267e-05,
      "loss": 0.8951,
      "step": 27520
    },
    {
      "epoch": 1.436755515545292,
      "grad_norm": 4.89197301864624,
      "learning_rate": 2.6061618217559976e-05,
      "loss": 0.9139,
      "step": 27530
    },
    {
      "epoch": 1.437277388547497,
      "grad_norm": 5.267276287078857,
      "learning_rate": 2.605291999373728e-05,
      "loss": 0.9061,
      "step": 27540
    },
    {
      "epoch": 1.437799261549702,
      "grad_norm": 4.957343101501465,
      "learning_rate": 2.6044221769914583e-05,
      "loss": 0.8283,
      "step": 27550
    },
    {
      "epoch": 1.4383211345519067,
      "grad_norm": 5.165684700012207,
      "learning_rate": 2.603552354609189e-05,
      "loss": 0.8775,
      "step": 27560
    },
    {
      "epoch": 1.4388430075541117,
      "grad_norm": 3.901585817337036,
      "learning_rate": 2.6026825322269194e-05,
      "loss": 0.834,
      "step": 27570
    },
    {
      "epoch": 1.4393648805563166,
      "grad_norm": 3.748190402984619,
      "learning_rate": 2.6018127098446497e-05,
      "loss": 0.865,
      "step": 27580
    },
    {
      "epoch": 1.4398867535585216,
      "grad_norm": 4.334434509277344,
      "learning_rate": 2.6009428874623804e-05,
      "loss": 0.9435,
      "step": 27590
    },
    {
      "epoch": 1.4404086265607265,
      "grad_norm": 3.8428828716278076,
      "learning_rate": 2.6000730650801108e-05,
      "loss": 0.7798,
      "step": 27600
    },
    {
      "epoch": 1.4409304995629313,
      "grad_norm": 3.892637252807617,
      "learning_rate": 2.5992032426978415e-05,
      "loss": 0.8974,
      "step": 27610
    },
    {
      "epoch": 1.4414523725651363,
      "grad_norm": 5.029571056365967,
      "learning_rate": 2.5983334203155718e-05,
      "loss": 0.8441,
      "step": 27620
    },
    {
      "epoch": 1.4419742455673412,
      "grad_norm": 4.57822847366333,
      "learning_rate": 2.597463597933302e-05,
      "loss": 0.9368,
      "step": 27630
    },
    {
      "epoch": 1.4424961185695462,
      "grad_norm": 4.07247257232666,
      "learning_rate": 2.596593775551033e-05,
      "loss": 0.8228,
      "step": 27640
    },
    {
      "epoch": 1.4430179915717511,
      "grad_norm": 5.3632612228393555,
      "learning_rate": 2.5957239531687632e-05,
      "loss": 0.9491,
      "step": 27650
    },
    {
      "epoch": 1.4435398645739559,
      "grad_norm": 4.740351676940918,
      "learning_rate": 2.594854130786494e-05,
      "loss": 0.9613,
      "step": 27660
    },
    {
      "epoch": 1.4440617375761609,
      "grad_norm": 4.019773960113525,
      "learning_rate": 2.5939843084042243e-05,
      "loss": 0.8949,
      "step": 27670
    },
    {
      "epoch": 1.4445836105783658,
      "grad_norm": 4.408958911895752,
      "learning_rate": 2.5931144860219543e-05,
      "loss": 0.8587,
      "step": 27680
    },
    {
      "epoch": 1.4451054835805706,
      "grad_norm": 5.339865684509277,
      "learning_rate": 2.5922446636396846e-05,
      "loss": 0.9414,
      "step": 27690
    },
    {
      "epoch": 1.4456273565827755,
      "grad_norm": 4.529674053192139,
      "learning_rate": 2.591374841257415e-05,
      "loss": 0.8459,
      "step": 27700
    },
    {
      "epoch": 1.4461492295849805,
      "grad_norm": 3.846524238586426,
      "learning_rate": 2.5905050188751457e-05,
      "loss": 0.8655,
      "step": 27710
    },
    {
      "epoch": 1.4466711025871855,
      "grad_norm": 5.0164265632629395,
      "learning_rate": 2.589635196492876e-05,
      "loss": 0.9413,
      "step": 27720
    },
    {
      "epoch": 1.4471929755893904,
      "grad_norm": 4.298518657684326,
      "learning_rate": 2.5887653741106067e-05,
      "loss": 0.9435,
      "step": 27730
    },
    {
      "epoch": 1.4477148485915952,
      "grad_norm": 4.621660232543945,
      "learning_rate": 2.587895551728337e-05,
      "loss": 0.8941,
      "step": 27740
    },
    {
      "epoch": 1.4482367215938001,
      "grad_norm": 4.005955219268799,
      "learning_rate": 2.5870257293460674e-05,
      "loss": 0.8941,
      "step": 27750
    },
    {
      "epoch": 1.448758594596005,
      "grad_norm": 4.150588512420654,
      "learning_rate": 2.586155906963798e-05,
      "loss": 0.8577,
      "step": 27760
    },
    {
      "epoch": 1.44928046759821,
      "grad_norm": 5.084754943847656,
      "learning_rate": 2.5852860845815285e-05,
      "loss": 0.8839,
      "step": 27770
    },
    {
      "epoch": 1.449802340600415,
      "grad_norm": 4.416642665863037,
      "learning_rate": 2.5844162621992592e-05,
      "loss": 0.8915,
      "step": 27780
    },
    {
      "epoch": 1.4503242136026198,
      "grad_norm": 4.260943412780762,
      "learning_rate": 2.5835464398169895e-05,
      "loss": 0.9639,
      "step": 27790
    },
    {
      "epoch": 1.4508460866048247,
      "grad_norm": 4.34232759475708,
      "learning_rate": 2.58267661743472e-05,
      "loss": 0.8175,
      "step": 27800
    },
    {
      "epoch": 1.4513679596070297,
      "grad_norm": 4.343894004821777,
      "learning_rate": 2.5818067950524506e-05,
      "loss": 0.9887,
      "step": 27810
    },
    {
      "epoch": 1.4518898326092344,
      "grad_norm": 3.798435926437378,
      "learning_rate": 2.580936972670181e-05,
      "loss": 0.8512,
      "step": 27820
    },
    {
      "epoch": 1.4524117056114394,
      "grad_norm": 4.5706892013549805,
      "learning_rate": 2.5800671502879113e-05,
      "loss": 0.8159,
      "step": 27830
    },
    {
      "epoch": 1.4529335786136444,
      "grad_norm": 4.3677659034729,
      "learning_rate": 2.579197327905642e-05,
      "loss": 0.8903,
      "step": 27840
    },
    {
      "epoch": 1.4534554516158493,
      "grad_norm": 4.8328776359558105,
      "learning_rate": 2.5783275055233723e-05,
      "loss": 0.9403,
      "step": 27850
    },
    {
      "epoch": 1.4539773246180543,
      "grad_norm": 3.721590757369995,
      "learning_rate": 2.577457683141103e-05,
      "loss": 0.8793,
      "step": 27860
    },
    {
      "epoch": 1.454499197620259,
      "grad_norm": 3.467414140701294,
      "learning_rate": 2.5765878607588334e-05,
      "loss": 0.8589,
      "step": 27870
    },
    {
      "epoch": 1.455021070622464,
      "grad_norm": 4.014456272125244,
      "learning_rate": 2.5757180383765637e-05,
      "loss": 0.9402,
      "step": 27880
    },
    {
      "epoch": 1.455542943624669,
      "grad_norm": 4.297079563140869,
      "learning_rate": 2.5748482159942938e-05,
      "loss": 0.8096,
      "step": 27890
    },
    {
      "epoch": 1.456064816626874,
      "grad_norm": 4.0614824295043945,
      "learning_rate": 2.5739783936120245e-05,
      "loss": 0.8103,
      "step": 27900
    },
    {
      "epoch": 1.456586689629079,
      "grad_norm": 4.073077201843262,
      "learning_rate": 2.5731085712297548e-05,
      "loss": 0.8271,
      "step": 27910
    },
    {
      "epoch": 1.4571085626312836,
      "grad_norm": 3.8398683071136475,
      "learning_rate": 2.572238748847485e-05,
      "loss": 0.8869,
      "step": 27920
    },
    {
      "epoch": 1.4576304356334886,
      "grad_norm": 4.343626499176025,
      "learning_rate": 2.571368926465216e-05,
      "loss": 0.9081,
      "step": 27930
    },
    {
      "epoch": 1.4581523086356936,
      "grad_norm": 5.041576862335205,
      "learning_rate": 2.5704991040829462e-05,
      "loss": 0.9046,
      "step": 27940
    },
    {
      "epoch": 1.4586741816378983,
      "grad_norm": 5.051131725311279,
      "learning_rate": 2.5696292817006766e-05,
      "loss": 0.8784,
      "step": 27950
    },
    {
      "epoch": 1.4591960546401033,
      "grad_norm": 4.685862064361572,
      "learning_rate": 2.5687594593184073e-05,
      "loss": 0.8669,
      "step": 27960
    },
    {
      "epoch": 1.4597179276423082,
      "grad_norm": 4.87841796875,
      "learning_rate": 2.5678896369361376e-05,
      "loss": 0.9403,
      "step": 27970
    },
    {
      "epoch": 1.4602398006445132,
      "grad_norm": 4.652006149291992,
      "learning_rate": 2.5670198145538683e-05,
      "loss": 0.9385,
      "step": 27980
    },
    {
      "epoch": 1.4607616736467182,
      "grad_norm": 4.588211536407471,
      "learning_rate": 2.5661499921715987e-05,
      "loss": 0.9227,
      "step": 27990
    },
    {
      "epoch": 1.461283546648923,
      "grad_norm": 3.965273380279541,
      "learning_rate": 2.565280169789329e-05,
      "loss": 0.938,
      "step": 28000
    },
    {
      "epoch": 1.4618054196511279,
      "grad_norm": 4.373464107513428,
      "learning_rate": 2.5644103474070597e-05,
      "loss": 0.8235,
      "step": 28010
    },
    {
      "epoch": 1.4623272926533328,
      "grad_norm": 4.799328804016113,
      "learning_rate": 2.56354052502479e-05,
      "loss": 0.8281,
      "step": 28020
    },
    {
      "epoch": 1.4628491656555378,
      "grad_norm": 4.482022285461426,
      "learning_rate": 2.5626707026425208e-05,
      "loss": 0.9393,
      "step": 28030
    },
    {
      "epoch": 1.4633710386577428,
      "grad_norm": 4.491878509521484,
      "learning_rate": 2.561800880260251e-05,
      "loss": 0.9205,
      "step": 28040
    },
    {
      "epoch": 1.4638929116599475,
      "grad_norm": 4.295896053314209,
      "learning_rate": 2.5609310578779815e-05,
      "loss": 0.8486,
      "step": 28050
    },
    {
      "epoch": 1.4644147846621525,
      "grad_norm": 4.964907169342041,
      "learning_rate": 2.560061235495712e-05,
      "loss": 0.9416,
      "step": 28060
    },
    {
      "epoch": 1.4649366576643574,
      "grad_norm": 4.867657661437988,
      "learning_rate": 2.5591914131134425e-05,
      "loss": 0.9362,
      "step": 28070
    },
    {
      "epoch": 1.4654585306665622,
      "grad_norm": 5.2082014083862305,
      "learning_rate": 2.5583215907311732e-05,
      "loss": 0.9289,
      "step": 28080
    },
    {
      "epoch": 1.4659804036687671,
      "grad_norm": 3.5961241722106934,
      "learning_rate": 2.5574517683489036e-05,
      "loss": 0.947,
      "step": 28090
    },
    {
      "epoch": 1.466502276670972,
      "grad_norm": 4.633357048034668,
      "learning_rate": 2.5565819459666336e-05,
      "loss": 0.9348,
      "step": 28100
    },
    {
      "epoch": 1.467024149673177,
      "grad_norm": 3.4615252017974854,
      "learning_rate": 2.555712123584364e-05,
      "loss": 0.8026,
      "step": 28110
    },
    {
      "epoch": 1.467546022675382,
      "grad_norm": 4.268000602722168,
      "learning_rate": 2.5548423012020943e-05,
      "loss": 0.8927,
      "step": 28120
    },
    {
      "epoch": 1.4680678956775868,
      "grad_norm": 3.994067668914795,
      "learning_rate": 2.553972478819825e-05,
      "loss": 0.8317,
      "step": 28130
    },
    {
      "epoch": 1.4685897686797917,
      "grad_norm": 4.276475429534912,
      "learning_rate": 2.5531026564375553e-05,
      "loss": 0.8219,
      "step": 28140
    },
    {
      "epoch": 1.4691116416819967,
      "grad_norm": 4.538402080535889,
      "learning_rate": 2.552232834055286e-05,
      "loss": 0.9235,
      "step": 28150
    },
    {
      "epoch": 1.4696335146842017,
      "grad_norm": 4.55469274520874,
      "learning_rate": 2.5513630116730164e-05,
      "loss": 0.8967,
      "step": 28160
    },
    {
      "epoch": 1.4701553876864066,
      "grad_norm": 4.237813472747803,
      "learning_rate": 2.5504931892907468e-05,
      "loss": 0.8821,
      "step": 28170
    },
    {
      "epoch": 1.4706772606886114,
      "grad_norm": 5.392696857452393,
      "learning_rate": 2.5496233669084774e-05,
      "loss": 0.9104,
      "step": 28180
    },
    {
      "epoch": 1.4711991336908163,
      "grad_norm": 4.0172014236450195,
      "learning_rate": 2.5487535445262078e-05,
      "loss": 0.877,
      "step": 28190
    },
    {
      "epoch": 1.4717210066930213,
      "grad_norm": 4.614392280578613,
      "learning_rate": 2.5478837221439385e-05,
      "loss": 0.9623,
      "step": 28200
    },
    {
      "epoch": 1.472242879695226,
      "grad_norm": 4.459718704223633,
      "learning_rate": 2.547013899761669e-05,
      "loss": 0.894,
      "step": 28210
    },
    {
      "epoch": 1.472764752697431,
      "grad_norm": 4.705679893493652,
      "learning_rate": 2.5461440773793992e-05,
      "loss": 0.8622,
      "step": 28220
    },
    {
      "epoch": 1.473286625699636,
      "grad_norm": 4.658029079437256,
      "learning_rate": 2.54527425499713e-05,
      "loss": 0.9121,
      "step": 28230
    },
    {
      "epoch": 1.473808498701841,
      "grad_norm": 4.395186424255371,
      "learning_rate": 2.5444044326148603e-05,
      "loss": 0.8671,
      "step": 28240
    },
    {
      "epoch": 1.474330371704046,
      "grad_norm": 4.005694389343262,
      "learning_rate": 2.5435346102325906e-05,
      "loss": 0.9467,
      "step": 28250
    },
    {
      "epoch": 1.4748522447062506,
      "grad_norm": 4.250456809997559,
      "learning_rate": 2.5426647878503213e-05,
      "loss": 0.834,
      "step": 28260
    },
    {
      "epoch": 1.4753741177084556,
      "grad_norm": 3.8858282566070557,
      "learning_rate": 2.5417949654680517e-05,
      "loss": 0.9311,
      "step": 28270
    },
    {
      "epoch": 1.4758959907106606,
      "grad_norm": 3.676831007003784,
      "learning_rate": 2.5409251430857823e-05,
      "loss": 0.9101,
      "step": 28280
    },
    {
      "epoch": 1.4764178637128655,
      "grad_norm": 4.861669540405273,
      "learning_rate": 2.5400553207035127e-05,
      "loss": 1.0018,
      "step": 28290
    },
    {
      "epoch": 1.4769397367150705,
      "grad_norm": 4.293959617614746,
      "learning_rate": 2.539185498321243e-05,
      "loss": 0.8596,
      "step": 28300
    },
    {
      "epoch": 1.4774616097172752,
      "grad_norm": 3.8493492603302,
      "learning_rate": 2.538315675938973e-05,
      "loss": 0.8817,
      "step": 28310
    },
    {
      "epoch": 1.4779834827194802,
      "grad_norm": 5.033198356628418,
      "learning_rate": 2.5374458535567038e-05,
      "loss": 0.8622,
      "step": 28320
    },
    {
      "epoch": 1.4785053557216852,
      "grad_norm": 4.216207504272461,
      "learning_rate": 2.536576031174434e-05,
      "loss": 0.8706,
      "step": 28330
    },
    {
      "epoch": 1.47902722872389,
      "grad_norm": 4.7153167724609375,
      "learning_rate": 2.5357062087921645e-05,
      "loss": 0.9006,
      "step": 28340
    },
    {
      "epoch": 1.4795491017260949,
      "grad_norm": 3.488003969192505,
      "learning_rate": 2.5348363864098952e-05,
      "loss": 0.9223,
      "step": 28350
    },
    {
      "epoch": 1.4800709747282998,
      "grad_norm": 4.939211368560791,
      "learning_rate": 2.5339665640276255e-05,
      "loss": 0.8946,
      "step": 28360
    },
    {
      "epoch": 1.4805928477305048,
      "grad_norm": 4.749478816986084,
      "learning_rate": 2.533096741645356e-05,
      "loss": 0.9228,
      "step": 28370
    },
    {
      "epoch": 1.4811147207327098,
      "grad_norm": 4.6758131980896,
      "learning_rate": 2.5322269192630866e-05,
      "loss": 0.8791,
      "step": 28380
    },
    {
      "epoch": 1.4816365937349145,
      "grad_norm": 3.6872782707214355,
      "learning_rate": 2.531357096880817e-05,
      "loss": 0.8939,
      "step": 28390
    },
    {
      "epoch": 1.4821584667371195,
      "grad_norm": 4.204070091247559,
      "learning_rate": 2.5304872744985476e-05,
      "loss": 0.9257,
      "step": 28400
    },
    {
      "epoch": 1.4826803397393244,
      "grad_norm": 4.284514427185059,
      "learning_rate": 2.529617452116278e-05,
      "loss": 0.9371,
      "step": 28410
    },
    {
      "epoch": 1.4832022127415294,
      "grad_norm": 3.778132438659668,
      "learning_rate": 2.5287476297340083e-05,
      "loss": 0.8639,
      "step": 28420
    },
    {
      "epoch": 1.4837240857437344,
      "grad_norm": 4.425532341003418,
      "learning_rate": 2.527877807351739e-05,
      "loss": 0.8721,
      "step": 28430
    },
    {
      "epoch": 1.4842459587459391,
      "grad_norm": 4.330901145935059,
      "learning_rate": 2.5270079849694694e-05,
      "loss": 0.8244,
      "step": 28440
    },
    {
      "epoch": 1.484767831748144,
      "grad_norm": 5.435919761657715,
      "learning_rate": 2.5261381625872e-05,
      "loss": 0.9269,
      "step": 28450
    },
    {
      "epoch": 1.485289704750349,
      "grad_norm": 4.998034477233887,
      "learning_rate": 2.5252683402049304e-05,
      "loss": 0.9303,
      "step": 28460
    },
    {
      "epoch": 1.4858115777525538,
      "grad_norm": 3.8577136993408203,
      "learning_rate": 2.5243985178226608e-05,
      "loss": 0.8334,
      "step": 28470
    },
    {
      "epoch": 1.4863334507547588,
      "grad_norm": 4.537549018859863,
      "learning_rate": 2.5235286954403915e-05,
      "loss": 0.88,
      "step": 28480
    },
    {
      "epoch": 1.4868553237569637,
      "grad_norm": 4.068451881408691,
      "learning_rate": 2.522658873058122e-05,
      "loss": 0.9104,
      "step": 28490
    },
    {
      "epoch": 1.4873771967591687,
      "grad_norm": 3.6976468563079834,
      "learning_rate": 2.5217890506758525e-05,
      "loss": 0.9023,
      "step": 28500
    },
    {
      "epoch": 1.4878990697613736,
      "grad_norm": 4.06358528137207,
      "learning_rate": 2.520919228293583e-05,
      "loss": 0.8963,
      "step": 28510
    },
    {
      "epoch": 1.4884209427635784,
      "grad_norm": 4.351291179656982,
      "learning_rate": 2.520049405911313e-05,
      "loss": 0.9418,
      "step": 28520
    },
    {
      "epoch": 1.4889428157657834,
      "grad_norm": 3.1447880268096924,
      "learning_rate": 2.5191795835290433e-05,
      "loss": 0.7647,
      "step": 28530
    },
    {
      "epoch": 1.4894646887679883,
      "grad_norm": 4.8618621826171875,
      "learning_rate": 2.5183097611467736e-05,
      "loss": 0.8703,
      "step": 28540
    },
    {
      "epoch": 1.4899865617701933,
      "grad_norm": 4.771792888641357,
      "learning_rate": 2.5174399387645043e-05,
      "loss": 0.8715,
      "step": 28550
    },
    {
      "epoch": 1.4905084347723982,
      "grad_norm": 4.8834452629089355,
      "learning_rate": 2.5165701163822347e-05,
      "loss": 0.8,
      "step": 28560
    },
    {
      "epoch": 1.491030307774603,
      "grad_norm": 4.6381988525390625,
      "learning_rate": 2.5157002939999654e-05,
      "loss": 0.9372,
      "step": 28570
    },
    {
      "epoch": 1.491552180776808,
      "grad_norm": 5.165176868438721,
      "learning_rate": 2.5148304716176957e-05,
      "loss": 0.8524,
      "step": 28580
    },
    {
      "epoch": 1.492074053779013,
      "grad_norm": 3.9766621589660645,
      "learning_rate": 2.513960649235426e-05,
      "loss": 0.8654,
      "step": 28590
    },
    {
      "epoch": 1.4925959267812177,
      "grad_norm": 4.4084367752075195,
      "learning_rate": 2.5130908268531568e-05,
      "loss": 0.9213,
      "step": 28600
    },
    {
      "epoch": 1.4931177997834226,
      "grad_norm": 3.371981143951416,
      "learning_rate": 2.512221004470887e-05,
      "loss": 0.8564,
      "step": 28610
    },
    {
      "epoch": 1.4936396727856276,
      "grad_norm": 4.371697902679443,
      "learning_rate": 2.5113511820886178e-05,
      "loss": 0.8021,
      "step": 28620
    },
    {
      "epoch": 1.4941615457878326,
      "grad_norm": 4.3284077644348145,
      "learning_rate": 2.510481359706348e-05,
      "loss": 0.9082,
      "step": 28630
    },
    {
      "epoch": 1.4946834187900375,
      "grad_norm": 4.081945419311523,
      "learning_rate": 2.5096115373240785e-05,
      "loss": 0.866,
      "step": 28640
    },
    {
      "epoch": 1.4952052917922423,
      "grad_norm": 4.450414180755615,
      "learning_rate": 2.5087417149418092e-05,
      "loss": 0.9059,
      "step": 28650
    },
    {
      "epoch": 1.4957271647944472,
      "grad_norm": 4.255247592926025,
      "learning_rate": 2.5078718925595396e-05,
      "loss": 0.8423,
      "step": 28660
    },
    {
      "epoch": 1.4962490377966522,
      "grad_norm": 3.8535172939300537,
      "learning_rate": 2.50700207017727e-05,
      "loss": 0.8534,
      "step": 28670
    },
    {
      "epoch": 1.4967709107988572,
      "grad_norm": 4.3181257247924805,
      "learning_rate": 2.5061322477950006e-05,
      "loss": 0.8907,
      "step": 28680
    },
    {
      "epoch": 1.4972927838010621,
      "grad_norm": 4.63736629486084,
      "learning_rate": 2.505262425412731e-05,
      "loss": 0.9287,
      "step": 28690
    },
    {
      "epoch": 1.4978146568032669,
      "grad_norm": 4.1980977058410645,
      "learning_rate": 2.5043926030304617e-05,
      "loss": 0.9352,
      "step": 28700
    },
    {
      "epoch": 1.4983365298054718,
      "grad_norm": 4.2132439613342285,
      "learning_rate": 2.503522780648192e-05,
      "loss": 0.9716,
      "step": 28710
    },
    {
      "epoch": 1.4988584028076768,
      "grad_norm": 4.677734851837158,
      "learning_rate": 2.502652958265922e-05,
      "loss": 0.897,
      "step": 28720
    },
    {
      "epoch": 1.4993802758098815,
      "grad_norm": 5.240469455718994,
      "learning_rate": 2.5017831358836524e-05,
      "loss": 1.0094,
      "step": 28730
    },
    {
      "epoch": 1.4999021488120865,
      "grad_norm": 4.56255578994751,
      "learning_rate": 2.500913313501383e-05,
      "loss": 0.9521,
      "step": 28740
    },
    {
      "epoch": 1.5004240218142915,
      "grad_norm": 4.1481523513793945,
      "learning_rate": 2.5000434911191134e-05,
      "loss": 0.9377,
      "step": 28750
    },
    {
      "epoch": 1.5009458948164964,
      "grad_norm": 3.770047187805176,
      "learning_rate": 2.499173668736844e-05,
      "loss": 0.9052,
      "step": 28760
    },
    {
      "epoch": 1.5014677678187014,
      "grad_norm": 4.389443397521973,
      "learning_rate": 2.4983038463545748e-05,
      "loss": 0.8775,
      "step": 28770
    },
    {
      "epoch": 1.5019896408209061,
      "grad_norm": 5.08860969543457,
      "learning_rate": 2.497434023972305e-05,
      "loss": 0.9391,
      "step": 28780
    },
    {
      "epoch": 1.502511513823111,
      "grad_norm": 4.317767143249512,
      "learning_rate": 2.4965642015900352e-05,
      "loss": 0.8896,
      "step": 28790
    },
    {
      "epoch": 1.503033386825316,
      "grad_norm": 4.6966071128845215,
      "learning_rate": 2.495694379207766e-05,
      "loss": 0.8479,
      "step": 28800
    },
    {
      "epoch": 1.5035552598275208,
      "grad_norm": 4.206530570983887,
      "learning_rate": 2.4948245568254962e-05,
      "loss": 0.8815,
      "step": 28810
    },
    {
      "epoch": 1.504077132829726,
      "grad_norm": 4.34868049621582,
      "learning_rate": 2.493954734443227e-05,
      "loss": 0.8642,
      "step": 28820
    },
    {
      "epoch": 1.5045990058319307,
      "grad_norm": 4.901916027069092,
      "learning_rate": 2.4930849120609573e-05,
      "loss": 0.9532,
      "step": 28830
    },
    {
      "epoch": 1.5051208788341357,
      "grad_norm": 4.312140464782715,
      "learning_rate": 2.4922150896786876e-05,
      "loss": 0.8197,
      "step": 28840
    },
    {
      "epoch": 1.5056427518363407,
      "grad_norm": 3.9232304096221924,
      "learning_rate": 2.4913452672964183e-05,
      "loss": 0.7675,
      "step": 28850
    },
    {
      "epoch": 1.5061646248385454,
      "grad_norm": 4.717877388000488,
      "learning_rate": 2.4904754449141487e-05,
      "loss": 0.8801,
      "step": 28860
    },
    {
      "epoch": 1.5066864978407506,
      "grad_norm": 4.348151683807373,
      "learning_rate": 2.4896056225318794e-05,
      "loss": 0.9401,
      "step": 28870
    },
    {
      "epoch": 1.5072083708429553,
      "grad_norm": 4.495950698852539,
      "learning_rate": 2.4887358001496094e-05,
      "loss": 0.8704,
      "step": 28880
    },
    {
      "epoch": 1.5077302438451603,
      "grad_norm": 4.11843729019165,
      "learning_rate": 2.48786597776734e-05,
      "loss": 0.8299,
      "step": 28890
    },
    {
      "epoch": 1.5082521168473653,
      "grad_norm": 4.4993109703063965,
      "learning_rate": 2.4869961553850705e-05,
      "loss": 0.8948,
      "step": 28900
    },
    {
      "epoch": 1.50877398984957,
      "grad_norm": 5.085200309753418,
      "learning_rate": 2.4861263330028008e-05,
      "loss": 0.9235,
      "step": 28910
    },
    {
      "epoch": 1.509295862851775,
      "grad_norm": 3.140223741531372,
      "learning_rate": 2.4852565106205315e-05,
      "loss": 0.8384,
      "step": 28920
    },
    {
      "epoch": 1.50981773585398,
      "grad_norm": 3.7348945140838623,
      "learning_rate": 2.484386688238262e-05,
      "loss": 0.8792,
      "step": 28930
    },
    {
      "epoch": 1.510339608856185,
      "grad_norm": 3.5853168964385986,
      "learning_rate": 2.4835168658559922e-05,
      "loss": 0.8776,
      "step": 28940
    },
    {
      "epoch": 1.5108614818583899,
      "grad_norm": 4.311371326446533,
      "learning_rate": 2.482647043473723e-05,
      "loss": 0.8612,
      "step": 28950
    },
    {
      "epoch": 1.5113833548605946,
      "grad_norm": 5.403906345367432,
      "learning_rate": 2.4817772210914533e-05,
      "loss": 0.9553,
      "step": 28960
    },
    {
      "epoch": 1.5119052278627996,
      "grad_norm": 3.8856308460235596,
      "learning_rate": 2.480907398709184e-05,
      "loss": 0.927,
      "step": 28970
    },
    {
      "epoch": 1.5124271008650045,
      "grad_norm": 4.080332279205322,
      "learning_rate": 2.480037576326914e-05,
      "loss": 0.8621,
      "step": 28980
    },
    {
      "epoch": 1.5129489738672093,
      "grad_norm": 4.360219955444336,
      "learning_rate": 2.4791677539446447e-05,
      "loss": 0.8751,
      "step": 28990
    },
    {
      "epoch": 1.5134708468694145,
      "grad_norm": 4.9402289390563965,
      "learning_rate": 2.478297931562375e-05,
      "loss": 0.8983,
      "step": 29000
    },
    {
      "epoch": 1.5139927198716192,
      "grad_norm": 3.934001922607422,
      "learning_rate": 2.4774281091801054e-05,
      "loss": 0.8364,
      "step": 29010
    },
    {
      "epoch": 1.5145145928738242,
      "grad_norm": 4.497485160827637,
      "learning_rate": 2.476558286797836e-05,
      "loss": 0.8837,
      "step": 29020
    },
    {
      "epoch": 1.5150364658760291,
      "grad_norm": 5.338430881500244,
      "learning_rate": 2.4756884644155664e-05,
      "loss": 0.8143,
      "step": 29030
    },
    {
      "epoch": 1.5155583388782339,
      "grad_norm": 4.906882286071777,
      "learning_rate": 2.4748186420332968e-05,
      "loss": 0.9,
      "step": 29040
    },
    {
      "epoch": 1.5160802118804388,
      "grad_norm": 4.707489013671875,
      "learning_rate": 2.4739488196510275e-05,
      "loss": 0.8606,
      "step": 29050
    },
    {
      "epoch": 1.5166020848826438,
      "grad_norm": 4.120673656463623,
      "learning_rate": 2.4730789972687578e-05,
      "loss": 0.8868,
      "step": 29060
    },
    {
      "epoch": 1.5171239578848488,
      "grad_norm": 4.464841842651367,
      "learning_rate": 2.4722091748864885e-05,
      "loss": 0.82,
      "step": 29070
    },
    {
      "epoch": 1.5176458308870537,
      "grad_norm": 3.735649824142456,
      "learning_rate": 2.471339352504219e-05,
      "loss": 0.8251,
      "step": 29080
    },
    {
      "epoch": 1.5181677038892585,
      "grad_norm": 6.258118629455566,
      "learning_rate": 2.4704695301219492e-05,
      "loss": 0.8757,
      "step": 29090
    },
    {
      "epoch": 1.5186895768914634,
      "grad_norm": 4.865391731262207,
      "learning_rate": 2.4695997077396796e-05,
      "loss": 0.8504,
      "step": 29100
    },
    {
      "epoch": 1.5192114498936684,
      "grad_norm": 4.2346367835998535,
      "learning_rate": 2.46872988535741e-05,
      "loss": 0.8415,
      "step": 29110
    },
    {
      "epoch": 1.5197333228958732,
      "grad_norm": 4.349894046783447,
      "learning_rate": 2.4678600629751406e-05,
      "loss": 0.9098,
      "step": 29120
    },
    {
      "epoch": 1.5202551958980783,
      "grad_norm": 4.928592681884766,
      "learning_rate": 2.466990240592871e-05,
      "loss": 0.891,
      "step": 29130
    },
    {
      "epoch": 1.520777068900283,
      "grad_norm": 4.074479579925537,
      "learning_rate": 2.4661204182106017e-05,
      "loss": 0.9496,
      "step": 29140
    },
    {
      "epoch": 1.521298941902488,
      "grad_norm": 4.343429088592529,
      "learning_rate": 2.465250595828332e-05,
      "loss": 0.9315,
      "step": 29150
    },
    {
      "epoch": 1.521820814904693,
      "grad_norm": 4.023849010467529,
      "learning_rate": 2.4643807734460624e-05,
      "loss": 0.8732,
      "step": 29160
    },
    {
      "epoch": 1.5223426879068978,
      "grad_norm": 3.8609249591827393,
      "learning_rate": 2.463510951063793e-05,
      "loss": 0.8379,
      "step": 29170
    },
    {
      "epoch": 1.5228645609091027,
      "grad_norm": 4.232429504394531,
      "learning_rate": 2.4627281109197504e-05,
      "loss": 0.9093,
      "step": 29180
    },
    {
      "epoch": 1.5233864339113077,
      "grad_norm": 5.160411834716797,
      "learning_rate": 2.4618582885374808e-05,
      "loss": 0.9689,
      "step": 29190
    },
    {
      "epoch": 1.5239083069135126,
      "grad_norm": 4.351081848144531,
      "learning_rate": 2.4609884661552114e-05,
      "loss": 0.8317,
      "step": 29200
    },
    {
      "epoch": 1.5244301799157176,
      "grad_norm": 5.5905070304870605,
      "learning_rate": 2.4601186437729418e-05,
      "loss": 0.9354,
      "step": 29210
    },
    {
      "epoch": 1.5249520529179224,
      "grad_norm": 4.5919952392578125,
      "learning_rate": 2.459248821390672e-05,
      "loss": 0.868,
      "step": 29220
    },
    {
      "epoch": 1.5254739259201273,
      "grad_norm": 4.679220676422119,
      "learning_rate": 2.458378999008403e-05,
      "loss": 0.9726,
      "step": 29230
    },
    {
      "epoch": 1.5259957989223323,
      "grad_norm": 4.7472310066223145,
      "learning_rate": 2.457509176626133e-05,
      "loss": 0.904,
      "step": 29240
    },
    {
      "epoch": 1.526517671924537,
      "grad_norm": 4.5780110359191895,
      "learning_rate": 2.4566393542438636e-05,
      "loss": 0.8861,
      "step": 29250
    },
    {
      "epoch": 1.5270395449267422,
      "grad_norm": 4.115054130554199,
      "learning_rate": 2.455769531861594e-05,
      "loss": 0.8826,
      "step": 29260
    },
    {
      "epoch": 1.527561417928947,
      "grad_norm": 4.170389175415039,
      "learning_rate": 2.4548997094793243e-05,
      "loss": 0.8155,
      "step": 29270
    },
    {
      "epoch": 1.528083290931152,
      "grad_norm": 4.6286234855651855,
      "learning_rate": 2.454029887097055e-05,
      "loss": 0.9106,
      "step": 29280
    },
    {
      "epoch": 1.5286051639333569,
      "grad_norm": 4.533905982971191,
      "learning_rate": 2.4531600647147853e-05,
      "loss": 0.9703,
      "step": 29290
    },
    {
      "epoch": 1.5291270369355616,
      "grad_norm": 4.4740142822265625,
      "learning_rate": 2.452290242332516e-05,
      "loss": 0.8119,
      "step": 29300
    },
    {
      "epoch": 1.5296489099377668,
      "grad_norm": 4.53373384475708,
      "learning_rate": 2.4514204199502464e-05,
      "loss": 0.9621,
      "step": 29310
    },
    {
      "epoch": 1.5301707829399716,
      "grad_norm": 4.40983772277832,
      "learning_rate": 2.4505505975679767e-05,
      "loss": 0.9348,
      "step": 29320
    },
    {
      "epoch": 1.5306926559421765,
      "grad_norm": 4.124830722808838,
      "learning_rate": 2.4496807751857074e-05,
      "loss": 0.8813,
      "step": 29330
    },
    {
      "epoch": 1.5312145289443815,
      "grad_norm": 4.518355846405029,
      "learning_rate": 2.4488109528034374e-05,
      "loss": 0.8783,
      "step": 29340
    },
    {
      "epoch": 1.5317364019465862,
      "grad_norm": 4.30764627456665,
      "learning_rate": 2.447941130421168e-05,
      "loss": 0.8663,
      "step": 29350
    },
    {
      "epoch": 1.5322582749487912,
      "grad_norm": 5.091348648071289,
      "learning_rate": 2.4470713080388985e-05,
      "loss": 0.8478,
      "step": 29360
    },
    {
      "epoch": 1.5327801479509962,
      "grad_norm": 4.740347862243652,
      "learning_rate": 2.446201485656629e-05,
      "loss": 0.8787,
      "step": 29370
    },
    {
      "epoch": 1.533302020953201,
      "grad_norm": 4.682522296905518,
      "learning_rate": 2.4453316632743595e-05,
      "loss": 0.9435,
      "step": 29380
    },
    {
      "epoch": 1.533823893955406,
      "grad_norm": 4.166355133056641,
      "learning_rate": 2.44446184089209e-05,
      "loss": 0.828,
      "step": 29390
    },
    {
      "epoch": 1.5343457669576108,
      "grad_norm": 3.692828893661499,
      "learning_rate": 2.4435920185098206e-05,
      "loss": 0.8851,
      "step": 29400
    },
    {
      "epoch": 1.5348676399598158,
      "grad_norm": 4.477396488189697,
      "learning_rate": 2.442722196127551e-05,
      "loss": 0.9466,
      "step": 29410
    },
    {
      "epoch": 1.5353895129620208,
      "grad_norm": 4.35376501083374,
      "learning_rate": 2.4418523737452813e-05,
      "loss": 0.9311,
      "step": 29420
    },
    {
      "epoch": 1.5359113859642255,
      "grad_norm": 5.745685577392578,
      "learning_rate": 2.440982551363012e-05,
      "loss": 0.9357,
      "step": 29430
    },
    {
      "epoch": 1.5364332589664307,
      "grad_norm": 4.540332317352295,
      "learning_rate": 2.440112728980742e-05,
      "loss": 0.8002,
      "step": 29440
    },
    {
      "epoch": 1.5369551319686354,
      "grad_norm": 4.534502983093262,
      "learning_rate": 2.4392429065984727e-05,
      "loss": 0.9306,
      "step": 29450
    },
    {
      "epoch": 1.5374770049708404,
      "grad_norm": 5.1136064529418945,
      "learning_rate": 2.438373084216203e-05,
      "loss": 0.8735,
      "step": 29460
    },
    {
      "epoch": 1.5379988779730454,
      "grad_norm": 4.090729713439941,
      "learning_rate": 2.4375032618339334e-05,
      "loss": 0.9148,
      "step": 29470
    },
    {
      "epoch": 1.53852075097525,
      "grad_norm": 4.159240245819092,
      "learning_rate": 2.436633439451664e-05,
      "loss": 0.9378,
      "step": 29480
    },
    {
      "epoch": 1.539042623977455,
      "grad_norm": 4.95281982421875,
      "learning_rate": 2.4357636170693945e-05,
      "loss": 0.9198,
      "step": 29490
    },
    {
      "epoch": 1.53956449697966,
      "grad_norm": 4.12916374206543,
      "learning_rate": 2.434893794687125e-05,
      "loss": 0.8367,
      "step": 29500
    },
    {
      "epoch": 1.5400863699818648,
      "grad_norm": 5.17556619644165,
      "learning_rate": 2.4340239723048555e-05,
      "loss": 0.9111,
      "step": 29510
    },
    {
      "epoch": 1.54060824298407,
      "grad_norm": 4.422023296356201,
      "learning_rate": 2.433154149922586e-05,
      "loss": 0.867,
      "step": 29520
    },
    {
      "epoch": 1.5411301159862747,
      "grad_norm": 4.152021884918213,
      "learning_rate": 2.4322843275403165e-05,
      "loss": 0.9219,
      "step": 29530
    },
    {
      "epoch": 1.5416519889884797,
      "grad_norm": 4.551474094390869,
      "learning_rate": 2.431414505158047e-05,
      "loss": 0.8912,
      "step": 29540
    },
    {
      "epoch": 1.5421738619906846,
      "grad_norm": 4.599895477294922,
      "learning_rate": 2.4305446827757773e-05,
      "loss": 0.9796,
      "step": 29550
    },
    {
      "epoch": 1.5426957349928894,
      "grad_norm": 4.202670097351074,
      "learning_rate": 2.4296748603935076e-05,
      "loss": 0.854,
      "step": 29560
    },
    {
      "epoch": 1.5432176079950946,
      "grad_norm": 3.9501054286956787,
      "learning_rate": 2.4288050380112383e-05,
      "loss": 0.8748,
      "step": 29570
    },
    {
      "epoch": 1.5437394809972993,
      "grad_norm": 4.384153366088867,
      "learning_rate": 2.4279352156289687e-05,
      "loss": 0.9164,
      "step": 29580
    },
    {
      "epoch": 1.5442613539995043,
      "grad_norm": 4.0738205909729,
      "learning_rate": 2.427065393246699e-05,
      "loss": 0.8795,
      "step": 29590
    },
    {
      "epoch": 1.5447832270017092,
      "grad_norm": 4.460211753845215,
      "learning_rate": 2.4261955708644297e-05,
      "loss": 0.7788,
      "step": 29600
    },
    {
      "epoch": 1.545305100003914,
      "grad_norm": 4.124172210693359,
      "learning_rate": 2.42532574848216e-05,
      "loss": 0.8818,
      "step": 29610
    },
    {
      "epoch": 1.545826973006119,
      "grad_norm": 4.592668056488037,
      "learning_rate": 2.4244559260998904e-05,
      "loss": 0.8262,
      "step": 29620
    },
    {
      "epoch": 1.546348846008324,
      "grad_norm": 5.378935813903809,
      "learning_rate": 2.423586103717621e-05,
      "loss": 0.938,
      "step": 29630
    },
    {
      "epoch": 1.5468707190105286,
      "grad_norm": 4.634387969970703,
      "learning_rate": 2.4227162813353515e-05,
      "loss": 0.8208,
      "step": 29640
    },
    {
      "epoch": 1.5473925920127338,
      "grad_norm": 3.433356761932373,
      "learning_rate": 2.4218464589530818e-05,
      "loss": 0.8414,
      "step": 29650
    },
    {
      "epoch": 1.5479144650149386,
      "grad_norm": 4.105254173278809,
      "learning_rate": 2.4209766365708122e-05,
      "loss": 0.8629,
      "step": 29660
    },
    {
      "epoch": 1.5484363380171435,
      "grad_norm": 4.0529584884643555,
      "learning_rate": 2.420106814188543e-05,
      "loss": 0.8517,
      "step": 29670
    },
    {
      "epoch": 1.5489582110193485,
      "grad_norm": 5.617641925811768,
      "learning_rate": 2.4192369918062732e-05,
      "loss": 0.9677,
      "step": 29680
    },
    {
      "epoch": 1.5494800840215532,
      "grad_norm": 3.888415813446045,
      "learning_rate": 2.4183671694240036e-05,
      "loss": 0.7778,
      "step": 29690
    },
    {
      "epoch": 1.5500019570237584,
      "grad_norm": 4.131734848022461,
      "learning_rate": 2.4174973470417343e-05,
      "loss": 0.9545,
      "step": 29700
    },
    {
      "epoch": 1.5505238300259632,
      "grad_norm": 3.81856369972229,
      "learning_rate": 2.4166275246594646e-05,
      "loss": 0.8122,
      "step": 29710
    },
    {
      "epoch": 1.5510457030281681,
      "grad_norm": 4.038430690765381,
      "learning_rate": 2.4157577022771953e-05,
      "loss": 0.9145,
      "step": 29720
    },
    {
      "epoch": 1.551567576030373,
      "grad_norm": 4.743447303771973,
      "learning_rate": 2.4148878798949257e-05,
      "loss": 0.904,
      "step": 29730
    },
    {
      "epoch": 1.5520894490325778,
      "grad_norm": 4.593698501586914,
      "learning_rate": 2.414018057512656e-05,
      "loss": 0.8887,
      "step": 29740
    },
    {
      "epoch": 1.5526113220347828,
      "grad_norm": 4.707101821899414,
      "learning_rate": 2.4131482351303867e-05,
      "loss": 0.8453,
      "step": 29750
    },
    {
      "epoch": 1.5531331950369878,
      "grad_norm": 4.740389347076416,
      "learning_rate": 2.4122784127481167e-05,
      "loss": 0.9269,
      "step": 29760
    },
    {
      "epoch": 1.5536550680391925,
      "grad_norm": 3.749655246734619,
      "learning_rate": 2.4114085903658474e-05,
      "loss": 0.8892,
      "step": 29770
    },
    {
      "epoch": 1.5541769410413977,
      "grad_norm": 4.521129608154297,
      "learning_rate": 2.4105387679835778e-05,
      "loss": 0.8755,
      "step": 29780
    },
    {
      "epoch": 1.5546988140436024,
      "grad_norm": 4.151467323303223,
      "learning_rate": 2.409668945601308e-05,
      "loss": 0.9277,
      "step": 29790
    },
    {
      "epoch": 1.5552206870458074,
      "grad_norm": 4.471797943115234,
      "learning_rate": 2.408799123219039e-05,
      "loss": 0.86,
      "step": 29800
    },
    {
      "epoch": 1.5557425600480124,
      "grad_norm": 3.7230725288391113,
      "learning_rate": 2.4079293008367692e-05,
      "loss": 0.9679,
      "step": 29810
    },
    {
      "epoch": 1.556264433050217,
      "grad_norm": 3.902865171432495,
      "learning_rate": 2.4070594784545e-05,
      "loss": 0.8663,
      "step": 29820
    },
    {
      "epoch": 1.5567863060524223,
      "grad_norm": 3.354915142059326,
      "learning_rate": 2.4061896560722302e-05,
      "loss": 0.8693,
      "step": 29830
    },
    {
      "epoch": 1.557308179054627,
      "grad_norm": 4.799990653991699,
      "learning_rate": 2.4053198336899606e-05,
      "loss": 0.9244,
      "step": 29840
    },
    {
      "epoch": 1.557830052056832,
      "grad_norm": 4.448543548583984,
      "learning_rate": 2.4044500113076913e-05,
      "loss": 0.9229,
      "step": 29850
    },
    {
      "epoch": 1.558351925059037,
      "grad_norm": 5.114007949829102,
      "learning_rate": 2.4035801889254213e-05,
      "loss": 0.9467,
      "step": 29860
    },
    {
      "epoch": 1.5588737980612417,
      "grad_norm": 4.4040608406066895,
      "learning_rate": 2.402710366543152e-05,
      "loss": 0.8468,
      "step": 29870
    },
    {
      "epoch": 1.5593956710634467,
      "grad_norm": 4.430417537689209,
      "learning_rate": 2.4018405441608824e-05,
      "loss": 0.8899,
      "step": 29880
    },
    {
      "epoch": 1.5599175440656516,
      "grad_norm": 5.090496063232422,
      "learning_rate": 2.4009707217786127e-05,
      "loss": 0.873,
      "step": 29890
    },
    {
      "epoch": 1.5604394170678564,
      "grad_norm": 4.08676815032959,
      "learning_rate": 2.4001008993963434e-05,
      "loss": 0.9353,
      "step": 29900
    },
    {
      "epoch": 1.5609612900700616,
      "grad_norm": 4.010806560516357,
      "learning_rate": 2.3992310770140738e-05,
      "loss": 0.8033,
      "step": 29910
    },
    {
      "epoch": 1.5614831630722663,
      "grad_norm": 4.231506824493408,
      "learning_rate": 2.3983612546318045e-05,
      "loss": 0.8452,
      "step": 29920
    },
    {
      "epoch": 1.5620050360744713,
      "grad_norm": 4.488639831542969,
      "learning_rate": 2.3974914322495348e-05,
      "loss": 0.8678,
      "step": 29930
    },
    {
      "epoch": 1.5625269090766762,
      "grad_norm": 4.151968955993652,
      "learning_rate": 2.396621609867265e-05,
      "loss": 0.9129,
      "step": 29940
    },
    {
      "epoch": 1.563048782078881,
      "grad_norm": 5.139235019683838,
      "learning_rate": 2.395751787484996e-05,
      "loss": 0.9819,
      "step": 29950
    },
    {
      "epoch": 1.5635706550810862,
      "grad_norm": 4.711560249328613,
      "learning_rate": 2.394881965102726e-05,
      "loss": 0.9105,
      "step": 29960
    },
    {
      "epoch": 1.564092528083291,
      "grad_norm": 3.5131938457489014,
      "learning_rate": 2.3940121427204566e-05,
      "loss": 0.9056,
      "step": 29970
    },
    {
      "epoch": 1.5646144010854959,
      "grad_norm": 5.279308319091797,
      "learning_rate": 2.393142320338187e-05,
      "loss": 0.9079,
      "step": 29980
    },
    {
      "epoch": 1.5651362740877008,
      "grad_norm": 4.11793851852417,
      "learning_rate": 2.3922724979559176e-05,
      "loss": 0.8649,
      "step": 29990
    },
    {
      "epoch": 1.5656581470899056,
      "grad_norm": 3.7529709339141846,
      "learning_rate": 2.391402675573648e-05,
      "loss": 0.8788,
      "step": 30000
    },
    {
      "epoch": 1.5661800200921105,
      "grad_norm": 4.885772228240967,
      "learning_rate": 2.3905328531913783e-05,
      "loss": 0.9194,
      "step": 30010
    },
    {
      "epoch": 1.5667018930943155,
      "grad_norm": 4.940096855163574,
      "learning_rate": 2.389663030809109e-05,
      "loss": 0.9077,
      "step": 30020
    },
    {
      "epoch": 1.5672237660965203,
      "grad_norm": 3.848501682281494,
      "learning_rate": 2.3887932084268394e-05,
      "loss": 0.9509,
      "step": 30030
    },
    {
      "epoch": 1.5677456390987254,
      "grad_norm": 3.9534268379211426,
      "learning_rate": 2.3879233860445697e-05,
      "loss": 0.8864,
      "step": 30040
    },
    {
      "epoch": 1.5682675121009302,
      "grad_norm": 4.122944355010986,
      "learning_rate": 2.3870535636623004e-05,
      "loss": 0.8463,
      "step": 30050
    },
    {
      "epoch": 1.5687893851031351,
      "grad_norm": 3.983039379119873,
      "learning_rate": 2.3861837412800308e-05,
      "loss": 0.9039,
      "step": 30060
    },
    {
      "epoch": 1.5693112581053401,
      "grad_norm": 4.751025676727295,
      "learning_rate": 2.385313918897761e-05,
      "loss": 0.9895,
      "step": 30070
    },
    {
      "epoch": 1.5698331311075449,
      "grad_norm": 4.576523780822754,
      "learning_rate": 2.3844440965154915e-05,
      "loss": 0.8893,
      "step": 30080
    },
    {
      "epoch": 1.57035500410975,
      "grad_norm": 4.467830657958984,
      "learning_rate": 2.3835742741332222e-05,
      "loss": 0.9087,
      "step": 30090
    },
    {
      "epoch": 1.5708768771119548,
      "grad_norm": 4.588686466217041,
      "learning_rate": 2.3827044517509525e-05,
      "loss": 0.9078,
      "step": 30100
    },
    {
      "epoch": 1.5713987501141597,
      "grad_norm": 4.457155704498291,
      "learning_rate": 2.381834629368683e-05,
      "loss": 0.8881,
      "step": 30110
    },
    {
      "epoch": 1.5719206231163647,
      "grad_norm": 4.769628524780273,
      "learning_rate": 2.3809648069864136e-05,
      "loss": 0.9569,
      "step": 30120
    },
    {
      "epoch": 1.5724424961185695,
      "grad_norm": 4.3091864585876465,
      "learning_rate": 2.380094984604144e-05,
      "loss": 0.8574,
      "step": 30130
    },
    {
      "epoch": 1.5729643691207744,
      "grad_norm": 4.662984848022461,
      "learning_rate": 2.3792251622218746e-05,
      "loss": 0.9065,
      "step": 30140
    },
    {
      "epoch": 1.5734862421229794,
      "grad_norm": 5.074677467346191,
      "learning_rate": 2.378355339839605e-05,
      "loss": 0.862,
      "step": 30150
    },
    {
      "epoch": 1.5740081151251841,
      "grad_norm": 4.2264862060546875,
      "learning_rate": 2.3774855174573353e-05,
      "loss": 0.8677,
      "step": 30160
    },
    {
      "epoch": 1.5745299881273893,
      "grad_norm": 4.0640997886657715,
      "learning_rate": 2.3766156950750657e-05,
      "loss": 0.975,
      "step": 30170
    },
    {
      "epoch": 1.575051861129594,
      "grad_norm": 4.389324188232422,
      "learning_rate": 2.375745872692796e-05,
      "loss": 0.9327,
      "step": 30180
    },
    {
      "epoch": 1.575573734131799,
      "grad_norm": 5.095334053039551,
      "learning_rate": 2.3748760503105267e-05,
      "loss": 0.9085,
      "step": 30190
    },
    {
      "epoch": 1.576095607134004,
      "grad_norm": 4.638504981994629,
      "learning_rate": 2.374006227928257e-05,
      "loss": 0.8792,
      "step": 30200
    },
    {
      "epoch": 1.5766174801362087,
      "grad_norm": 3.807204246520996,
      "learning_rate": 2.3731364055459875e-05,
      "loss": 0.876,
      "step": 30210
    },
    {
      "epoch": 1.577139353138414,
      "grad_norm": 4.334570407867432,
      "learning_rate": 2.372266583163718e-05,
      "loss": 0.9151,
      "step": 30220
    },
    {
      "epoch": 1.5776612261406187,
      "grad_norm": 4.798520565032959,
      "learning_rate": 2.3713967607814485e-05,
      "loss": 0.8585,
      "step": 30230
    },
    {
      "epoch": 1.5781830991428236,
      "grad_norm": 4.653836250305176,
      "learning_rate": 2.3705269383991792e-05,
      "loss": 0.8559,
      "step": 30240
    },
    {
      "epoch": 1.5787049721450286,
      "grad_norm": 4.703880786895752,
      "learning_rate": 2.3696571160169096e-05,
      "loss": 0.9265,
      "step": 30250
    },
    {
      "epoch": 1.5792268451472333,
      "grad_norm": 4.270035266876221,
      "learning_rate": 2.36878729363464e-05,
      "loss": 0.8937,
      "step": 30260
    },
    {
      "epoch": 1.5797487181494383,
      "grad_norm": 5.226301193237305,
      "learning_rate": 2.3679174712523706e-05,
      "loss": 0.8844,
      "step": 30270
    },
    {
      "epoch": 1.5802705911516433,
      "grad_norm": 4.4745001792907715,
      "learning_rate": 2.3670476488701006e-05,
      "loss": 0.911,
      "step": 30280
    },
    {
      "epoch": 1.580792464153848,
      "grad_norm": 5.180912494659424,
      "learning_rate": 2.3661778264878313e-05,
      "loss": 0.9643,
      "step": 30290
    },
    {
      "epoch": 1.5813143371560532,
      "grad_norm": 4.350968360900879,
      "learning_rate": 2.3653080041055617e-05,
      "loss": 0.8715,
      "step": 30300
    },
    {
      "epoch": 1.581836210158258,
      "grad_norm": 4.377456188201904,
      "learning_rate": 2.364438181723292e-05,
      "loss": 0.939,
      "step": 30310
    },
    {
      "epoch": 1.582358083160463,
      "grad_norm": 4.045595169067383,
      "learning_rate": 2.3635683593410227e-05,
      "loss": 0.8949,
      "step": 30320
    },
    {
      "epoch": 1.5828799561626679,
      "grad_norm": 5.527597427368164,
      "learning_rate": 2.362698536958753e-05,
      "loss": 0.8834,
      "step": 30330
    },
    {
      "epoch": 1.5834018291648726,
      "grad_norm": 4.310717582702637,
      "learning_rate": 2.3618287145764838e-05,
      "loss": 0.9004,
      "step": 30340
    },
    {
      "epoch": 1.5839237021670778,
      "grad_norm": 4.3409295082092285,
      "learning_rate": 2.360958892194214e-05,
      "loss": 0.7644,
      "step": 30350
    },
    {
      "epoch": 1.5844455751692825,
      "grad_norm": 5.2173333168029785,
      "learning_rate": 2.3600890698119445e-05,
      "loss": 0.9321,
      "step": 30360
    },
    {
      "epoch": 1.5849674481714875,
      "grad_norm": 4.429026126861572,
      "learning_rate": 2.3592192474296752e-05,
      "loss": 0.8747,
      "step": 30370
    },
    {
      "epoch": 1.5854893211736925,
      "grad_norm": 4.012906074523926,
      "learning_rate": 2.3583494250474052e-05,
      "loss": 0.842,
      "step": 30380
    },
    {
      "epoch": 1.5860111941758972,
      "grad_norm": 3.435654878616333,
      "learning_rate": 2.357479602665136e-05,
      "loss": 0.8338,
      "step": 30390
    },
    {
      "epoch": 1.5865330671781022,
      "grad_norm": 4.564944744110107,
      "learning_rate": 2.3566097802828662e-05,
      "loss": 0.941,
      "step": 30400
    },
    {
      "epoch": 1.5870549401803071,
      "grad_norm": 3.999626636505127,
      "learning_rate": 2.355739957900597e-05,
      "loss": 0.8702,
      "step": 30410
    },
    {
      "epoch": 1.5875768131825119,
      "grad_norm": 4.536900520324707,
      "learning_rate": 2.3548701355183273e-05,
      "loss": 0.8779,
      "step": 30420
    },
    {
      "epoch": 1.588098686184717,
      "grad_norm": 4.444675922393799,
      "learning_rate": 2.3540003131360576e-05,
      "loss": 0.8614,
      "step": 30430
    },
    {
      "epoch": 1.5886205591869218,
      "grad_norm": 4.809320449829102,
      "learning_rate": 2.3531304907537883e-05,
      "loss": 0.9599,
      "step": 30440
    },
    {
      "epoch": 1.5891424321891268,
      "grad_norm": 5.365639686584473,
      "learning_rate": 2.3522606683715187e-05,
      "loss": 0.8788,
      "step": 30450
    },
    {
      "epoch": 1.5896643051913317,
      "grad_norm": 3.7389719486236572,
      "learning_rate": 2.351390845989249e-05,
      "loss": 0.878,
      "step": 30460
    },
    {
      "epoch": 1.5901861781935365,
      "grad_norm": 4.0603413581848145,
      "learning_rate": 2.3505210236069797e-05,
      "loss": 0.8763,
      "step": 30470
    },
    {
      "epoch": 1.5907080511957417,
      "grad_norm": 4.654353618621826,
      "learning_rate": 2.34965120122471e-05,
      "loss": 0.8075,
      "step": 30480
    },
    {
      "epoch": 1.5912299241979464,
      "grad_norm": 5.343881130218506,
      "learning_rate": 2.3487813788424404e-05,
      "loss": 0.8677,
      "step": 30490
    },
    {
      "epoch": 1.5917517972001514,
      "grad_norm": 4.89635705947876,
      "learning_rate": 2.3479115564601708e-05,
      "loss": 0.9224,
      "step": 30500
    },
    {
      "epoch": 1.5922736702023563,
      "grad_norm": 4.943815231323242,
      "learning_rate": 2.3470417340779015e-05,
      "loss": 0.8458,
      "step": 30510
    },
    {
      "epoch": 1.592795543204561,
      "grad_norm": 4.335348606109619,
      "learning_rate": 2.346171911695632e-05,
      "loss": 0.8761,
      "step": 30520
    },
    {
      "epoch": 1.593317416206766,
      "grad_norm": 5.555238246917725,
      "learning_rate": 2.3453020893133622e-05,
      "loss": 0.9204,
      "step": 30530
    },
    {
      "epoch": 1.593839289208971,
      "grad_norm": 3.614656925201416,
      "learning_rate": 2.344432266931093e-05,
      "loss": 0.7958,
      "step": 30540
    },
    {
      "epoch": 1.594361162211176,
      "grad_norm": 4.850757122039795,
      "learning_rate": 2.3435624445488233e-05,
      "loss": 0.8996,
      "step": 30550
    },
    {
      "epoch": 1.594883035213381,
      "grad_norm": 4.720836639404297,
      "learning_rate": 2.3426926221665536e-05,
      "loss": 0.9478,
      "step": 30560
    },
    {
      "epoch": 1.5954049082155857,
      "grad_norm": 4.803941249847412,
      "learning_rate": 2.3418227997842843e-05,
      "loss": 0.8567,
      "step": 30570
    },
    {
      "epoch": 1.5959267812177906,
      "grad_norm": 3.7405614852905273,
      "learning_rate": 2.3409529774020147e-05,
      "loss": 0.8442,
      "step": 30580
    },
    {
      "epoch": 1.5964486542199956,
      "grad_norm": 4.379957675933838,
      "learning_rate": 2.340083155019745e-05,
      "loss": 0.8615,
      "step": 30590
    },
    {
      "epoch": 1.5969705272222003,
      "grad_norm": 4.790497779846191,
      "learning_rate": 2.3392133326374754e-05,
      "loss": 0.9214,
      "step": 30600
    },
    {
      "epoch": 1.5974924002244055,
      "grad_norm": 4.286136150360107,
      "learning_rate": 2.338343510255206e-05,
      "loss": 0.8387,
      "step": 30610
    },
    {
      "epoch": 1.5980142732266103,
      "grad_norm": 3.6967878341674805,
      "learning_rate": 2.3374736878729364e-05,
      "loss": 0.86,
      "step": 30620
    },
    {
      "epoch": 1.5985361462288152,
      "grad_norm": 4.763147830963135,
      "learning_rate": 2.3366038654906668e-05,
      "loss": 0.9446,
      "step": 30630
    },
    {
      "epoch": 1.5990580192310202,
      "grad_norm": 5.450368881225586,
      "learning_rate": 2.3357340431083975e-05,
      "loss": 0.8961,
      "step": 30640
    },
    {
      "epoch": 1.599579892233225,
      "grad_norm": 4.578090190887451,
      "learning_rate": 2.3348642207261278e-05,
      "loss": 0.8567,
      "step": 30650
    },
    {
      "epoch": 1.60010176523543,
      "grad_norm": 3.9018993377685547,
      "learning_rate": 2.3339943983438585e-05,
      "loss": 0.8769,
      "step": 30660
    },
    {
      "epoch": 1.6006236382376349,
      "grad_norm": 4.593533515930176,
      "learning_rate": 2.333124575961589e-05,
      "loss": 0.8455,
      "step": 30670
    },
    {
      "epoch": 1.6011455112398398,
      "grad_norm": 4.886646747589111,
      "learning_rate": 2.3322547535793192e-05,
      "loss": 0.8453,
      "step": 30680
    },
    {
      "epoch": 1.6016673842420448,
      "grad_norm": 4.9089250564575195,
      "learning_rate": 2.3313849311970496e-05,
      "loss": 0.9029,
      "step": 30690
    },
    {
      "epoch": 1.6021892572442495,
      "grad_norm": 4.515152454376221,
      "learning_rate": 2.33051510881478e-05,
      "loss": 0.8128,
      "step": 30700
    },
    {
      "epoch": 1.6027111302464545,
      "grad_norm": 4.114607334136963,
      "learning_rate": 2.3296452864325106e-05,
      "loss": 0.9198,
      "step": 30710
    },
    {
      "epoch": 1.6032330032486595,
      "grad_norm": 5.014618396759033,
      "learning_rate": 2.328775464050241e-05,
      "loss": 0.9289,
      "step": 30720
    },
    {
      "epoch": 1.6037548762508642,
      "grad_norm": 4.095240592956543,
      "learning_rate": 2.3279056416679713e-05,
      "loss": 0.8158,
      "step": 30730
    },
    {
      "epoch": 1.6042767492530694,
      "grad_norm": 4.719027042388916,
      "learning_rate": 2.327035819285702e-05,
      "loss": 0.9509,
      "step": 30740
    },
    {
      "epoch": 1.6047986222552741,
      "grad_norm": 4.502074241638184,
      "learning_rate": 2.3261659969034324e-05,
      "loss": 0.8897,
      "step": 30750
    },
    {
      "epoch": 1.605320495257479,
      "grad_norm": 4.63348388671875,
      "learning_rate": 2.325296174521163e-05,
      "loss": 0.9076,
      "step": 30760
    },
    {
      "epoch": 1.605842368259684,
      "grad_norm": 4.477261543273926,
      "learning_rate": 2.3244263521388934e-05,
      "loss": 0.8607,
      "step": 30770
    },
    {
      "epoch": 1.6063642412618888,
      "grad_norm": 4.577803611755371,
      "learning_rate": 2.3235565297566238e-05,
      "loss": 0.8352,
      "step": 30780
    },
    {
      "epoch": 1.6068861142640938,
      "grad_norm": 4.337814807891846,
      "learning_rate": 2.3226867073743545e-05,
      "loss": 0.9325,
      "step": 30790
    },
    {
      "epoch": 1.6074079872662987,
      "grad_norm": 3.3127548694610596,
      "learning_rate": 2.3218168849920845e-05,
      "loss": 0.9029,
      "step": 30800
    },
    {
      "epoch": 1.6079298602685037,
      "grad_norm": 3.5592410564422607,
      "learning_rate": 2.3209470626098152e-05,
      "loss": 0.9078,
      "step": 30810
    },
    {
      "epoch": 1.6084517332707087,
      "grad_norm": 3.907534122467041,
      "learning_rate": 2.3200772402275455e-05,
      "loss": 0.9285,
      "step": 30820
    },
    {
      "epoch": 1.6089736062729134,
      "grad_norm": 4.7160515785217285,
      "learning_rate": 2.319207417845276e-05,
      "loss": 0.9703,
      "step": 30830
    },
    {
      "epoch": 1.6094954792751184,
      "grad_norm": 4.661138534545898,
      "learning_rate": 2.3183375954630066e-05,
      "loss": 0.9428,
      "step": 30840
    },
    {
      "epoch": 1.6100173522773233,
      "grad_norm": 4.951833724975586,
      "learning_rate": 2.317467773080737e-05,
      "loss": 0.8935,
      "step": 30850
    },
    {
      "epoch": 1.610539225279528,
      "grad_norm": 3.7159199714660645,
      "learning_rate": 2.3165979506984676e-05,
      "loss": 0.7951,
      "step": 30860
    },
    {
      "epoch": 1.6110610982817333,
      "grad_norm": 4.206649303436279,
      "learning_rate": 2.315728128316198e-05,
      "loss": 0.9152,
      "step": 30870
    },
    {
      "epoch": 1.611582971283938,
      "grad_norm": 4.348914623260498,
      "learning_rate": 2.3148583059339284e-05,
      "loss": 0.9636,
      "step": 30880
    },
    {
      "epoch": 1.612104844286143,
      "grad_norm": 4.902575492858887,
      "learning_rate": 2.313988483551659e-05,
      "loss": 0.9083,
      "step": 30890
    },
    {
      "epoch": 1.612626717288348,
      "grad_norm": 4.488248348236084,
      "learning_rate": 2.313118661169389e-05,
      "loss": 0.8496,
      "step": 30900
    },
    {
      "epoch": 1.6131485902905527,
      "grad_norm": 4.462622165679932,
      "learning_rate": 2.3122488387871198e-05,
      "loss": 0.8763,
      "step": 30910
    },
    {
      "epoch": 1.6136704632927579,
      "grad_norm": 4.504836559295654,
      "learning_rate": 2.31137901640485e-05,
      "loss": 0.8353,
      "step": 30920
    },
    {
      "epoch": 1.6141923362949626,
      "grad_norm": 3.802318572998047,
      "learning_rate": 2.3105091940225808e-05,
      "loss": 0.824,
      "step": 30930
    },
    {
      "epoch": 1.6147142092971676,
      "grad_norm": 3.898719072341919,
      "learning_rate": 2.309639371640311e-05,
      "loss": 0.8331,
      "step": 30940
    },
    {
      "epoch": 1.6152360822993725,
      "grad_norm": 3.7308273315429688,
      "learning_rate": 2.3087695492580415e-05,
      "loss": 0.849,
      "step": 30950
    },
    {
      "epoch": 1.6157579553015773,
      "grad_norm": 4.519367694854736,
      "learning_rate": 2.3078997268757722e-05,
      "loss": 0.9174,
      "step": 30960
    },
    {
      "epoch": 1.6162798283037823,
      "grad_norm": 4.470608711242676,
      "learning_rate": 2.3070299044935026e-05,
      "loss": 0.9142,
      "step": 30970
    },
    {
      "epoch": 1.6168017013059872,
      "grad_norm": 5.043359279632568,
      "learning_rate": 2.306160082111233e-05,
      "loss": 0.8361,
      "step": 30980
    },
    {
      "epoch": 1.617323574308192,
      "grad_norm": 4.521616458892822,
      "learning_rate": 2.3052902597289636e-05,
      "loss": 0.9054,
      "step": 30990
    },
    {
      "epoch": 1.6178454473103971,
      "grad_norm": 4.537341117858887,
      "learning_rate": 2.304420437346694e-05,
      "loss": 0.8802,
      "step": 31000
    },
    {
      "epoch": 1.6183673203126019,
      "grad_norm": 4.982354164123535,
      "learning_rate": 2.3035506149644243e-05,
      "loss": 0.8315,
      "step": 31010
    },
    {
      "epoch": 1.6188891933148069,
      "grad_norm": 3.793428897857666,
      "learning_rate": 2.3026807925821547e-05,
      "loss": 0.9068,
      "step": 31020
    },
    {
      "epoch": 1.6194110663170118,
      "grad_norm": 4.708272457122803,
      "learning_rate": 2.3018109701998854e-05,
      "loss": 0.9168,
      "step": 31030
    },
    {
      "epoch": 1.6199329393192166,
      "grad_norm": 4.1349263191223145,
      "learning_rate": 2.3009411478176157e-05,
      "loss": 0.8994,
      "step": 31040
    },
    {
      "epoch": 1.6204548123214217,
      "grad_norm": 3.579288959503174,
      "learning_rate": 2.300071325435346e-05,
      "loss": 0.8757,
      "step": 31050
    },
    {
      "epoch": 1.6209766853236265,
      "grad_norm": 4.309892177581787,
      "learning_rate": 2.2992015030530768e-05,
      "loss": 0.866,
      "step": 31060
    },
    {
      "epoch": 1.6214985583258315,
      "grad_norm": 4.452885627746582,
      "learning_rate": 2.298331680670807e-05,
      "loss": 0.8998,
      "step": 31070
    },
    {
      "epoch": 1.6220204313280364,
      "grad_norm": 4.475430011749268,
      "learning_rate": 2.2974618582885378e-05,
      "loss": 0.8146,
      "step": 31080
    },
    {
      "epoch": 1.6225423043302412,
      "grad_norm": 3.770242929458618,
      "learning_rate": 2.2965920359062682e-05,
      "loss": 0.8987,
      "step": 31090
    },
    {
      "epoch": 1.6230641773324461,
      "grad_norm": 4.26011848449707,
      "learning_rate": 2.2957222135239985e-05,
      "loss": 0.9026,
      "step": 31100
    },
    {
      "epoch": 1.623586050334651,
      "grad_norm": 3.8995187282562256,
      "learning_rate": 2.294852391141729e-05,
      "loss": 0.8209,
      "step": 31110
    },
    {
      "epoch": 1.6241079233368558,
      "grad_norm": 4.70529842376709,
      "learning_rate": 2.2939825687594592e-05,
      "loss": 0.9438,
      "step": 31120
    },
    {
      "epoch": 1.624629796339061,
      "grad_norm": 4.189418315887451,
      "learning_rate": 2.29311274637719e-05,
      "loss": 0.8111,
      "step": 31130
    },
    {
      "epoch": 1.6251516693412658,
      "grad_norm": 4.8038201332092285,
      "learning_rate": 2.2922429239949203e-05,
      "loss": 0.9458,
      "step": 31140
    },
    {
      "epoch": 1.6256735423434707,
      "grad_norm": 4.404758453369141,
      "learning_rate": 2.2913731016126506e-05,
      "loss": 0.8873,
      "step": 31150
    },
    {
      "epoch": 1.6261954153456757,
      "grad_norm": 3.92877459526062,
      "learning_rate": 2.2905032792303813e-05,
      "loss": 0.9342,
      "step": 31160
    },
    {
      "epoch": 1.6267172883478804,
      "grad_norm": 4.49752140045166,
      "learning_rate": 2.2896334568481117e-05,
      "loss": 0.8261,
      "step": 31170
    },
    {
      "epoch": 1.6272391613500856,
      "grad_norm": 3.932708501815796,
      "learning_rate": 2.2887636344658424e-05,
      "loss": 0.8748,
      "step": 31180
    },
    {
      "epoch": 1.6277610343522904,
      "grad_norm": 5.445003032684326,
      "learning_rate": 2.2878938120835727e-05,
      "loss": 0.951,
      "step": 31190
    },
    {
      "epoch": 1.6282829073544953,
      "grad_norm": 4.62001371383667,
      "learning_rate": 2.287023989701303e-05,
      "loss": 0.8655,
      "step": 31200
    },
    {
      "epoch": 1.6288047803567003,
      "grad_norm": 4.58080530166626,
      "learning_rate": 2.2861541673190335e-05,
      "loss": 0.9224,
      "step": 31210
    },
    {
      "epoch": 1.629326653358905,
      "grad_norm": 4.351977825164795,
      "learning_rate": 2.2852843449367638e-05,
      "loss": 0.8867,
      "step": 31220
    },
    {
      "epoch": 1.62984852636111,
      "grad_norm": 3.898319721221924,
      "learning_rate": 2.2844145225544945e-05,
      "loss": 0.8743,
      "step": 31230
    },
    {
      "epoch": 1.630370399363315,
      "grad_norm": 4.93252420425415,
      "learning_rate": 2.283544700172225e-05,
      "loss": 0.8887,
      "step": 31240
    },
    {
      "epoch": 1.6308922723655197,
      "grad_norm": 4.448753356933594,
      "learning_rate": 2.2826748777899552e-05,
      "loss": 0.91,
      "step": 31250
    },
    {
      "epoch": 1.6314141453677249,
      "grad_norm": 4.152189254760742,
      "learning_rate": 2.281805055407686e-05,
      "loss": 0.8899,
      "step": 31260
    },
    {
      "epoch": 1.6319360183699296,
      "grad_norm": 5.097165107727051,
      "learning_rate": 2.2809352330254163e-05,
      "loss": 0.8783,
      "step": 31270
    },
    {
      "epoch": 1.6324578913721346,
      "grad_norm": 3.9409730434417725,
      "learning_rate": 2.280065410643147e-05,
      "loss": 0.8581,
      "step": 31280
    },
    {
      "epoch": 1.6329797643743396,
      "grad_norm": 4.129436016082764,
      "learning_rate": 2.2791955882608773e-05,
      "loss": 0.8163,
      "step": 31290
    },
    {
      "epoch": 1.6335016373765443,
      "grad_norm": 3.9106802940368652,
      "learning_rate": 2.2783257658786077e-05,
      "loss": 0.9603,
      "step": 31300
    },
    {
      "epoch": 1.6340235103787495,
      "grad_norm": 3.9258344173431396,
      "learning_rate": 2.2774559434963384e-05,
      "loss": 0.8539,
      "step": 31310
    },
    {
      "epoch": 1.6345453833809542,
      "grad_norm": 5.798863410949707,
      "learning_rate": 2.2765861211140684e-05,
      "loss": 0.8825,
      "step": 31320
    },
    {
      "epoch": 1.6350672563831592,
      "grad_norm": 4.409562110900879,
      "learning_rate": 2.275716298731799e-05,
      "loss": 0.969,
      "step": 31330
    },
    {
      "epoch": 1.6355891293853642,
      "grad_norm": 4.904183387756348,
      "learning_rate": 2.2748464763495294e-05,
      "loss": 0.8711,
      "step": 31340
    },
    {
      "epoch": 1.636111002387569,
      "grad_norm": 5.1846137046813965,
      "learning_rate": 2.27397665396726e-05,
      "loss": 0.9528,
      "step": 31350
    },
    {
      "epoch": 1.6366328753897739,
      "grad_norm": 3.3243584632873535,
      "learning_rate": 2.2731068315849905e-05,
      "loss": 0.8964,
      "step": 31360
    },
    {
      "epoch": 1.6371547483919788,
      "grad_norm": 4.745312213897705,
      "learning_rate": 2.2722370092027208e-05,
      "loss": 0.8037,
      "step": 31370
    },
    {
      "epoch": 1.6376766213941836,
      "grad_norm": 4.733745098114014,
      "learning_rate": 2.2713671868204515e-05,
      "loss": 0.9111,
      "step": 31380
    },
    {
      "epoch": 1.6381984943963888,
      "grad_norm": 3.5705366134643555,
      "learning_rate": 2.270497364438182e-05,
      "loss": 0.9207,
      "step": 31390
    },
    {
      "epoch": 1.6387203673985935,
      "grad_norm": 5.214579105377197,
      "learning_rate": 2.2696275420559122e-05,
      "loss": 1.0033,
      "step": 31400
    },
    {
      "epoch": 1.6392422404007985,
      "grad_norm": 5.035223960876465,
      "learning_rate": 2.268757719673643e-05,
      "loss": 0.8188,
      "step": 31410
    },
    {
      "epoch": 1.6397641134030034,
      "grad_norm": 4.33901309967041,
      "learning_rate": 2.267887897291373e-05,
      "loss": 0.8477,
      "step": 31420
    },
    {
      "epoch": 1.6402859864052082,
      "grad_norm": 4.309035301208496,
      "learning_rate": 2.2670180749091036e-05,
      "loss": 0.8039,
      "step": 31430
    },
    {
      "epoch": 1.6408078594074134,
      "grad_norm": 4.6710405349731445,
      "learning_rate": 2.266148252526834e-05,
      "loss": 0.9421,
      "step": 31440
    },
    {
      "epoch": 1.641329732409618,
      "grad_norm": 4.212867259979248,
      "learning_rate": 2.2652784301445647e-05,
      "loss": 0.8634,
      "step": 31450
    },
    {
      "epoch": 1.641851605411823,
      "grad_norm": 4.213176727294922,
      "learning_rate": 2.264408607762295e-05,
      "loss": 0.8192,
      "step": 31460
    },
    {
      "epoch": 1.642373478414028,
      "grad_norm": 3.724661111831665,
      "learning_rate": 2.2635387853800254e-05,
      "loss": 0.9633,
      "step": 31470
    },
    {
      "epoch": 1.6428953514162328,
      "grad_norm": 4.468320846557617,
      "learning_rate": 2.262668962997756e-05,
      "loss": 0.9033,
      "step": 31480
    },
    {
      "epoch": 1.6434172244184377,
      "grad_norm": 3.295680046081543,
      "learning_rate": 2.2617991406154864e-05,
      "loss": 0.7887,
      "step": 31490
    },
    {
      "epoch": 1.6439390974206427,
      "grad_norm": 4.747121810913086,
      "learning_rate": 2.260929318233217e-05,
      "loss": 0.8618,
      "step": 31500
    },
    {
      "epoch": 1.6444609704228474,
      "grad_norm": 4.328982353210449,
      "learning_rate": 2.2600594958509475e-05,
      "loss": 0.8715,
      "step": 31510
    },
    {
      "epoch": 1.6449828434250526,
      "grad_norm": 4.9157209396362305,
      "learning_rate": 2.259189673468678e-05,
      "loss": 0.941,
      "step": 31520
    },
    {
      "epoch": 1.6455047164272574,
      "grad_norm": 4.224100589752197,
      "learning_rate": 2.2583198510864082e-05,
      "loss": 0.9183,
      "step": 31530
    },
    {
      "epoch": 1.6460265894294623,
      "grad_norm": 4.4475884437561035,
      "learning_rate": 2.2574500287041386e-05,
      "loss": 0.9242,
      "step": 31540
    },
    {
      "epoch": 1.6465484624316673,
      "grad_norm": 4.5142645835876465,
      "learning_rate": 2.2565802063218692e-05,
      "loss": 0.8541,
      "step": 31550
    },
    {
      "epoch": 1.647070335433872,
      "grad_norm": 4.107538223266602,
      "learning_rate": 2.2557103839395996e-05,
      "loss": 0.9233,
      "step": 31560
    },
    {
      "epoch": 1.6475922084360772,
      "grad_norm": 4.551361083984375,
      "learning_rate": 2.25484056155733e-05,
      "loss": 0.922,
      "step": 31570
    },
    {
      "epoch": 1.648114081438282,
      "grad_norm": 4.4498443603515625,
      "learning_rate": 2.2539707391750606e-05,
      "loss": 0.8982,
      "step": 31580
    },
    {
      "epoch": 1.648635954440487,
      "grad_norm": 4.230557441711426,
      "learning_rate": 2.253100916792791e-05,
      "loss": 0.8433,
      "step": 31590
    },
    {
      "epoch": 1.649157827442692,
      "grad_norm": 4.915653705596924,
      "learning_rate": 2.2522310944105217e-05,
      "loss": 0.916,
      "step": 31600
    },
    {
      "epoch": 1.6496797004448966,
      "grad_norm": 3.4293978214263916,
      "learning_rate": 2.251361272028252e-05,
      "loss": 0.8896,
      "step": 31610
    },
    {
      "epoch": 1.6502015734471016,
      "grad_norm": 3.4934210777282715,
      "learning_rate": 2.2504914496459824e-05,
      "loss": 0.9095,
      "step": 31620
    },
    {
      "epoch": 1.6507234464493066,
      "grad_norm": 3.8872764110565186,
      "learning_rate": 2.2496216272637128e-05,
      "loss": 0.8585,
      "step": 31630
    },
    {
      "epoch": 1.6512453194515113,
      "grad_norm": 4.6349382400512695,
      "learning_rate": 2.248751804881443e-05,
      "loss": 0.8979,
      "step": 31640
    },
    {
      "epoch": 1.6517671924537165,
      "grad_norm": 4.324112892150879,
      "learning_rate": 2.2478819824991738e-05,
      "loss": 0.8264,
      "step": 31650
    },
    {
      "epoch": 1.6522890654559212,
      "grad_norm": 4.047694206237793,
      "learning_rate": 2.247012160116904e-05,
      "loss": 0.9217,
      "step": 31660
    },
    {
      "epoch": 1.6528109384581262,
      "grad_norm": 5.106344223022461,
      "learning_rate": 2.2461423377346345e-05,
      "loss": 0.8936,
      "step": 31670
    },
    {
      "epoch": 1.6533328114603312,
      "grad_norm": 4.265960693359375,
      "learning_rate": 2.2452725153523652e-05,
      "loss": 0.8033,
      "step": 31680
    },
    {
      "epoch": 1.653854684462536,
      "grad_norm": 3.857173204421997,
      "learning_rate": 2.2444026929700956e-05,
      "loss": 0.85,
      "step": 31690
    },
    {
      "epoch": 1.654376557464741,
      "grad_norm": 3.5806846618652344,
      "learning_rate": 2.2435328705878263e-05,
      "loss": 0.8895,
      "step": 31700
    },
    {
      "epoch": 1.6548984304669458,
      "grad_norm": 3.914991617202759,
      "learning_rate": 2.2426630482055566e-05,
      "loss": 0.8077,
      "step": 31710
    },
    {
      "epoch": 1.6554203034691508,
      "grad_norm": 4.503724575042725,
      "learning_rate": 2.241793225823287e-05,
      "loss": 0.8218,
      "step": 31720
    },
    {
      "epoch": 1.6559421764713558,
      "grad_norm": 4.316145420074463,
      "learning_rate": 2.2409234034410177e-05,
      "loss": 0.8752,
      "step": 31730
    },
    {
      "epoch": 1.6564640494735605,
      "grad_norm": 4.660048007965088,
      "learning_rate": 2.2400535810587477e-05,
      "loss": 0.9183,
      "step": 31740
    },
    {
      "epoch": 1.6569859224757655,
      "grad_norm": 4.529979228973389,
      "learning_rate": 2.2391837586764784e-05,
      "loss": 0.9577,
      "step": 31750
    },
    {
      "epoch": 1.6575077954779704,
      "grad_norm": 4.908136367797852,
      "learning_rate": 2.2383139362942087e-05,
      "loss": 0.8918,
      "step": 31760
    },
    {
      "epoch": 1.6580296684801752,
      "grad_norm": 4.687617301940918,
      "learning_rate": 2.2374441139119394e-05,
      "loss": 0.9143,
      "step": 31770
    },
    {
      "epoch": 1.6585515414823804,
      "grad_norm": 5.1954264640808105,
      "learning_rate": 2.2365742915296698e-05,
      "loss": 0.897,
      "step": 31780
    },
    {
      "epoch": 1.6590734144845851,
      "grad_norm": 3.8304202556610107,
      "learning_rate": 2.2357044691474e-05,
      "loss": 0.7854,
      "step": 31790
    },
    {
      "epoch": 1.65959528748679,
      "grad_norm": 4.2904486656188965,
      "learning_rate": 2.2348346467651308e-05,
      "loss": 0.8766,
      "step": 31800
    },
    {
      "epoch": 1.660117160488995,
      "grad_norm": 5.166255950927734,
      "learning_rate": 2.2339648243828612e-05,
      "loss": 0.8705,
      "step": 31810
    },
    {
      "epoch": 1.6606390334911998,
      "grad_norm": 4.89880895614624,
      "learning_rate": 2.2330950020005915e-05,
      "loss": 0.9696,
      "step": 31820
    },
    {
      "epoch": 1.661160906493405,
      "grad_norm": 4.154317855834961,
      "learning_rate": 2.2322251796183222e-05,
      "loss": 0.8684,
      "step": 31830
    },
    {
      "epoch": 1.6616827794956097,
      "grad_norm": 5.168162822723389,
      "learning_rate": 2.2313553572360522e-05,
      "loss": 0.8435,
      "step": 31840
    },
    {
      "epoch": 1.6622046524978147,
      "grad_norm": 4.212821960449219,
      "learning_rate": 2.230485534853783e-05,
      "loss": 0.8854,
      "step": 31850
    },
    {
      "epoch": 1.6627265255000196,
      "grad_norm": 4.303337574005127,
      "learning_rate": 2.2296157124715133e-05,
      "loss": 0.8884,
      "step": 31860
    },
    {
      "epoch": 1.6632483985022244,
      "grad_norm": 5.238061904907227,
      "learning_rate": 2.228745890089244e-05,
      "loss": 0.9523,
      "step": 31870
    },
    {
      "epoch": 1.6637702715044294,
      "grad_norm": 3.80830717086792,
      "learning_rate": 2.2278760677069743e-05,
      "loss": 0.8105,
      "step": 31880
    },
    {
      "epoch": 1.6642921445066343,
      "grad_norm": 4.609603404998779,
      "learning_rate": 2.2270062453247047e-05,
      "loss": 0.9985,
      "step": 31890
    },
    {
      "epoch": 1.664814017508839,
      "grad_norm": 4.416158676147461,
      "learning_rate": 2.2261364229424354e-05,
      "loss": 0.9408,
      "step": 31900
    },
    {
      "epoch": 1.6653358905110442,
      "grad_norm": 4.433771133422852,
      "learning_rate": 2.2252666005601657e-05,
      "loss": 0.9295,
      "step": 31910
    },
    {
      "epoch": 1.665857763513249,
      "grad_norm": 4.030964374542236,
      "learning_rate": 2.224396778177896e-05,
      "loss": 0.8464,
      "step": 31920
    },
    {
      "epoch": 1.666379636515454,
      "grad_norm": 3.924119234085083,
      "learning_rate": 2.2235269557956268e-05,
      "loss": 0.8828,
      "step": 31930
    },
    {
      "epoch": 1.666901509517659,
      "grad_norm": 5.070577144622803,
      "learning_rate": 2.2226571334133568e-05,
      "loss": 1.0277,
      "step": 31940
    },
    {
      "epoch": 1.6674233825198637,
      "grad_norm": 4.168317794799805,
      "learning_rate": 2.2217873110310875e-05,
      "loss": 0.8901,
      "step": 31950
    },
    {
      "epoch": 1.6679452555220688,
      "grad_norm": 5.151978492736816,
      "learning_rate": 2.220917488648818e-05,
      "loss": 0.9177,
      "step": 31960
    },
    {
      "epoch": 1.6684671285242736,
      "grad_norm": 4.699355602264404,
      "learning_rate": 2.2200476662665486e-05,
      "loss": 0.9271,
      "step": 31970
    },
    {
      "epoch": 1.6689890015264786,
      "grad_norm": 4.211207866668701,
      "learning_rate": 2.219177843884279e-05,
      "loss": 0.8502,
      "step": 31980
    },
    {
      "epoch": 1.6695108745286835,
      "grad_norm": 4.0218939781188965,
      "learning_rate": 2.2183080215020093e-05,
      "loss": 0.818,
      "step": 31990
    },
    {
      "epoch": 1.6700327475308883,
      "grad_norm": 4.685274124145508,
      "learning_rate": 2.21743819911974e-05,
      "loss": 0.8931,
      "step": 32000
    },
    {
      "epoch": 1.6705546205330932,
      "grad_norm": 4.97462272644043,
      "learning_rate": 2.2165683767374703e-05,
      "loss": 0.9469,
      "step": 32010
    },
    {
      "epoch": 1.6710764935352982,
      "grad_norm": 4.4146809577941895,
      "learning_rate": 2.215698554355201e-05,
      "loss": 0.9183,
      "step": 32020
    },
    {
      "epoch": 1.671598366537503,
      "grad_norm": 4.091416835784912,
      "learning_rate": 2.2148287319729314e-05,
      "loss": 0.988,
      "step": 32030
    },
    {
      "epoch": 1.6721202395397081,
      "grad_norm": 4.551691055297852,
      "learning_rate": 2.2139589095906617e-05,
      "loss": 0.8456,
      "step": 32040
    },
    {
      "epoch": 1.6726421125419129,
      "grad_norm": 3.485966682434082,
      "learning_rate": 2.213089087208392e-05,
      "loss": 0.9437,
      "step": 32050
    },
    {
      "epoch": 1.6731639855441178,
      "grad_norm": 5.649214744567871,
      "learning_rate": 2.2122192648261224e-05,
      "loss": 0.9309,
      "step": 32060
    },
    {
      "epoch": 1.6736858585463228,
      "grad_norm": 5.135087490081787,
      "learning_rate": 2.211349442443853e-05,
      "loss": 0.8215,
      "step": 32070
    },
    {
      "epoch": 1.6742077315485275,
      "grad_norm": 3.576678514480591,
      "learning_rate": 2.2104796200615835e-05,
      "loss": 0.8098,
      "step": 32080
    },
    {
      "epoch": 1.6747296045507327,
      "grad_norm": 4.233923435211182,
      "learning_rate": 2.2096097976793138e-05,
      "loss": 0.8925,
      "step": 32090
    },
    {
      "epoch": 1.6752514775529375,
      "grad_norm": 4.450843334197998,
      "learning_rate": 2.2087399752970445e-05,
      "loss": 0.8879,
      "step": 32100
    },
    {
      "epoch": 1.6757733505551424,
      "grad_norm": 4.862620830535889,
      "learning_rate": 2.207870152914775e-05,
      "loss": 0.8829,
      "step": 32110
    },
    {
      "epoch": 1.6762952235573474,
      "grad_norm": 4.46626615524292,
      "learning_rate": 2.2070003305325056e-05,
      "loss": 0.8478,
      "step": 32120
    },
    {
      "epoch": 1.6768170965595521,
      "grad_norm": 4.898765563964844,
      "learning_rate": 2.206130508150236e-05,
      "loss": 0.9342,
      "step": 32130
    },
    {
      "epoch": 1.677338969561757,
      "grad_norm": 3.602051258087158,
      "learning_rate": 2.2052606857679663e-05,
      "loss": 0.8959,
      "step": 32140
    },
    {
      "epoch": 1.677860842563962,
      "grad_norm": 4.839296340942383,
      "learning_rate": 2.2043908633856966e-05,
      "loss": 0.8741,
      "step": 32150
    },
    {
      "epoch": 1.678382715566167,
      "grad_norm": 4.626546859741211,
      "learning_rate": 2.203521041003427e-05,
      "loss": 0.8647,
      "step": 32160
    },
    {
      "epoch": 1.678904588568372,
      "grad_norm": 5.098807334899902,
      "learning_rate": 2.2026512186211577e-05,
      "loss": 0.8847,
      "step": 32170
    },
    {
      "epoch": 1.6794264615705767,
      "grad_norm": 4.476650714874268,
      "learning_rate": 2.201781396238888e-05,
      "loss": 0.9366,
      "step": 32180
    },
    {
      "epoch": 1.6799483345727817,
      "grad_norm": 5.878854751586914,
      "learning_rate": 2.2009115738566184e-05,
      "loss": 0.8771,
      "step": 32190
    },
    {
      "epoch": 1.6804702075749867,
      "grad_norm": 3.8732993602752686,
      "learning_rate": 2.200041751474349e-05,
      "loss": 0.8724,
      "step": 32200
    },
    {
      "epoch": 1.6809920805771914,
      "grad_norm": 4.181655406951904,
      "learning_rate": 2.1991719290920794e-05,
      "loss": 1.026,
      "step": 32210
    },
    {
      "epoch": 1.6815139535793966,
      "grad_norm": 4.3734612464904785,
      "learning_rate": 2.19830210670981e-05,
      "loss": 0.9346,
      "step": 32220
    },
    {
      "epoch": 1.6820358265816013,
      "grad_norm": 4.3135175704956055,
      "learning_rate": 2.1974322843275405e-05,
      "loss": 0.9076,
      "step": 32230
    },
    {
      "epoch": 1.6825576995838063,
      "grad_norm": 4.36670446395874,
      "learning_rate": 2.196562461945271e-05,
      "loss": 0.867,
      "step": 32240
    },
    {
      "epoch": 1.6830795725860113,
      "grad_norm": 3.15246844291687,
      "learning_rate": 2.1956926395630015e-05,
      "loss": 0.9069,
      "step": 32250
    },
    {
      "epoch": 1.683601445588216,
      "grad_norm": 5.342029094696045,
      "learning_rate": 2.1948228171807316e-05,
      "loss": 0.8461,
      "step": 32260
    },
    {
      "epoch": 1.684123318590421,
      "grad_norm": 4.483734130859375,
      "learning_rate": 2.1939529947984623e-05,
      "loss": 0.8231,
      "step": 32270
    },
    {
      "epoch": 1.684645191592626,
      "grad_norm": 5.046065807342529,
      "learning_rate": 2.1930831724161926e-05,
      "loss": 0.9027,
      "step": 32280
    },
    {
      "epoch": 1.685167064594831,
      "grad_norm": 3.940979480743408,
      "learning_rate": 2.1922133500339233e-05,
      "loss": 0.8369,
      "step": 32290
    },
    {
      "epoch": 1.6856889375970359,
      "grad_norm": 4.473959922790527,
      "learning_rate": 2.1913435276516537e-05,
      "loss": 0.8417,
      "step": 32300
    },
    {
      "epoch": 1.6862108105992406,
      "grad_norm": 4.123091697692871,
      "learning_rate": 2.190473705269384e-05,
      "loss": 0.8699,
      "step": 32310
    },
    {
      "epoch": 1.6867326836014456,
      "grad_norm": 3.661376953125,
      "learning_rate": 2.1896038828871147e-05,
      "loss": 0.8094,
      "step": 32320
    },
    {
      "epoch": 1.6872545566036505,
      "grad_norm": 3.981161117553711,
      "learning_rate": 2.188734060504845e-05,
      "loss": 0.8764,
      "step": 32330
    },
    {
      "epoch": 1.6877764296058553,
      "grad_norm": 4.724438190460205,
      "learning_rate": 2.1878642381225754e-05,
      "loss": 0.856,
      "step": 32340
    },
    {
      "epoch": 1.6882983026080605,
      "grad_norm": 3.8309590816497803,
      "learning_rate": 2.186994415740306e-05,
      "loss": 0.897,
      "step": 32350
    },
    {
      "epoch": 1.6888201756102652,
      "grad_norm": 3.284897804260254,
      "learning_rate": 2.186124593358036e-05,
      "loss": 0.8367,
      "step": 32360
    },
    {
      "epoch": 1.6893420486124702,
      "grad_norm": 4.512352466583252,
      "learning_rate": 2.1852547709757668e-05,
      "loss": 0.9653,
      "step": 32370
    },
    {
      "epoch": 1.6898639216146751,
      "grad_norm": 4.512889862060547,
      "learning_rate": 2.1843849485934972e-05,
      "loss": 0.8127,
      "step": 32380
    },
    {
      "epoch": 1.6903857946168799,
      "grad_norm": 3.736633539199829,
      "learning_rate": 2.183515126211228e-05,
      "loss": 0.8859,
      "step": 32390
    },
    {
      "epoch": 1.6909076676190848,
      "grad_norm": 4.102280616760254,
      "learning_rate": 2.1826453038289582e-05,
      "loss": 0.8301,
      "step": 32400
    },
    {
      "epoch": 1.6914295406212898,
      "grad_norm": 3.8267831802368164,
      "learning_rate": 2.1817754814466886e-05,
      "loss": 0.9062,
      "step": 32410
    },
    {
      "epoch": 1.6919514136234948,
      "grad_norm": 4.323731422424316,
      "learning_rate": 2.1809056590644193e-05,
      "loss": 0.8633,
      "step": 32420
    },
    {
      "epoch": 1.6924732866256997,
      "grad_norm": 3.364715099334717,
      "learning_rate": 2.1800358366821496e-05,
      "loss": 0.8915,
      "step": 32430
    },
    {
      "epoch": 1.6929951596279045,
      "grad_norm": 4.956745147705078,
      "learning_rate": 2.1791660142998803e-05,
      "loss": 0.782,
      "step": 32440
    },
    {
      "epoch": 1.6935170326301094,
      "grad_norm": 5.102099418640137,
      "learning_rate": 2.1782961919176107e-05,
      "loss": 0.8822,
      "step": 32450
    },
    {
      "epoch": 1.6940389056323144,
      "grad_norm": 3.648073673248291,
      "learning_rate": 2.177426369535341e-05,
      "loss": 0.8341,
      "step": 32460
    },
    {
      "epoch": 1.6945607786345191,
      "grad_norm": 4.565392017364502,
      "learning_rate": 2.1765565471530714e-05,
      "loss": 0.8952,
      "step": 32470
    },
    {
      "epoch": 1.6950826516367243,
      "grad_norm": 4.593977928161621,
      "learning_rate": 2.1756867247708017e-05,
      "loss": 0.7852,
      "step": 32480
    },
    {
      "epoch": 1.695604524638929,
      "grad_norm": 4.418860912322998,
      "learning_rate": 2.1748169023885324e-05,
      "loss": 0.951,
      "step": 32490
    },
    {
      "epoch": 1.696126397641134,
      "grad_norm": 3.98283052444458,
      "learning_rate": 2.1739470800062628e-05,
      "loss": 0.8742,
      "step": 32500
    },
    {
      "epoch": 1.696648270643339,
      "grad_norm": 3.9309420585632324,
      "learning_rate": 2.173077257623993e-05,
      "loss": 0.8724,
      "step": 32510
    },
    {
      "epoch": 1.6971701436455437,
      "grad_norm": 3.9744224548339844,
      "learning_rate": 2.172207435241724e-05,
      "loss": 0.8616,
      "step": 32520
    },
    {
      "epoch": 1.697692016647749,
      "grad_norm": 4.3672919273376465,
      "learning_rate": 2.1713376128594542e-05,
      "loss": 0.9424,
      "step": 32530
    },
    {
      "epoch": 1.6982138896499537,
      "grad_norm": 5.044766426086426,
      "learning_rate": 2.170467790477185e-05,
      "loss": 0.9067,
      "step": 32540
    },
    {
      "epoch": 1.6987357626521586,
      "grad_norm": 5.316970348358154,
      "learning_rate": 2.1695979680949152e-05,
      "loss": 0.8926,
      "step": 32550
    },
    {
      "epoch": 1.6992576356543636,
      "grad_norm": 4.023646831512451,
      "learning_rate": 2.1687281457126456e-05,
      "loss": 0.958,
      "step": 32560
    },
    {
      "epoch": 1.6997795086565683,
      "grad_norm": 4.680333614349365,
      "learning_rate": 2.167858323330376e-05,
      "loss": 0.9282,
      "step": 32570
    },
    {
      "epoch": 1.7003013816587733,
      "grad_norm": 4.587648868560791,
      "learning_rate": 2.1669885009481063e-05,
      "loss": 0.8318,
      "step": 32580
    },
    {
      "epoch": 1.7008232546609783,
      "grad_norm": 4.536739826202393,
      "learning_rate": 2.166118678565837e-05,
      "loss": 0.9964,
      "step": 32590
    },
    {
      "epoch": 1.701345127663183,
      "grad_norm": 4.445550441741943,
      "learning_rate": 2.1652488561835674e-05,
      "loss": 0.9342,
      "step": 32600
    },
    {
      "epoch": 1.7018670006653882,
      "grad_norm": 4.424290180206299,
      "learning_rate": 2.1643790338012977e-05,
      "loss": 0.8823,
      "step": 32610
    },
    {
      "epoch": 1.702388873667593,
      "grad_norm": 4.88515043258667,
      "learning_rate": 2.1635092114190284e-05,
      "loss": 0.8678,
      "step": 32620
    },
    {
      "epoch": 1.702910746669798,
      "grad_norm": 4.4513421058654785,
      "learning_rate": 2.1626393890367588e-05,
      "loss": 0.8346,
      "step": 32630
    },
    {
      "epoch": 1.7034326196720029,
      "grad_norm": 4.686405181884766,
      "learning_rate": 2.1617695666544894e-05,
      "loss": 0.9167,
      "step": 32640
    },
    {
      "epoch": 1.7039544926742076,
      "grad_norm": 4.195723056793213,
      "learning_rate": 2.1608997442722198e-05,
      "loss": 0.8832,
      "step": 32650
    },
    {
      "epoch": 1.7044763656764128,
      "grad_norm": 4.024808883666992,
      "learning_rate": 2.16002992188995e-05,
      "loss": 0.8937,
      "step": 32660
    },
    {
      "epoch": 1.7049982386786176,
      "grad_norm": 4.175025463104248,
      "learning_rate": 2.1591600995076805e-05,
      "loss": 0.9002,
      "step": 32670
    },
    {
      "epoch": 1.7055201116808225,
      "grad_norm": 4.012749195098877,
      "learning_rate": 2.158290277125411e-05,
      "loss": 0.8969,
      "step": 32680
    },
    {
      "epoch": 1.7060419846830275,
      "grad_norm": 3.867300271987915,
      "learning_rate": 2.1574204547431416e-05,
      "loss": 0.8906,
      "step": 32690
    },
    {
      "epoch": 1.7065638576852322,
      "grad_norm": 3.8341851234436035,
      "learning_rate": 2.156550632360872e-05,
      "loss": 0.7535,
      "step": 32700
    },
    {
      "epoch": 1.7070857306874372,
      "grad_norm": 4.747807502746582,
      "learning_rate": 2.1557677922168296e-05,
      "loss": 0.8995,
      "step": 32710
    },
    {
      "epoch": 1.7076076036896422,
      "grad_norm": 4.283434867858887,
      "learning_rate": 2.15489796983456e-05,
      "loss": 0.8088,
      "step": 32720
    },
    {
      "epoch": 1.708129476691847,
      "grad_norm": 4.0769124031066895,
      "learning_rate": 2.1540281474522903e-05,
      "loss": 0.8931,
      "step": 32730
    },
    {
      "epoch": 1.708651349694052,
      "grad_norm": 4.671130657196045,
      "learning_rate": 2.1531583250700206e-05,
      "loss": 0.8769,
      "step": 32740
    },
    {
      "epoch": 1.7091732226962568,
      "grad_norm": 4.526453018188477,
      "learning_rate": 2.1522885026877513e-05,
      "loss": 0.9134,
      "step": 32750
    },
    {
      "epoch": 1.7096950956984618,
      "grad_norm": 4.7088117599487305,
      "learning_rate": 2.1514186803054817e-05,
      "loss": 0.8107,
      "step": 32760
    },
    {
      "epoch": 1.7102169687006668,
      "grad_norm": 4.36797571182251,
      "learning_rate": 2.150548857923212e-05,
      "loss": 0.9121,
      "step": 32770
    },
    {
      "epoch": 1.7107388417028715,
      "grad_norm": 4.004418849945068,
      "learning_rate": 2.1496790355409427e-05,
      "loss": 0.9247,
      "step": 32780
    },
    {
      "epoch": 1.7112607147050767,
      "grad_norm": 3.9039392471313477,
      "learning_rate": 2.148809213158673e-05,
      "loss": 0.807,
      "step": 32790
    },
    {
      "epoch": 1.7117825877072814,
      "grad_norm": 4.3806939125061035,
      "learning_rate": 2.1479393907764038e-05,
      "loss": 0.8732,
      "step": 32800
    },
    {
      "epoch": 1.7123044607094864,
      "grad_norm": 4.179403305053711,
      "learning_rate": 2.147069568394134e-05,
      "loss": 0.8109,
      "step": 32810
    },
    {
      "epoch": 1.7128263337116914,
      "grad_norm": 4.153931617736816,
      "learning_rate": 2.1461997460118645e-05,
      "loss": 0.8337,
      "step": 32820
    },
    {
      "epoch": 1.713348206713896,
      "grad_norm": 4.56329345703125,
      "learning_rate": 2.145329923629595e-05,
      "loss": 0.8563,
      "step": 32830
    },
    {
      "epoch": 1.713870079716101,
      "grad_norm": 4.117197513580322,
      "learning_rate": 2.1444601012473252e-05,
      "loss": 0.7427,
      "step": 32840
    },
    {
      "epoch": 1.714391952718306,
      "grad_norm": 4.589719295501709,
      "learning_rate": 2.143590278865056e-05,
      "loss": 0.939,
      "step": 32850
    },
    {
      "epoch": 1.7149138257205108,
      "grad_norm": 4.933427333831787,
      "learning_rate": 2.1427204564827863e-05,
      "loss": 0.9335,
      "step": 32860
    },
    {
      "epoch": 1.715435698722716,
      "grad_norm": 4.805187225341797,
      "learning_rate": 2.141850634100517e-05,
      "loss": 0.9262,
      "step": 32870
    },
    {
      "epoch": 1.7159575717249207,
      "grad_norm": 4.731837272644043,
      "learning_rate": 2.1409808117182473e-05,
      "loss": 0.9382,
      "step": 32880
    },
    {
      "epoch": 1.7164794447271257,
      "grad_norm": 4.5754289627075195,
      "learning_rate": 2.1401109893359777e-05,
      "loss": 0.9053,
      "step": 32890
    },
    {
      "epoch": 1.7170013177293306,
      "grad_norm": 4.470219612121582,
      "learning_rate": 2.1392411669537084e-05,
      "loss": 0.8634,
      "step": 32900
    },
    {
      "epoch": 1.7175231907315354,
      "grad_norm": 4.416051864624023,
      "learning_rate": 2.1383713445714387e-05,
      "loss": 0.8767,
      "step": 32910
    },
    {
      "epoch": 1.7180450637337406,
      "grad_norm": 4.027129650115967,
      "learning_rate": 2.137501522189169e-05,
      "loss": 0.9189,
      "step": 32920
    },
    {
      "epoch": 1.7185669367359453,
      "grad_norm": 4.059096336364746,
      "learning_rate": 2.1366316998068994e-05,
      "loss": 0.8644,
      "step": 32930
    },
    {
      "epoch": 1.7190888097381503,
      "grad_norm": 4.677875518798828,
      "learning_rate": 2.1357618774246298e-05,
      "loss": 0.8029,
      "step": 32940
    },
    {
      "epoch": 1.7196106827403552,
      "grad_norm": 4.481884956359863,
      "learning_rate": 2.1348920550423605e-05,
      "loss": 0.9233,
      "step": 32950
    },
    {
      "epoch": 1.72013255574256,
      "grad_norm": 4.981136322021484,
      "learning_rate": 2.1340222326600908e-05,
      "loss": 0.8179,
      "step": 32960
    },
    {
      "epoch": 1.720654428744765,
      "grad_norm": 4.521089553833008,
      "learning_rate": 2.1331524102778215e-05,
      "loss": 0.7784,
      "step": 32970
    },
    {
      "epoch": 1.72117630174697,
      "grad_norm": 5.438846588134766,
      "learning_rate": 2.132282587895552e-05,
      "loss": 0.8273,
      "step": 32980
    },
    {
      "epoch": 1.7216981747491746,
      "grad_norm": 4.812513828277588,
      "learning_rate": 2.1314127655132822e-05,
      "loss": 0.9241,
      "step": 32990
    },
    {
      "epoch": 1.7222200477513798,
      "grad_norm": 3.6132137775421143,
      "learning_rate": 2.130542943131013e-05,
      "loss": 0.8241,
      "step": 33000
    },
    {
      "epoch": 1.7227419207535846,
      "grad_norm": 4.221011638641357,
      "learning_rate": 2.1296731207487433e-05,
      "loss": 0.8645,
      "step": 33010
    },
    {
      "epoch": 1.7232637937557895,
      "grad_norm": 4.952931880950928,
      "learning_rate": 2.128803298366474e-05,
      "loss": 0.9238,
      "step": 33020
    },
    {
      "epoch": 1.7237856667579945,
      "grad_norm": 4.086175441741943,
      "learning_rate": 2.127933475984204e-05,
      "loss": 0.9044,
      "step": 33030
    },
    {
      "epoch": 1.7243075397601992,
      "grad_norm": 3.898190498352051,
      "learning_rate": 2.1270636536019343e-05,
      "loss": 0.8885,
      "step": 33040
    },
    {
      "epoch": 1.7248294127624044,
      "grad_norm": 5.126851558685303,
      "learning_rate": 2.126193831219665e-05,
      "loss": 0.7854,
      "step": 33050
    },
    {
      "epoch": 1.7253512857646092,
      "grad_norm": 4.523890972137451,
      "learning_rate": 2.1253240088373954e-05,
      "loss": 0.9258,
      "step": 33060
    },
    {
      "epoch": 1.7258731587668141,
      "grad_norm": 4.618351936340332,
      "learning_rate": 2.124454186455126e-05,
      "loss": 0.9228,
      "step": 33070
    },
    {
      "epoch": 1.726395031769019,
      "grad_norm": 4.470690727233887,
      "learning_rate": 2.1235843640728564e-05,
      "loss": 0.8797,
      "step": 33080
    },
    {
      "epoch": 1.7269169047712238,
      "grad_norm": 4.880716323852539,
      "learning_rate": 2.1227145416905868e-05,
      "loss": 0.9111,
      "step": 33090
    },
    {
      "epoch": 1.7274387777734288,
      "grad_norm": 4.950567245483398,
      "learning_rate": 2.1218447193083175e-05,
      "loss": 0.8545,
      "step": 33100
    },
    {
      "epoch": 1.7279606507756338,
      "grad_norm": 4.355497360229492,
      "learning_rate": 2.120974896926048e-05,
      "loss": 0.8856,
      "step": 33110
    },
    {
      "epoch": 1.7284825237778385,
      "grad_norm": 4.276588439941406,
      "learning_rate": 2.1201050745437785e-05,
      "loss": 0.8607,
      "step": 33120
    },
    {
      "epoch": 1.7290043967800437,
      "grad_norm": 5.233550548553467,
      "learning_rate": 2.1192352521615085e-05,
      "loss": 0.7961,
      "step": 33130
    },
    {
      "epoch": 1.7295262697822484,
      "grad_norm": 4.980090141296387,
      "learning_rate": 2.1183654297792392e-05,
      "loss": 0.8912,
      "step": 33140
    },
    {
      "epoch": 1.7300481427844534,
      "grad_norm": 4.444425106048584,
      "learning_rate": 2.1174956073969696e-05,
      "loss": 0.9045,
      "step": 33150
    },
    {
      "epoch": 1.7305700157866584,
      "grad_norm": 4.2200751304626465,
      "learning_rate": 2.1166257850147e-05,
      "loss": 0.9772,
      "step": 33160
    },
    {
      "epoch": 1.731091888788863,
      "grad_norm": 4.116724014282227,
      "learning_rate": 2.1157559626324306e-05,
      "loss": 0.9014,
      "step": 33170
    },
    {
      "epoch": 1.7316137617910683,
      "grad_norm": 4.391144752502441,
      "learning_rate": 2.114886140250161e-05,
      "loss": 0.8811,
      "step": 33180
    },
    {
      "epoch": 1.732135634793273,
      "grad_norm": 4.7476325035095215,
      "learning_rate": 2.1140163178678914e-05,
      "loss": 0.8633,
      "step": 33190
    },
    {
      "epoch": 1.732657507795478,
      "grad_norm": 4.357847690582275,
      "learning_rate": 2.113146495485622e-05,
      "loss": 0.9387,
      "step": 33200
    },
    {
      "epoch": 1.733179380797683,
      "grad_norm": 4.372402667999268,
      "learning_rate": 2.1122766731033524e-05,
      "loss": 0.8468,
      "step": 33210
    },
    {
      "epoch": 1.7337012537998877,
      "grad_norm": 4.382437705993652,
      "learning_rate": 2.111406850721083e-05,
      "loss": 0.8921,
      "step": 33220
    },
    {
      "epoch": 1.7342231268020927,
      "grad_norm": 4.266725540161133,
      "learning_rate": 2.1105370283388134e-05,
      "loss": 0.8778,
      "step": 33230
    },
    {
      "epoch": 1.7347449998042976,
      "grad_norm": 4.62318229675293,
      "learning_rate": 2.1096672059565438e-05,
      "loss": 0.8845,
      "step": 33240
    },
    {
      "epoch": 1.7352668728065024,
      "grad_norm": 4.523634433746338,
      "learning_rate": 2.108797383574274e-05,
      "loss": 0.8406,
      "step": 33250
    },
    {
      "epoch": 1.7357887458087076,
      "grad_norm": 5.019440650939941,
      "learning_rate": 2.1079275611920045e-05,
      "loss": 0.937,
      "step": 33260
    },
    {
      "epoch": 1.7363106188109123,
      "grad_norm": 5.495645999908447,
      "learning_rate": 2.1070577388097352e-05,
      "loss": 0.8376,
      "step": 33270
    },
    {
      "epoch": 1.7368324918131173,
      "grad_norm": 5.120506763458252,
      "learning_rate": 2.1061879164274656e-05,
      "loss": 0.8165,
      "step": 33280
    },
    {
      "epoch": 1.7373543648153222,
      "grad_norm": 3.0055179595947266,
      "learning_rate": 2.1053180940451963e-05,
      "loss": 0.822,
      "step": 33290
    },
    {
      "epoch": 1.737876237817527,
      "grad_norm": 4.448794364929199,
      "learning_rate": 2.1044482716629266e-05,
      "loss": 0.8211,
      "step": 33300
    },
    {
      "epoch": 1.7383981108197322,
      "grad_norm": 4.103291034698486,
      "learning_rate": 2.103578449280657e-05,
      "loss": 0.8837,
      "step": 33310
    },
    {
      "epoch": 1.738919983821937,
      "grad_norm": 4.92669677734375,
      "learning_rate": 2.1027086268983877e-05,
      "loss": 0.9757,
      "step": 33320
    },
    {
      "epoch": 1.7394418568241419,
      "grad_norm": 4.832005977630615,
      "learning_rate": 2.101838804516118e-05,
      "loss": 0.888,
      "step": 33330
    },
    {
      "epoch": 1.7399637298263468,
      "grad_norm": 5.263591766357422,
      "learning_rate": 2.1009689821338484e-05,
      "loss": 0.9144,
      "step": 33340
    },
    {
      "epoch": 1.7404856028285516,
      "grad_norm": 4.219412803649902,
      "learning_rate": 2.1000991597515787e-05,
      "loss": 0.8157,
      "step": 33350
    },
    {
      "epoch": 1.7410074758307565,
      "grad_norm": 3.97511887550354,
      "learning_rate": 2.099229337369309e-05,
      "loss": 0.8322,
      "step": 33360
    },
    {
      "epoch": 1.7415293488329615,
      "grad_norm": 4.275241851806641,
      "learning_rate": 2.0983595149870398e-05,
      "loss": 0.8145,
      "step": 33370
    },
    {
      "epoch": 1.7420512218351663,
      "grad_norm": 4.19310998916626,
      "learning_rate": 2.09748969260477e-05,
      "loss": 0.8036,
      "step": 33380
    },
    {
      "epoch": 1.7425730948373714,
      "grad_norm": 3.8836629390716553,
      "learning_rate": 2.0966198702225008e-05,
      "loss": 0.8538,
      "step": 33390
    },
    {
      "epoch": 1.7430949678395762,
      "grad_norm": 4.36687707901001,
      "learning_rate": 2.0957500478402312e-05,
      "loss": 0.8575,
      "step": 33400
    },
    {
      "epoch": 1.7436168408417811,
      "grad_norm": 4.632702827453613,
      "learning_rate": 2.0948802254579615e-05,
      "loss": 0.9234,
      "step": 33410
    },
    {
      "epoch": 1.744138713843986,
      "grad_norm": 4.769333839416504,
      "learning_rate": 2.0940104030756922e-05,
      "loss": 0.8691,
      "step": 33420
    },
    {
      "epoch": 1.7446605868461909,
      "grad_norm": 4.373323440551758,
      "learning_rate": 2.0931405806934226e-05,
      "loss": 0.9252,
      "step": 33430
    },
    {
      "epoch": 1.745182459848396,
      "grad_norm": 4.395350456237793,
      "learning_rate": 2.092270758311153e-05,
      "loss": 0.8821,
      "step": 33440
    },
    {
      "epoch": 1.7457043328506008,
      "grad_norm": 4.109138011932373,
      "learning_rate": 2.0914009359288833e-05,
      "loss": 0.884,
      "step": 33450
    },
    {
      "epoch": 1.7462262058528057,
      "grad_norm": 4.572453498840332,
      "learning_rate": 2.0905311135466136e-05,
      "loss": 0.8535,
      "step": 33460
    },
    {
      "epoch": 1.7467480788550107,
      "grad_norm": 4.312504291534424,
      "learning_rate": 2.0896612911643443e-05,
      "loss": 0.888,
      "step": 33470
    },
    {
      "epoch": 1.7472699518572155,
      "grad_norm": 4.24976921081543,
      "learning_rate": 2.0887914687820747e-05,
      "loss": 0.8777,
      "step": 33480
    },
    {
      "epoch": 1.7477918248594204,
      "grad_norm": 4.461116790771484,
      "learning_rate": 2.0879216463998054e-05,
      "loss": 0.8219,
      "step": 33490
    },
    {
      "epoch": 1.7483136978616254,
      "grad_norm": 4.515280246734619,
      "learning_rate": 2.0870518240175357e-05,
      "loss": 0.8216,
      "step": 33500
    },
    {
      "epoch": 1.7488355708638301,
      "grad_norm": 3.947326421737671,
      "learning_rate": 2.086182001635266e-05,
      "loss": 0.7981,
      "step": 33510
    },
    {
      "epoch": 1.7493574438660353,
      "grad_norm": 4.835865497589111,
      "learning_rate": 2.0853121792529968e-05,
      "loss": 0.902,
      "step": 33520
    },
    {
      "epoch": 1.74987931686824,
      "grad_norm": 3.9184420108795166,
      "learning_rate": 2.084442356870727e-05,
      "loss": 0.8973,
      "step": 33530
    },
    {
      "epoch": 1.750401189870445,
      "grad_norm": 4.062273025512695,
      "learning_rate": 2.083572534488458e-05,
      "loss": 0.8318,
      "step": 33540
    },
    {
      "epoch": 1.75092306287265,
      "grad_norm": 4.95658016204834,
      "learning_rate": 2.082702712106188e-05,
      "loss": 0.8836,
      "step": 33550
    },
    {
      "epoch": 1.7514449358748547,
      "grad_norm": 5.727819919586182,
      "learning_rate": 2.0818328897239182e-05,
      "loss": 0.9061,
      "step": 33560
    },
    {
      "epoch": 1.75196680887706,
      "grad_norm": 3.880363702774048,
      "learning_rate": 2.080963067341649e-05,
      "loss": 0.8474,
      "step": 33570
    },
    {
      "epoch": 1.7524886818792647,
      "grad_norm": 5.200648307800293,
      "learning_rate": 2.0800932449593793e-05,
      "loss": 0.9732,
      "step": 33580
    },
    {
      "epoch": 1.7530105548814696,
      "grad_norm": 4.803839683532715,
      "learning_rate": 2.07922342257711e-05,
      "loss": 0.8566,
      "step": 33590
    },
    {
      "epoch": 1.7535324278836746,
      "grad_norm": 4.412642478942871,
      "learning_rate": 2.0783536001948403e-05,
      "loss": 0.85,
      "step": 33600
    },
    {
      "epoch": 1.7540543008858793,
      "grad_norm": 4.990494251251221,
      "learning_rate": 2.0774837778125707e-05,
      "loss": 0.858,
      "step": 33610
    },
    {
      "epoch": 1.7545761738880843,
      "grad_norm": 4.917747497558594,
      "learning_rate": 2.0766139554303014e-05,
      "loss": 0.7848,
      "step": 33620
    },
    {
      "epoch": 1.7550980468902893,
      "grad_norm": 4.711954116821289,
      "learning_rate": 2.0757441330480317e-05,
      "loss": 0.9544,
      "step": 33630
    },
    {
      "epoch": 1.755619919892494,
      "grad_norm": 4.282995700836182,
      "learning_rate": 2.0748743106657624e-05,
      "loss": 0.8753,
      "step": 33640
    },
    {
      "epoch": 1.7561417928946992,
      "grad_norm": 4.568233966827393,
      "learning_rate": 2.0740044882834924e-05,
      "loss": 0.8833,
      "step": 33650
    },
    {
      "epoch": 1.756663665896904,
      "grad_norm": 4.829364776611328,
      "learning_rate": 2.073134665901223e-05,
      "loss": 0.951,
      "step": 33660
    },
    {
      "epoch": 1.757185538899109,
      "grad_norm": 4.298947334289551,
      "learning_rate": 2.0722648435189535e-05,
      "loss": 0.8833,
      "step": 33670
    },
    {
      "epoch": 1.7577074119013139,
      "grad_norm": 3.76005482673645,
      "learning_rate": 2.0713950211366838e-05,
      "loss": 0.959,
      "step": 33680
    },
    {
      "epoch": 1.7582292849035186,
      "grad_norm": 5.098559856414795,
      "learning_rate": 2.0705251987544145e-05,
      "loss": 0.892,
      "step": 33690
    },
    {
      "epoch": 1.7587511579057238,
      "grad_norm": 4.961452484130859,
      "learning_rate": 2.069655376372145e-05,
      "loss": 0.8618,
      "step": 33700
    },
    {
      "epoch": 1.7592730309079285,
      "grad_norm": 4.285682201385498,
      "learning_rate": 2.0687855539898752e-05,
      "loss": 0.9119,
      "step": 33710
    },
    {
      "epoch": 1.7597949039101335,
      "grad_norm": 4.116184711456299,
      "learning_rate": 2.067915731607606e-05,
      "loss": 0.9641,
      "step": 33720
    },
    {
      "epoch": 1.7603167769123385,
      "grad_norm": 3.629729986190796,
      "learning_rate": 2.0670459092253363e-05,
      "loss": 0.8482,
      "step": 33730
    },
    {
      "epoch": 1.7608386499145432,
      "grad_norm": 4.451644420623779,
      "learning_rate": 2.066176086843067e-05,
      "loss": 0.9113,
      "step": 33740
    },
    {
      "epoch": 1.7613605229167482,
      "grad_norm": 4.465325355529785,
      "learning_rate": 2.0653062644607973e-05,
      "loss": 0.8442,
      "step": 33750
    },
    {
      "epoch": 1.7618823959189531,
      "grad_norm": 3.79679536819458,
      "learning_rate": 2.0644364420785277e-05,
      "loss": 0.8005,
      "step": 33760
    },
    {
      "epoch": 1.762404268921158,
      "grad_norm": 4.478224754333496,
      "learning_rate": 2.063566619696258e-05,
      "loss": 0.9008,
      "step": 33770
    },
    {
      "epoch": 1.762926141923363,
      "grad_norm": 4.67733097076416,
      "learning_rate": 2.0626967973139884e-05,
      "loss": 0.8312,
      "step": 33780
    },
    {
      "epoch": 1.7634480149255678,
      "grad_norm": 4.502527713775635,
      "learning_rate": 2.061826974931719e-05,
      "loss": 0.915,
      "step": 33790
    },
    {
      "epoch": 1.7639698879277728,
      "grad_norm": 4.376049041748047,
      "learning_rate": 2.0609571525494494e-05,
      "loss": 0.8631,
      "step": 33800
    },
    {
      "epoch": 1.7644917609299777,
      "grad_norm": 3.1476497650146484,
      "learning_rate": 2.06008733016718e-05,
      "loss": 0.831,
      "step": 33810
    },
    {
      "epoch": 1.7650136339321825,
      "grad_norm": 3.798327922821045,
      "learning_rate": 2.0592175077849105e-05,
      "loss": 0.941,
      "step": 33820
    },
    {
      "epoch": 1.7655355069343877,
      "grad_norm": 5.391599178314209,
      "learning_rate": 2.058347685402641e-05,
      "loss": 0.8646,
      "step": 33830
    },
    {
      "epoch": 1.7660573799365924,
      "grad_norm": 4.7124247550964355,
      "learning_rate": 2.0574778630203715e-05,
      "loss": 0.7941,
      "step": 33840
    },
    {
      "epoch": 1.7665792529387974,
      "grad_norm": 4.648942947387695,
      "learning_rate": 2.056608040638102e-05,
      "loss": 0.9,
      "step": 33850
    },
    {
      "epoch": 1.7671011259410023,
      "grad_norm": 5.123752117156982,
      "learning_rate": 2.0557382182558322e-05,
      "loss": 0.8541,
      "step": 33860
    },
    {
      "epoch": 1.767622998943207,
      "grad_norm": 4.318906784057617,
      "learning_rate": 2.0548683958735626e-05,
      "loss": 0.8657,
      "step": 33870
    },
    {
      "epoch": 1.768144871945412,
      "grad_norm": 4.383978366851807,
      "learning_rate": 2.053998573491293e-05,
      "loss": 0.8478,
      "step": 33880
    },
    {
      "epoch": 1.768666744947617,
      "grad_norm": 3.9584524631500244,
      "learning_rate": 2.0531287511090236e-05,
      "loss": 0.8517,
      "step": 33890
    },
    {
      "epoch": 1.769188617949822,
      "grad_norm": 4.573880195617676,
      "learning_rate": 2.052258928726754e-05,
      "loss": 0.8536,
      "step": 33900
    },
    {
      "epoch": 1.769710490952027,
      "grad_norm": 3.9705841541290283,
      "learning_rate": 2.0513891063444847e-05,
      "loss": 0.8291,
      "step": 33910
    },
    {
      "epoch": 1.7702323639542317,
      "grad_norm": 4.8420023918151855,
      "learning_rate": 2.050519283962215e-05,
      "loss": 0.8929,
      "step": 33920
    },
    {
      "epoch": 1.7707542369564366,
      "grad_norm": 3.965435743331909,
      "learning_rate": 2.0496494615799454e-05,
      "loss": 0.851,
      "step": 33930
    },
    {
      "epoch": 1.7712761099586416,
      "grad_norm": 4.658250331878662,
      "learning_rate": 2.048779639197676e-05,
      "loss": 0.867,
      "step": 33940
    },
    {
      "epoch": 1.7717979829608463,
      "grad_norm": 4.421164512634277,
      "learning_rate": 2.0479098168154065e-05,
      "loss": 0.902,
      "step": 33950
    },
    {
      "epoch": 1.7723198559630515,
      "grad_norm": 4.224760055541992,
      "learning_rate": 2.047039994433137e-05,
      "loss": 0.8062,
      "step": 33960
    },
    {
      "epoch": 1.7728417289652563,
      "grad_norm": 4.595778465270996,
      "learning_rate": 2.046170172050867e-05,
      "loss": 0.8164,
      "step": 33970
    },
    {
      "epoch": 1.7733636019674612,
      "grad_norm": 5.50383996963501,
      "learning_rate": 2.0453003496685975e-05,
      "loss": 0.8772,
      "step": 33980
    },
    {
      "epoch": 1.7738854749696662,
      "grad_norm": 5.343939781188965,
      "learning_rate": 2.0444305272863282e-05,
      "loss": 0.9054,
      "step": 33990
    },
    {
      "epoch": 1.774407347971871,
      "grad_norm": 4.284253120422363,
      "learning_rate": 2.0435607049040586e-05,
      "loss": 0.8377,
      "step": 34000
    },
    {
      "epoch": 1.774929220974076,
      "grad_norm": 4.586179733276367,
      "learning_rate": 2.0426908825217893e-05,
      "loss": 0.8156,
      "step": 34010
    },
    {
      "epoch": 1.7754510939762809,
      "grad_norm": 4.471229076385498,
      "learning_rate": 2.0418210601395196e-05,
      "loss": 0.8635,
      "step": 34020
    },
    {
      "epoch": 1.7759729669784858,
      "grad_norm": 4.872770309448242,
      "learning_rate": 2.04095123775725e-05,
      "loss": 0.8909,
      "step": 34030
    },
    {
      "epoch": 1.7764948399806908,
      "grad_norm": 4.790401935577393,
      "learning_rate": 2.0400814153749807e-05,
      "loss": 0.9343,
      "step": 34040
    },
    {
      "epoch": 1.7770167129828955,
      "grad_norm": 4.543038845062256,
      "learning_rate": 2.039211592992711e-05,
      "loss": 0.8831,
      "step": 34050
    },
    {
      "epoch": 1.7775385859851005,
      "grad_norm": 4.698968887329102,
      "learning_rate": 2.0383417706104417e-05,
      "loss": 0.9617,
      "step": 34060
    },
    {
      "epoch": 1.7780604589873055,
      "grad_norm": 4.062999725341797,
      "learning_rate": 2.0374719482281717e-05,
      "loss": 0.8503,
      "step": 34070
    },
    {
      "epoch": 1.7785823319895102,
      "grad_norm": 5.050594806671143,
      "learning_rate": 2.0366021258459024e-05,
      "loss": 0.8671,
      "step": 34080
    },
    {
      "epoch": 1.7791042049917154,
      "grad_norm": 3.9955716133117676,
      "learning_rate": 2.0357323034636328e-05,
      "loss": 0.8811,
      "step": 34090
    },
    {
      "epoch": 1.7796260779939201,
      "grad_norm": 3.9434123039245605,
      "learning_rate": 2.034862481081363e-05,
      "loss": 0.8657,
      "step": 34100
    },
    {
      "epoch": 1.780147950996125,
      "grad_norm": 4.114490985870361,
      "learning_rate": 2.0339926586990938e-05,
      "loss": 0.8491,
      "step": 34110
    },
    {
      "epoch": 1.78066982399833,
      "grad_norm": 4.3291335105896,
      "learning_rate": 2.0331228363168242e-05,
      "loss": 0.8755,
      "step": 34120
    },
    {
      "epoch": 1.7811916970005348,
      "grad_norm": 3.988638162612915,
      "learning_rate": 2.0322530139345545e-05,
      "loss": 0.8643,
      "step": 34130
    },
    {
      "epoch": 1.78171357000274,
      "grad_norm": 4.324277877807617,
      "learning_rate": 2.0313831915522852e-05,
      "loss": 0.8194,
      "step": 34140
    },
    {
      "epoch": 1.7822354430049447,
      "grad_norm": 4.535961151123047,
      "learning_rate": 2.0305133691700156e-05,
      "loss": 0.9186,
      "step": 34150
    },
    {
      "epoch": 1.7827573160071497,
      "grad_norm": 4.5899858474731445,
      "learning_rate": 2.0296435467877463e-05,
      "loss": 0.9235,
      "step": 34160
    },
    {
      "epoch": 1.7832791890093547,
      "grad_norm": 4.398530006408691,
      "learning_rate": 2.0287737244054763e-05,
      "loss": 0.8189,
      "step": 34170
    },
    {
      "epoch": 1.7838010620115594,
      "grad_norm": 5.074010848999023,
      "learning_rate": 2.027903902023207e-05,
      "loss": 0.8559,
      "step": 34180
    },
    {
      "epoch": 1.7843229350137644,
      "grad_norm": 4.640570640563965,
      "learning_rate": 2.0270340796409373e-05,
      "loss": 0.8007,
      "step": 34190
    },
    {
      "epoch": 1.7848448080159693,
      "grad_norm": 4.60498046875,
      "learning_rate": 2.0261642572586677e-05,
      "loss": 0.8575,
      "step": 34200
    },
    {
      "epoch": 1.785366681018174,
      "grad_norm": 5.672313690185547,
      "learning_rate": 2.0252944348763984e-05,
      "loss": 0.9797,
      "step": 34210
    },
    {
      "epoch": 1.7858885540203793,
      "grad_norm": 4.700933933258057,
      "learning_rate": 2.0244246124941287e-05,
      "loss": 0.8136,
      "step": 34220
    },
    {
      "epoch": 1.786410427022584,
      "grad_norm": 3.6121304035186768,
      "learning_rate": 2.0235547901118594e-05,
      "loss": 0.7576,
      "step": 34230
    },
    {
      "epoch": 1.786932300024789,
      "grad_norm": 5.4407758712768555,
      "learning_rate": 2.0226849677295898e-05,
      "loss": 0.9457,
      "step": 34240
    },
    {
      "epoch": 1.787454173026994,
      "grad_norm": 4.073922634124756,
      "learning_rate": 2.02181514534732e-05,
      "loss": 0.8649,
      "step": 34250
    },
    {
      "epoch": 1.7879760460291987,
      "grad_norm": 5.115663528442383,
      "learning_rate": 2.020945322965051e-05,
      "loss": 0.9053,
      "step": 34260
    },
    {
      "epoch": 1.7884979190314039,
      "grad_norm": 4.461368083953857,
      "learning_rate": 2.0200755005827812e-05,
      "loss": 0.8307,
      "step": 34270
    },
    {
      "epoch": 1.7890197920336086,
      "grad_norm": 4.00661039352417,
      "learning_rate": 2.0192056782005116e-05,
      "loss": 0.9553,
      "step": 34280
    },
    {
      "epoch": 1.7895416650358136,
      "grad_norm": 4.638489246368408,
      "learning_rate": 2.018335855818242e-05,
      "loss": 0.9027,
      "step": 34290
    },
    {
      "epoch": 1.7900635380380185,
      "grad_norm": 5.364394664764404,
      "learning_rate": 2.0174660334359723e-05,
      "loss": 0.9347,
      "step": 34300
    },
    {
      "epoch": 1.7905854110402233,
      "grad_norm": 5.117303371429443,
      "learning_rate": 2.016596211053703e-05,
      "loss": 0.8515,
      "step": 34310
    },
    {
      "epoch": 1.7911072840424282,
      "grad_norm": 4.9406819343566895,
      "learning_rate": 2.0157263886714333e-05,
      "loss": 0.9697,
      "step": 34320
    },
    {
      "epoch": 1.7916291570446332,
      "grad_norm": 4.188007831573486,
      "learning_rate": 2.014856566289164e-05,
      "loss": 0.8351,
      "step": 34330
    },
    {
      "epoch": 1.792151030046838,
      "grad_norm": 4.68690299987793,
      "learning_rate": 2.0139867439068944e-05,
      "loss": 0.7515,
      "step": 34340
    },
    {
      "epoch": 1.7926729030490431,
      "grad_norm": 4.932709217071533,
      "learning_rate": 2.0131169215246247e-05,
      "loss": 0.8194,
      "step": 34350
    },
    {
      "epoch": 1.7931947760512479,
      "grad_norm": 4.226227760314941,
      "learning_rate": 2.0122470991423554e-05,
      "loss": 0.8809,
      "step": 34360
    },
    {
      "epoch": 1.7937166490534528,
      "grad_norm": 4.892751693725586,
      "learning_rate": 2.0113772767600858e-05,
      "loss": 0.8953,
      "step": 34370
    },
    {
      "epoch": 1.7942385220556578,
      "grad_norm": 3.8301937580108643,
      "learning_rate": 2.010507454377816e-05,
      "loss": 0.8997,
      "step": 34380
    },
    {
      "epoch": 1.7947603950578626,
      "grad_norm": 3.951209783554077,
      "learning_rate": 2.0096376319955465e-05,
      "loss": 0.8861,
      "step": 34390
    },
    {
      "epoch": 1.7952822680600677,
      "grad_norm": 4.437810897827148,
      "learning_rate": 2.008767809613277e-05,
      "loss": 0.9341,
      "step": 34400
    },
    {
      "epoch": 1.7958041410622725,
      "grad_norm": 4.932376384735107,
      "learning_rate": 2.0078979872310075e-05,
      "loss": 0.8896,
      "step": 34410
    },
    {
      "epoch": 1.7963260140644774,
      "grad_norm": 4.351173400878906,
      "learning_rate": 2.007028164848738e-05,
      "loss": 0.8211,
      "step": 34420
    },
    {
      "epoch": 1.7968478870666824,
      "grad_norm": 4.493300437927246,
      "learning_rate": 2.0061583424664686e-05,
      "loss": 0.9016,
      "step": 34430
    },
    {
      "epoch": 1.7973697600688872,
      "grad_norm": 5.249021530151367,
      "learning_rate": 2.005288520084199e-05,
      "loss": 0.8489,
      "step": 34440
    },
    {
      "epoch": 1.7978916330710921,
      "grad_norm": 4.423837184906006,
      "learning_rate": 2.0044186977019293e-05,
      "loss": 0.8755,
      "step": 34450
    },
    {
      "epoch": 1.798413506073297,
      "grad_norm": 6.1520676612854,
      "learning_rate": 2.00354887531966e-05,
      "loss": 0.9063,
      "step": 34460
    },
    {
      "epoch": 1.7989353790755018,
      "grad_norm": 3.8460004329681396,
      "learning_rate": 2.0026790529373903e-05,
      "loss": 0.779,
      "step": 34470
    },
    {
      "epoch": 1.799457252077707,
      "grad_norm": 3.935392141342163,
      "learning_rate": 2.001809230555121e-05,
      "loss": 0.855,
      "step": 34480
    },
    {
      "epoch": 1.7999791250799118,
      "grad_norm": 5.221304893493652,
      "learning_rate": 2.000939408172851e-05,
      "loss": 0.8152,
      "step": 34490
    },
    {
      "epoch": 1.8005009980821167,
      "grad_norm": 4.626830577850342,
      "learning_rate": 2.0000695857905817e-05,
      "loss": 0.9546,
      "step": 34500
    },
    {
      "epoch": 1.8010228710843217,
      "grad_norm": 4.376445293426514,
      "learning_rate": 1.999199763408312e-05,
      "loss": 0.922,
      "step": 34510
    },
    {
      "epoch": 1.8015447440865264,
      "grad_norm": 5.006371021270752,
      "learning_rate": 1.9983299410260424e-05,
      "loss": 0.9169,
      "step": 34520
    },
    {
      "epoch": 1.8020666170887316,
      "grad_norm": 4.46021842956543,
      "learning_rate": 1.997460118643773e-05,
      "loss": 0.9708,
      "step": 34530
    },
    {
      "epoch": 1.8025884900909364,
      "grad_norm": 4.367992877960205,
      "learning_rate": 1.9965902962615035e-05,
      "loss": 0.9112,
      "step": 34540
    },
    {
      "epoch": 1.8031103630931413,
      "grad_norm": 3.65579891204834,
      "learning_rate": 1.995720473879234e-05,
      "loss": 0.8012,
      "step": 34550
    },
    {
      "epoch": 1.8036322360953463,
      "grad_norm": 4.113636016845703,
      "learning_rate": 1.9948506514969645e-05,
      "loss": 0.9324,
      "step": 34560
    },
    {
      "epoch": 1.804154109097551,
      "grad_norm": 6.18674898147583,
      "learning_rate": 1.993980829114695e-05,
      "loss": 0.8672,
      "step": 34570
    },
    {
      "epoch": 1.804675982099756,
      "grad_norm": 3.7235608100891113,
      "learning_rate": 1.9931110067324256e-05,
      "loss": 0.8297,
      "step": 34580
    },
    {
      "epoch": 1.805197855101961,
      "grad_norm": 4.719244003295898,
      "learning_rate": 1.9922411843501556e-05,
      "loss": 0.8542,
      "step": 34590
    },
    {
      "epoch": 1.8057197281041657,
      "grad_norm": 3.6520049571990967,
      "learning_rate": 1.9913713619678863e-05,
      "loss": 0.8003,
      "step": 34600
    },
    {
      "epoch": 1.8062416011063709,
      "grad_norm": 4.283812999725342,
      "learning_rate": 1.9905015395856167e-05,
      "loss": 0.9883,
      "step": 34610
    },
    {
      "epoch": 1.8067634741085756,
      "grad_norm": 3.9247524738311768,
      "learning_rate": 1.989631717203347e-05,
      "loss": 0.887,
      "step": 34620
    },
    {
      "epoch": 1.8072853471107806,
      "grad_norm": 4.796680450439453,
      "learning_rate": 1.9887618948210777e-05,
      "loss": 0.8295,
      "step": 34630
    },
    {
      "epoch": 1.8078072201129856,
      "grad_norm": 4.119104385375977,
      "learning_rate": 1.987892072438808e-05,
      "loss": 0.8654,
      "step": 34640
    },
    {
      "epoch": 1.8083290931151903,
      "grad_norm": 5.085479736328125,
      "learning_rate": 1.9870222500565384e-05,
      "loss": 0.8395,
      "step": 34650
    },
    {
      "epoch": 1.8088509661173955,
      "grad_norm": 3.866490125656128,
      "learning_rate": 1.986152427674269e-05,
      "loss": 0.8559,
      "step": 34660
    },
    {
      "epoch": 1.8093728391196002,
      "grad_norm": 4.651595592498779,
      "learning_rate": 1.9852826052919995e-05,
      "loss": 0.9167,
      "step": 34670
    },
    {
      "epoch": 1.8098947121218052,
      "grad_norm": 3.5324866771698,
      "learning_rate": 1.98441278290973e-05,
      "loss": 0.8613,
      "step": 34680
    },
    {
      "epoch": 1.8104165851240102,
      "grad_norm": 5.366532325744629,
      "learning_rate": 1.9835429605274605e-05,
      "loss": 0.8623,
      "step": 34690
    },
    {
      "epoch": 1.810938458126215,
      "grad_norm": 4.845004558563232,
      "learning_rate": 1.982673138145191e-05,
      "loss": 0.9113,
      "step": 34700
    },
    {
      "epoch": 1.8114603311284199,
      "grad_norm": 4.472628116607666,
      "learning_rate": 1.9818033157629212e-05,
      "loss": 0.9874,
      "step": 34710
    },
    {
      "epoch": 1.8119822041306248,
      "grad_norm": 3.9301247596740723,
      "learning_rate": 1.9809334933806516e-05,
      "loss": 0.8892,
      "step": 34720
    },
    {
      "epoch": 1.8125040771328296,
      "grad_norm": 4.4925127029418945,
      "learning_rate": 1.9800636709983823e-05,
      "loss": 0.8519,
      "step": 34730
    },
    {
      "epoch": 1.8130259501350348,
      "grad_norm": 4.280343532562256,
      "learning_rate": 1.9791938486161126e-05,
      "loss": 0.8129,
      "step": 34740
    },
    {
      "epoch": 1.8135478231372395,
      "grad_norm": 4.938344955444336,
      "learning_rate": 1.9783240262338433e-05,
      "loss": 0.8564,
      "step": 34750
    },
    {
      "epoch": 1.8140696961394445,
      "grad_norm": 4.146237850189209,
      "learning_rate": 1.9774542038515737e-05,
      "loss": 0.8206,
      "step": 34760
    },
    {
      "epoch": 1.8145915691416494,
      "grad_norm": 4.264082908630371,
      "learning_rate": 1.976584381469304e-05,
      "loss": 0.9192,
      "step": 34770
    },
    {
      "epoch": 1.8151134421438542,
      "grad_norm": 3.4098870754241943,
      "learning_rate": 1.9757145590870347e-05,
      "loss": 0.9131,
      "step": 34780
    },
    {
      "epoch": 1.8156353151460594,
      "grad_norm": 4.558673858642578,
      "learning_rate": 1.974844736704765e-05,
      "loss": 0.8379,
      "step": 34790
    },
    {
      "epoch": 1.816157188148264,
      "grad_norm": 4.86326265335083,
      "learning_rate": 1.9739749143224954e-05,
      "loss": 0.8712,
      "step": 34800
    },
    {
      "epoch": 1.816679061150469,
      "grad_norm": 4.654788970947266,
      "learning_rate": 1.9731050919402258e-05,
      "loss": 0.8453,
      "step": 34810
    },
    {
      "epoch": 1.817200934152674,
      "grad_norm": 3.8911783695220947,
      "learning_rate": 1.972235269557956e-05,
      "loss": 0.9387,
      "step": 34820
    },
    {
      "epoch": 1.8177228071548788,
      "grad_norm": 4.0756144523620605,
      "learning_rate": 1.971365447175687e-05,
      "loss": 0.8925,
      "step": 34830
    },
    {
      "epoch": 1.8182446801570837,
      "grad_norm": 5.474498271942139,
      "learning_rate": 1.9704956247934172e-05,
      "loss": 0.8159,
      "step": 34840
    },
    {
      "epoch": 1.8187665531592887,
      "grad_norm": 4.275674343109131,
      "learning_rate": 1.969625802411148e-05,
      "loss": 0.8646,
      "step": 34850
    },
    {
      "epoch": 1.8192884261614934,
      "grad_norm": 5.081242084503174,
      "learning_rate": 1.9687559800288782e-05,
      "loss": 0.8052,
      "step": 34860
    },
    {
      "epoch": 1.8198102991636986,
      "grad_norm": 4.92647123336792,
      "learning_rate": 1.9678861576466086e-05,
      "loss": 0.8098,
      "step": 34870
    },
    {
      "epoch": 1.8203321721659034,
      "grad_norm": 4.588618755340576,
      "learning_rate": 1.9670163352643393e-05,
      "loss": 0.9159,
      "step": 34880
    },
    {
      "epoch": 1.8208540451681083,
      "grad_norm": 3.8232944011688232,
      "learning_rate": 1.9661465128820696e-05,
      "loss": 0.8394,
      "step": 34890
    },
    {
      "epoch": 1.8213759181703133,
      "grad_norm": 4.737664222717285,
      "learning_rate": 1.9652766904998e-05,
      "loss": 0.78,
      "step": 34900
    },
    {
      "epoch": 1.821897791172518,
      "grad_norm": 4.2457051277160645,
      "learning_rate": 1.9644068681175304e-05,
      "loss": 0.7919,
      "step": 34910
    },
    {
      "epoch": 1.8224196641747232,
      "grad_norm": 4.150550365447998,
      "learning_rate": 1.9635370457352607e-05,
      "loss": 0.8691,
      "step": 34920
    },
    {
      "epoch": 1.822941537176928,
      "grad_norm": 4.678982257843018,
      "learning_rate": 1.9626672233529914e-05,
      "loss": 0.8602,
      "step": 34930
    },
    {
      "epoch": 1.823463410179133,
      "grad_norm": 4.577537536621094,
      "learning_rate": 1.9617974009707218e-05,
      "loss": 0.8864,
      "step": 34940
    },
    {
      "epoch": 1.823985283181338,
      "grad_norm": 3.8246028423309326,
      "learning_rate": 1.9609275785884524e-05,
      "loss": 0.8548,
      "step": 34950
    },
    {
      "epoch": 1.8245071561835426,
      "grad_norm": 4.424090385437012,
      "learning_rate": 1.9600577562061828e-05,
      "loss": 0.8546,
      "step": 34960
    },
    {
      "epoch": 1.8250290291857476,
      "grad_norm": 3.8742353916168213,
      "learning_rate": 1.959187933823913e-05,
      "loss": 0.8535,
      "step": 34970
    },
    {
      "epoch": 1.8255509021879526,
      "grad_norm": 3.7998716831207275,
      "learning_rate": 1.958318111441644e-05,
      "loss": 0.8035,
      "step": 34980
    },
    {
      "epoch": 1.8260727751901573,
      "grad_norm": 4.301342010498047,
      "learning_rate": 1.9574482890593742e-05,
      "loss": 0.8296,
      "step": 34990
    },
    {
      "epoch": 1.8265946481923625,
      "grad_norm": 4.486291408538818,
      "learning_rate": 1.956578466677105e-05,
      "loss": 0.8728,
      "step": 35000
    },
    {
      "epoch": 1.8271165211945672,
      "grad_norm": 4.340001106262207,
      "learning_rate": 1.955708644294835e-05,
      "loss": 0.9024,
      "step": 35010
    },
    {
      "epoch": 1.8276383941967722,
      "grad_norm": 4.407032012939453,
      "learning_rate": 1.9548388219125656e-05,
      "loss": 0.8682,
      "step": 35020
    },
    {
      "epoch": 1.8281602671989772,
      "grad_norm": 4.360724925994873,
      "learning_rate": 1.953968999530296e-05,
      "loss": 0.8691,
      "step": 35030
    },
    {
      "epoch": 1.828682140201182,
      "grad_norm": 3.5967702865600586,
      "learning_rate": 1.9530991771480263e-05,
      "loss": 0.8604,
      "step": 35040
    },
    {
      "epoch": 1.829204013203387,
      "grad_norm": 4.470441818237305,
      "learning_rate": 1.952229354765757e-05,
      "loss": 0.9243,
      "step": 35050
    },
    {
      "epoch": 1.8297258862055918,
      "grad_norm": 3.8748154640197754,
      "learning_rate": 1.9513595323834874e-05,
      "loss": 0.8902,
      "step": 35060
    },
    {
      "epoch": 1.8302477592077968,
      "grad_norm": 4.098511219024658,
      "learning_rate": 1.9504897100012177e-05,
      "loss": 0.8568,
      "step": 35070
    },
    {
      "epoch": 1.8307696322100018,
      "grad_norm": 4.024289608001709,
      "learning_rate": 1.9496198876189484e-05,
      "loss": 0.9041,
      "step": 35080
    },
    {
      "epoch": 1.8312915052122065,
      "grad_norm": 4.465190887451172,
      "learning_rate": 1.9487500652366788e-05,
      "loss": 0.9153,
      "step": 35090
    },
    {
      "epoch": 1.8318133782144115,
      "grad_norm": 4.385558128356934,
      "learning_rate": 1.9478802428544095e-05,
      "loss": 0.8622,
      "step": 35100
    },
    {
      "epoch": 1.8323352512166164,
      "grad_norm": 5.5931077003479,
      "learning_rate": 1.9470104204721395e-05,
      "loss": 0.965,
      "step": 35110
    },
    {
      "epoch": 1.8328571242188212,
      "grad_norm": 4.242404937744141,
      "learning_rate": 1.9461405980898702e-05,
      "loss": 0.8754,
      "step": 35120
    },
    {
      "epoch": 1.8333789972210264,
      "grad_norm": 4.15054178237915,
      "learning_rate": 1.9452707757076005e-05,
      "loss": 0.8207,
      "step": 35130
    },
    {
      "epoch": 1.8339008702232311,
      "grad_norm": 4.984052658081055,
      "learning_rate": 1.944400953325331e-05,
      "loss": 0.867,
      "step": 35140
    },
    {
      "epoch": 1.834422743225436,
      "grad_norm": 4.430976390838623,
      "learning_rate": 1.9435311309430616e-05,
      "loss": 0.8338,
      "step": 35150
    },
    {
      "epoch": 1.834944616227641,
      "grad_norm": 4.457293510437012,
      "learning_rate": 1.942661308560792e-05,
      "loss": 0.8488,
      "step": 35160
    },
    {
      "epoch": 1.8354664892298458,
      "grad_norm": 4.829634189605713,
      "learning_rate": 1.9417914861785226e-05,
      "loss": 0.9004,
      "step": 35170
    },
    {
      "epoch": 1.835988362232051,
      "grad_norm": 4.366313934326172,
      "learning_rate": 1.940921663796253e-05,
      "loss": 0.8885,
      "step": 35180
    },
    {
      "epoch": 1.8365102352342557,
      "grad_norm": 5.021000862121582,
      "learning_rate": 1.9400518414139833e-05,
      "loss": 0.895,
      "step": 35190
    },
    {
      "epoch": 1.8370321082364607,
      "grad_norm": 5.2217488288879395,
      "learning_rate": 1.939182019031714e-05,
      "loss": 0.8794,
      "step": 35200
    },
    {
      "epoch": 1.8375539812386656,
      "grad_norm": 4.157233715057373,
      "learning_rate": 1.9383121966494444e-05,
      "loss": 0.8289,
      "step": 35210
    },
    {
      "epoch": 1.8380758542408704,
      "grad_norm": 4.262942314147949,
      "learning_rate": 1.9374423742671747e-05,
      "loss": 0.8829,
      "step": 35220
    },
    {
      "epoch": 1.8385977272430754,
      "grad_norm": 4.842898368835449,
      "learning_rate": 1.936572551884905e-05,
      "loss": 1.0341,
      "step": 35230
    },
    {
      "epoch": 1.8391196002452803,
      "grad_norm": 5.746443748474121,
      "learning_rate": 1.9357027295026355e-05,
      "loss": 0.8429,
      "step": 35240
    },
    {
      "epoch": 1.839641473247485,
      "grad_norm": 4.627386569976807,
      "learning_rate": 1.934832907120366e-05,
      "loss": 1.022,
      "step": 35250
    },
    {
      "epoch": 1.8401633462496902,
      "grad_norm": 3.884023904800415,
      "learning_rate": 1.9339630847380965e-05,
      "loss": 0.8429,
      "step": 35260
    },
    {
      "epoch": 1.840685219251895,
      "grad_norm": 4.294251918792725,
      "learning_rate": 1.9330932623558272e-05,
      "loss": 0.9235,
      "step": 35270
    },
    {
      "epoch": 1.8412070922541,
      "grad_norm": 4.3447747230529785,
      "learning_rate": 1.9322234399735575e-05,
      "loss": 0.9154,
      "step": 35280
    },
    {
      "epoch": 1.841728965256305,
      "grad_norm": 4.479872703552246,
      "learning_rate": 1.931353617591288e-05,
      "loss": 0.8277,
      "step": 35290
    },
    {
      "epoch": 1.8422508382585097,
      "grad_norm": 4.04813814163208,
      "learning_rate": 1.9304837952090186e-05,
      "loss": 0.9124,
      "step": 35300
    },
    {
      "epoch": 1.8427727112607148,
      "grad_norm": 4.3535003662109375,
      "learning_rate": 1.929613972826749e-05,
      "loss": 0.8764,
      "step": 35310
    },
    {
      "epoch": 1.8432945842629196,
      "grad_norm": 5.244375705718994,
      "learning_rate": 1.9287441504444793e-05,
      "loss": 0.8889,
      "step": 35320
    },
    {
      "epoch": 1.8438164572651246,
      "grad_norm": 3.84555721282959,
      "learning_rate": 1.9278743280622097e-05,
      "loss": 0.788,
      "step": 35330
    },
    {
      "epoch": 1.8443383302673295,
      "grad_norm": 3.8854126930236816,
      "learning_rate": 1.92700450567994e-05,
      "loss": 0.8966,
      "step": 35340
    },
    {
      "epoch": 1.8448602032695343,
      "grad_norm": 4.352965831756592,
      "learning_rate": 1.9261346832976707e-05,
      "loss": 0.8005,
      "step": 35350
    },
    {
      "epoch": 1.8453820762717392,
      "grad_norm": 4.552047252655029,
      "learning_rate": 1.925264860915401e-05,
      "loss": 0.8606,
      "step": 35360
    },
    {
      "epoch": 1.8459039492739442,
      "grad_norm": 4.440095901489258,
      "learning_rate": 1.9243950385331318e-05,
      "loss": 0.8673,
      "step": 35370
    },
    {
      "epoch": 1.8464258222761492,
      "grad_norm": 4.540981292724609,
      "learning_rate": 1.923525216150862e-05,
      "loss": 0.8323,
      "step": 35380
    },
    {
      "epoch": 1.8469476952783541,
      "grad_norm": 4.299018859863281,
      "learning_rate": 1.9226553937685925e-05,
      "loss": 0.8111,
      "step": 35390
    },
    {
      "epoch": 1.8474695682805589,
      "grad_norm": 5.3689775466918945,
      "learning_rate": 1.921785571386323e-05,
      "loss": 1.0218,
      "step": 35400
    },
    {
      "epoch": 1.8479914412827638,
      "grad_norm": 5.513105392456055,
      "learning_rate": 1.9209157490040535e-05,
      "loss": 0.8786,
      "step": 35410
    },
    {
      "epoch": 1.8485133142849688,
      "grad_norm": 5.0444207191467285,
      "learning_rate": 1.920045926621784e-05,
      "loss": 0.8336,
      "step": 35420
    },
    {
      "epoch": 1.8490351872871735,
      "grad_norm": 4.391214370727539,
      "learning_rate": 1.9191761042395142e-05,
      "loss": 0.904,
      "step": 35430
    },
    {
      "epoch": 1.8495570602893787,
      "grad_norm": 3.2720305919647217,
      "learning_rate": 1.918306281857245e-05,
      "loss": 0.7932,
      "step": 35440
    },
    {
      "epoch": 1.8500789332915835,
      "grad_norm": 4.364068031311035,
      "learning_rate": 1.9174364594749753e-05,
      "loss": 0.9042,
      "step": 35450
    },
    {
      "epoch": 1.8506008062937884,
      "grad_norm": 4.713754653930664,
      "learning_rate": 1.9165666370927056e-05,
      "loss": 0.9469,
      "step": 35460
    },
    {
      "epoch": 1.8511226792959934,
      "grad_norm": 4.987170219421387,
      "learning_rate": 1.9156968147104363e-05,
      "loss": 0.9118,
      "step": 35470
    },
    {
      "epoch": 1.8516445522981981,
      "grad_norm": 4.462596893310547,
      "learning_rate": 1.9148269923281667e-05,
      "loss": 0.8104,
      "step": 35480
    },
    {
      "epoch": 1.852166425300403,
      "grad_norm": 4.868695259094238,
      "learning_rate": 1.913957169945897e-05,
      "loss": 0.8631,
      "step": 35490
    },
    {
      "epoch": 1.852688298302608,
      "grad_norm": 4.535022258758545,
      "learning_rate": 1.9130873475636277e-05,
      "loss": 0.9083,
      "step": 35500
    },
    {
      "epoch": 1.853210171304813,
      "grad_norm": 4.683709621429443,
      "learning_rate": 1.912217525181358e-05,
      "loss": 0.8811,
      "step": 35510
    },
    {
      "epoch": 1.853732044307018,
      "grad_norm": 4.491105079650879,
      "learning_rate": 1.9113477027990888e-05,
      "loss": 0.9214,
      "step": 35520
    },
    {
      "epoch": 1.8542539173092227,
      "grad_norm": 4.712558746337891,
      "learning_rate": 1.9104778804168188e-05,
      "loss": 0.9673,
      "step": 35530
    },
    {
      "epoch": 1.8547757903114277,
      "grad_norm": 4.006326198577881,
      "learning_rate": 1.9096080580345495e-05,
      "loss": 0.8746,
      "step": 35540
    },
    {
      "epoch": 1.8552976633136327,
      "grad_norm": 4.340251445770264,
      "learning_rate": 1.90873823565228e-05,
      "loss": 0.8764,
      "step": 35550
    },
    {
      "epoch": 1.8558195363158374,
      "grad_norm": 4.645751476287842,
      "learning_rate": 1.9078684132700102e-05,
      "loss": 0.8568,
      "step": 35560
    },
    {
      "epoch": 1.8563414093180426,
      "grad_norm": 3.936957836151123,
      "learning_rate": 1.906998590887741e-05,
      "loss": 0.923,
      "step": 35570
    },
    {
      "epoch": 1.8568632823202473,
      "grad_norm": 3.854435443878174,
      "learning_rate": 1.9061287685054712e-05,
      "loss": 0.8726,
      "step": 35580
    },
    {
      "epoch": 1.8573851553224523,
      "grad_norm": 4.269526958465576,
      "learning_rate": 1.905258946123202e-05,
      "loss": 0.9416,
      "step": 35590
    },
    {
      "epoch": 1.8579070283246573,
      "grad_norm": 4.5145158767700195,
      "learning_rate": 1.9043891237409323e-05,
      "loss": 0.8927,
      "step": 35600
    },
    {
      "epoch": 1.858428901326862,
      "grad_norm": 5.034039497375488,
      "learning_rate": 1.9035193013586626e-05,
      "loss": 0.8714,
      "step": 35610
    },
    {
      "epoch": 1.858950774329067,
      "grad_norm": 3.9809136390686035,
      "learning_rate": 1.9026494789763933e-05,
      "loss": 0.8618,
      "step": 35620
    },
    {
      "epoch": 1.859472647331272,
      "grad_norm": 4.071512699127197,
      "learning_rate": 1.9017796565941234e-05,
      "loss": 0.8595,
      "step": 35630
    },
    {
      "epoch": 1.859994520333477,
      "grad_norm": 4.705410957336426,
      "learning_rate": 1.900909834211854e-05,
      "loss": 0.8102,
      "step": 35640
    },
    {
      "epoch": 1.8605163933356819,
      "grad_norm": 5.105823516845703,
      "learning_rate": 1.9000400118295844e-05,
      "loss": 0.9016,
      "step": 35650
    },
    {
      "epoch": 1.8610382663378866,
      "grad_norm": 5.394533634185791,
      "learning_rate": 1.8991701894473148e-05,
      "loss": 0.9769,
      "step": 35660
    },
    {
      "epoch": 1.8615601393400916,
      "grad_norm": 3.9814951419830322,
      "learning_rate": 1.8983003670650455e-05,
      "loss": 0.8779,
      "step": 35670
    },
    {
      "epoch": 1.8620820123422965,
      "grad_norm": 4.072582244873047,
      "learning_rate": 1.8974305446827758e-05,
      "loss": 0.8809,
      "step": 35680
    },
    {
      "epoch": 1.8626038853445013,
      "grad_norm": 3.5541653633117676,
      "learning_rate": 1.8965607223005065e-05,
      "loss": 0.8344,
      "step": 35690
    },
    {
      "epoch": 1.8631257583467065,
      "grad_norm": 4.2491679191589355,
      "learning_rate": 1.895690899918237e-05,
      "loss": 0.8152,
      "step": 35700
    },
    {
      "epoch": 1.8636476313489112,
      "grad_norm": 4.639740943908691,
      "learning_rate": 1.8948210775359672e-05,
      "loss": 0.9904,
      "step": 35710
    },
    {
      "epoch": 1.8641695043511162,
      "grad_norm": 4.734663486480713,
      "learning_rate": 1.893951255153698e-05,
      "loss": 0.8225,
      "step": 35720
    },
    {
      "epoch": 1.8646913773533211,
      "grad_norm": 4.351414203643799,
      "learning_rate": 1.8930814327714283e-05,
      "loss": 0.8746,
      "step": 35730
    },
    {
      "epoch": 1.8652132503555259,
      "grad_norm": 4.413573265075684,
      "learning_rate": 1.8922116103891586e-05,
      "loss": 0.8659,
      "step": 35740
    },
    {
      "epoch": 1.865735123357731,
      "grad_norm": 5.267246246337891,
      "learning_rate": 1.891341788006889e-05,
      "loss": 0.8964,
      "step": 35750
    },
    {
      "epoch": 1.8662569963599358,
      "grad_norm": 4.6066203117370605,
      "learning_rate": 1.8904719656246193e-05,
      "loss": 0.8622,
      "step": 35760
    },
    {
      "epoch": 1.8667788693621408,
      "grad_norm": 4.789669036865234,
      "learning_rate": 1.88960214324235e-05,
      "loss": 0.9541,
      "step": 35770
    },
    {
      "epoch": 1.8673007423643457,
      "grad_norm": 4.067117214202881,
      "learning_rate": 1.8887323208600804e-05,
      "loss": 0.8198,
      "step": 35780
    },
    {
      "epoch": 1.8678226153665505,
      "grad_norm": 4.6758713722229,
      "learning_rate": 1.887862498477811e-05,
      "loss": 0.9039,
      "step": 35790
    },
    {
      "epoch": 1.8683444883687554,
      "grad_norm": 5.28241491317749,
      "learning_rate": 1.8869926760955414e-05,
      "loss": 0.9375,
      "step": 35800
    },
    {
      "epoch": 1.8688663613709604,
      "grad_norm": 3.730785846710205,
      "learning_rate": 1.8861228537132718e-05,
      "loss": 0.9152,
      "step": 35810
    },
    {
      "epoch": 1.8693882343731651,
      "grad_norm": 4.370660305023193,
      "learning_rate": 1.8852530313310025e-05,
      "loss": 0.8746,
      "step": 35820
    },
    {
      "epoch": 1.8699101073753703,
      "grad_norm": 4.533685684204102,
      "learning_rate": 1.8843832089487328e-05,
      "loss": 0.8283,
      "step": 35830
    },
    {
      "epoch": 1.870431980377575,
      "grad_norm": 4.086504936218262,
      "learning_rate": 1.8835133865664632e-05,
      "loss": 0.9121,
      "step": 35840
    },
    {
      "epoch": 1.87095385337978,
      "grad_norm": 4.110124588012695,
      "learning_rate": 1.8826435641841935e-05,
      "loss": 0.7971,
      "step": 35850
    },
    {
      "epoch": 1.871475726381985,
      "grad_norm": 3.9571828842163086,
      "learning_rate": 1.881773741801924e-05,
      "loss": 0.8666,
      "step": 35860
    },
    {
      "epoch": 1.8719975993841897,
      "grad_norm": 4.278980255126953,
      "learning_rate": 1.8809039194196546e-05,
      "loss": 0.8496,
      "step": 35870
    },
    {
      "epoch": 1.872519472386395,
      "grad_norm": 5.035486221313477,
      "learning_rate": 1.880034097037385e-05,
      "loss": 0.9154,
      "step": 35880
    },
    {
      "epoch": 1.8730413453885997,
      "grad_norm": 4.327139377593994,
      "learning_rate": 1.8791642746551156e-05,
      "loss": 0.9539,
      "step": 35890
    },
    {
      "epoch": 1.8735632183908046,
      "grad_norm": 4.812747478485107,
      "learning_rate": 1.878294452272846e-05,
      "loss": 0.8792,
      "step": 35900
    },
    {
      "epoch": 1.8740850913930096,
      "grad_norm": 4.305336952209473,
      "learning_rate": 1.8774246298905763e-05,
      "loss": 0.8392,
      "step": 35910
    },
    {
      "epoch": 1.8746069643952143,
      "grad_norm": 5.148357391357422,
      "learning_rate": 1.876554807508307e-05,
      "loss": 0.9299,
      "step": 35920
    },
    {
      "epoch": 1.8751288373974193,
      "grad_norm": 4.611941337585449,
      "learning_rate": 1.8756849851260374e-05,
      "loss": 0.8194,
      "step": 35930
    },
    {
      "epoch": 1.8756507103996243,
      "grad_norm": 5.246279239654541,
      "learning_rate": 1.874815162743768e-05,
      "loss": 0.8254,
      "step": 35940
    },
    {
      "epoch": 1.876172583401829,
      "grad_norm": 4.549319267272949,
      "learning_rate": 1.873945340361498e-05,
      "loss": 0.8986,
      "step": 35950
    },
    {
      "epoch": 1.8766944564040342,
      "grad_norm": 4.191241264343262,
      "learning_rate": 1.8730755179792288e-05,
      "loss": 0.8867,
      "step": 35960
    },
    {
      "epoch": 1.877216329406239,
      "grad_norm": 3.796668291091919,
      "learning_rate": 1.872205695596959e-05,
      "loss": 0.9199,
      "step": 35970
    },
    {
      "epoch": 1.877738202408444,
      "grad_norm": 4.263047695159912,
      "learning_rate": 1.8713358732146895e-05,
      "loss": 0.8,
      "step": 35980
    },
    {
      "epoch": 1.8782600754106489,
      "grad_norm": 4.232702732086182,
      "learning_rate": 1.8704660508324202e-05,
      "loss": 0.8548,
      "step": 35990
    },
    {
      "epoch": 1.8787819484128536,
      "grad_norm": 4.7479987144470215,
      "learning_rate": 1.8695962284501506e-05,
      "loss": 0.8506,
      "step": 36000
    },
    {
      "epoch": 1.8793038214150588,
      "grad_norm": 4.080903053283691,
      "learning_rate": 1.868726406067881e-05,
      "loss": 0.7857,
      "step": 36010
    },
    {
      "epoch": 1.8798256944172635,
      "grad_norm": 4.790701866149902,
      "learning_rate": 1.8678565836856116e-05,
      "loss": 0.9759,
      "step": 36020
    },
    {
      "epoch": 1.8803475674194685,
      "grad_norm": 4.900291442871094,
      "learning_rate": 1.866986761303342e-05,
      "loss": 0.9329,
      "step": 36030
    },
    {
      "epoch": 1.8808694404216735,
      "grad_norm": 4.575531959533691,
      "learning_rate": 1.8661169389210727e-05,
      "loss": 0.8818,
      "step": 36040
    },
    {
      "epoch": 1.8813913134238782,
      "grad_norm": 4.1790947914123535,
      "learning_rate": 1.8652471165388027e-05,
      "loss": 0.8401,
      "step": 36050
    },
    {
      "epoch": 1.8819131864260832,
      "grad_norm": 3.804788827896118,
      "learning_rate": 1.8643772941565334e-05,
      "loss": 0.8745,
      "step": 36060
    },
    {
      "epoch": 1.8824350594282881,
      "grad_norm": 3.968230962753296,
      "learning_rate": 1.8635074717742637e-05,
      "loss": 0.7752,
      "step": 36070
    },
    {
      "epoch": 1.882956932430493,
      "grad_norm": 3.9615941047668457,
      "learning_rate": 1.862637649391994e-05,
      "loss": 0.8065,
      "step": 36080
    },
    {
      "epoch": 1.883478805432698,
      "grad_norm": 3.5964317321777344,
      "learning_rate": 1.8617678270097248e-05,
      "loss": 0.8105,
      "step": 36090
    },
    {
      "epoch": 1.8840006784349028,
      "grad_norm": 4.178861141204834,
      "learning_rate": 1.860898004627455e-05,
      "loss": 0.8258,
      "step": 36100
    },
    {
      "epoch": 1.8845225514371078,
      "grad_norm": 4.844382286071777,
      "learning_rate": 1.8600281822451858e-05,
      "loss": 0.8873,
      "step": 36110
    },
    {
      "epoch": 1.8850444244393127,
      "grad_norm": 3.653204917907715,
      "learning_rate": 1.859158359862916e-05,
      "loss": 0.8288,
      "step": 36120
    },
    {
      "epoch": 1.8855662974415175,
      "grad_norm": 2.9973952770233154,
      "learning_rate": 1.8582885374806465e-05,
      "loss": 0.8698,
      "step": 36130
    },
    {
      "epoch": 1.8860881704437227,
      "grad_norm": 5.02204704284668,
      "learning_rate": 1.8574187150983772e-05,
      "loss": 0.9346,
      "step": 36140
    },
    {
      "epoch": 1.8866100434459274,
      "grad_norm": 4.06589937210083,
      "learning_rate": 1.8565488927161072e-05,
      "loss": 0.8357,
      "step": 36150
    },
    {
      "epoch": 1.8871319164481324,
      "grad_norm": 4.007309913635254,
      "learning_rate": 1.855679070333838e-05,
      "loss": 0.9203,
      "step": 36160
    },
    {
      "epoch": 1.8876537894503373,
      "grad_norm": 5.070856094360352,
      "learning_rate": 1.8548092479515683e-05,
      "loss": 0.8387,
      "step": 36170
    },
    {
      "epoch": 1.888175662452542,
      "grad_norm": 3.9958910942077637,
      "learning_rate": 1.8539394255692986e-05,
      "loss": 0.9247,
      "step": 36180
    },
    {
      "epoch": 1.888697535454747,
      "grad_norm": 4.6126532554626465,
      "learning_rate": 1.8530696031870293e-05,
      "loss": 0.8549,
      "step": 36190
    },
    {
      "epoch": 1.889219408456952,
      "grad_norm": 4.438258647918701,
      "learning_rate": 1.8521997808047597e-05,
      "loss": 0.8707,
      "step": 36200
    },
    {
      "epoch": 1.8897412814591568,
      "grad_norm": 4.2784647941589355,
      "learning_rate": 1.8513299584224904e-05,
      "loss": 0.8476,
      "step": 36210
    },
    {
      "epoch": 1.890263154461362,
      "grad_norm": 4.729820251464844,
      "learning_rate": 1.8504601360402207e-05,
      "loss": 0.8966,
      "step": 36220
    },
    {
      "epoch": 1.8907850274635667,
      "grad_norm": 4.159581661224365,
      "learning_rate": 1.849590313657951e-05,
      "loss": 0.823,
      "step": 36230
    },
    {
      "epoch": 1.8913069004657717,
      "grad_norm": 4.086581230163574,
      "learning_rate": 1.8487204912756818e-05,
      "loss": 0.7434,
      "step": 36240
    },
    {
      "epoch": 1.8918287734679766,
      "grad_norm": 4.257733345031738,
      "learning_rate": 1.847850668893412e-05,
      "loss": 0.8194,
      "step": 36250
    },
    {
      "epoch": 1.8923506464701814,
      "grad_norm": 4.0392303466796875,
      "learning_rate": 1.8469808465111425e-05,
      "loss": 0.8937,
      "step": 36260
    },
    {
      "epoch": 1.8928725194723865,
      "grad_norm": 4.814347267150879,
      "learning_rate": 1.846111024128873e-05,
      "loss": 0.8848,
      "step": 36270
    },
    {
      "epoch": 1.8933943924745913,
      "grad_norm": 4.574854850769043,
      "learning_rate": 1.8452412017466032e-05,
      "loss": 0.9179,
      "step": 36280
    },
    {
      "epoch": 1.8939162654767963,
      "grad_norm": 3.7900679111480713,
      "learning_rate": 1.844458361602561e-05,
      "loss": 0.8778,
      "step": 36290
    },
    {
      "epoch": 1.8944381384790012,
      "grad_norm": 4.1748762130737305,
      "learning_rate": 1.8435885392202912e-05,
      "loss": 0.837,
      "step": 36300
    },
    {
      "epoch": 1.894960011481206,
      "grad_norm": 4.77761173248291,
      "learning_rate": 1.8427187168380216e-05,
      "loss": 0.8695,
      "step": 36310
    },
    {
      "epoch": 1.895481884483411,
      "grad_norm": 4.869309425354004,
      "learning_rate": 1.8418488944557523e-05,
      "loss": 0.9017,
      "step": 36320
    },
    {
      "epoch": 1.896003757485616,
      "grad_norm": 4.4067816734313965,
      "learning_rate": 1.8409790720734826e-05,
      "loss": 0.9402,
      "step": 36330
    },
    {
      "epoch": 1.8965256304878206,
      "grad_norm": 4.890473365783691,
      "learning_rate": 1.840109249691213e-05,
      "loss": 0.9398,
      "step": 36340
    },
    {
      "epoch": 1.8970475034900258,
      "grad_norm": 4.888753414154053,
      "learning_rate": 1.8392394273089437e-05,
      "loss": 0.8251,
      "step": 36350
    },
    {
      "epoch": 1.8975693764922306,
      "grad_norm": 4.7054595947265625,
      "learning_rate": 1.838369604926674e-05,
      "loss": 0.8628,
      "step": 36360
    },
    {
      "epoch": 1.8980912494944355,
      "grad_norm": 5.033414363861084,
      "learning_rate": 1.8374997825444047e-05,
      "loss": 0.8777,
      "step": 36370
    },
    {
      "epoch": 1.8986131224966405,
      "grad_norm": 4.9955244064331055,
      "learning_rate": 1.836629960162135e-05,
      "loss": 0.8623,
      "step": 36380
    },
    {
      "epoch": 1.8991349954988452,
      "grad_norm": 4.16970157623291,
      "learning_rate": 1.8357601377798654e-05,
      "loss": 0.8855,
      "step": 36390
    },
    {
      "epoch": 1.8996568685010504,
      "grad_norm": 4.856767654418945,
      "learning_rate": 1.8348903153975958e-05,
      "loss": 0.8446,
      "step": 36400
    },
    {
      "epoch": 1.9001787415032552,
      "grad_norm": 5.203410625457764,
      "learning_rate": 1.834020493015326e-05,
      "loss": 0.7827,
      "step": 36410
    },
    {
      "epoch": 1.9007006145054601,
      "grad_norm": 4.009136199951172,
      "learning_rate": 1.8331506706330568e-05,
      "loss": 0.9013,
      "step": 36420
    },
    {
      "epoch": 1.901222487507665,
      "grad_norm": 3.7749059200286865,
      "learning_rate": 1.8322808482507872e-05,
      "loss": 0.8073,
      "step": 36430
    },
    {
      "epoch": 1.9017443605098698,
      "grad_norm": 3.695499897003174,
      "learning_rate": 1.8314110258685175e-05,
      "loss": 0.8292,
      "step": 36440
    },
    {
      "epoch": 1.9022662335120748,
      "grad_norm": 4.049537658691406,
      "learning_rate": 1.8305412034862482e-05,
      "loss": 0.8204,
      "step": 36450
    },
    {
      "epoch": 1.9027881065142798,
      "grad_norm": 4.317522048950195,
      "learning_rate": 1.8296713811039786e-05,
      "loss": 0.9493,
      "step": 36460
    },
    {
      "epoch": 1.9033099795164845,
      "grad_norm": 5.116553783416748,
      "learning_rate": 1.8288015587217093e-05,
      "loss": 0.9669,
      "step": 36470
    },
    {
      "epoch": 1.9038318525186897,
      "grad_norm": 4.513193607330322,
      "learning_rate": 1.8279317363394396e-05,
      "loss": 0.8292,
      "step": 36480
    },
    {
      "epoch": 1.9043537255208944,
      "grad_norm": 4.108364105224609,
      "learning_rate": 1.82706191395717e-05,
      "loss": 0.9775,
      "step": 36490
    },
    {
      "epoch": 1.9048755985230994,
      "grad_norm": 4.376500606536865,
      "learning_rate": 1.8261920915749007e-05,
      "loss": 0.8289,
      "step": 36500
    },
    {
      "epoch": 1.9053974715253044,
      "grad_norm": 4.888707160949707,
      "learning_rate": 1.8253222691926307e-05,
      "loss": 0.9198,
      "step": 36510
    },
    {
      "epoch": 1.905919344527509,
      "grad_norm": 3.681577444076538,
      "learning_rate": 1.8244524468103614e-05,
      "loss": 0.8471,
      "step": 36520
    },
    {
      "epoch": 1.9064412175297143,
      "grad_norm": 4.339632511138916,
      "learning_rate": 1.8235826244280917e-05,
      "loss": 0.917,
      "step": 36530
    },
    {
      "epoch": 1.906963090531919,
      "grad_norm": 5.45792818069458,
      "learning_rate": 1.8227128020458224e-05,
      "loss": 0.8393,
      "step": 36540
    },
    {
      "epoch": 1.907484963534124,
      "grad_norm": 4.9299421310424805,
      "learning_rate": 1.8218429796635528e-05,
      "loss": 0.8912,
      "step": 36550
    },
    {
      "epoch": 1.908006836536329,
      "grad_norm": 4.246283054351807,
      "learning_rate": 1.820973157281283e-05,
      "loss": 0.8788,
      "step": 36560
    },
    {
      "epoch": 1.9085287095385337,
      "grad_norm": 4.227943420410156,
      "learning_rate": 1.820103334899014e-05,
      "loss": 0.9107,
      "step": 36570
    },
    {
      "epoch": 1.9090505825407387,
      "grad_norm": 4.4101996421813965,
      "learning_rate": 1.8192335125167442e-05,
      "loss": 0.9003,
      "step": 36580
    },
    {
      "epoch": 1.9095724555429436,
      "grad_norm": 3.9653680324554443,
      "learning_rate": 1.8183636901344746e-05,
      "loss": 0.8988,
      "step": 36590
    },
    {
      "epoch": 1.9100943285451484,
      "grad_norm": 4.3569159507751465,
      "learning_rate": 1.8174938677522053e-05,
      "loss": 0.8287,
      "step": 36600
    },
    {
      "epoch": 1.9106162015473536,
      "grad_norm": 5.546977996826172,
      "learning_rate": 1.8166240453699353e-05,
      "loss": 0.8664,
      "step": 36610
    },
    {
      "epoch": 1.9111380745495583,
      "grad_norm": 4.427682399749756,
      "learning_rate": 1.815754222987666e-05,
      "loss": 0.8452,
      "step": 36620
    },
    {
      "epoch": 1.9116599475517633,
      "grad_norm": 3.6798794269561768,
      "learning_rate": 1.8148844006053963e-05,
      "loss": 0.7806,
      "step": 36630
    },
    {
      "epoch": 1.9121818205539682,
      "grad_norm": 4.532389163970947,
      "learning_rate": 1.814014578223127e-05,
      "loss": 0.8413,
      "step": 36640
    },
    {
      "epoch": 1.912703693556173,
      "grad_norm": 4.130167007446289,
      "learning_rate": 1.8131447558408574e-05,
      "loss": 0.9034,
      "step": 36650
    },
    {
      "epoch": 1.9132255665583782,
      "grad_norm": 4.571009159088135,
      "learning_rate": 1.8122749334585877e-05,
      "loss": 0.8906,
      "step": 36660
    },
    {
      "epoch": 1.913747439560583,
      "grad_norm": 5.14418363571167,
      "learning_rate": 1.8114051110763184e-05,
      "loss": 0.8546,
      "step": 36670
    },
    {
      "epoch": 1.9142693125627879,
      "grad_norm": 4.272397994995117,
      "learning_rate": 1.8105352886940488e-05,
      "loss": 0.8368,
      "step": 36680
    },
    {
      "epoch": 1.9147911855649928,
      "grad_norm": 4.120893955230713,
      "learning_rate": 1.8096654663117795e-05,
      "loss": 0.9231,
      "step": 36690
    },
    {
      "epoch": 1.9153130585671976,
      "grad_norm": 3.208308696746826,
      "learning_rate": 1.8087956439295098e-05,
      "loss": 0.8093,
      "step": 36700
    },
    {
      "epoch": 1.9158349315694025,
      "grad_norm": 3.9078569412231445,
      "learning_rate": 1.8079258215472402e-05,
      "loss": 0.967,
      "step": 36710
    },
    {
      "epoch": 1.9163568045716075,
      "grad_norm": 4.272000312805176,
      "learning_rate": 1.8070559991649705e-05,
      "loss": 0.8303,
      "step": 36720
    },
    {
      "epoch": 1.9168786775738123,
      "grad_norm": 3.9650027751922607,
      "learning_rate": 1.806186176782701e-05,
      "loss": 0.8559,
      "step": 36730
    },
    {
      "epoch": 1.9174005505760174,
      "grad_norm": 4.092687129974365,
      "learning_rate": 1.8053163544004316e-05,
      "loss": 0.8095,
      "step": 36740
    },
    {
      "epoch": 1.9179224235782222,
      "grad_norm": 4.968181610107422,
      "learning_rate": 1.804446532018162e-05,
      "loss": 0.9271,
      "step": 36750
    },
    {
      "epoch": 1.9184442965804271,
      "grad_norm": 4.925168514251709,
      "learning_rate": 1.8035767096358923e-05,
      "loss": 0.8619,
      "step": 36760
    },
    {
      "epoch": 1.918966169582632,
      "grad_norm": 3.9320497512817383,
      "learning_rate": 1.802706887253623e-05,
      "loss": 0.8331,
      "step": 36770
    },
    {
      "epoch": 1.9194880425848369,
      "grad_norm": 4.175881862640381,
      "learning_rate": 1.8018370648713533e-05,
      "loss": 0.8885,
      "step": 36780
    },
    {
      "epoch": 1.920009915587042,
      "grad_norm": 3.7502083778381348,
      "learning_rate": 1.800967242489084e-05,
      "loss": 0.8067,
      "step": 36790
    },
    {
      "epoch": 1.9205317885892468,
      "grad_norm": 4.229674816131592,
      "learning_rate": 1.8000974201068144e-05,
      "loss": 0.8634,
      "step": 36800
    },
    {
      "epoch": 1.9210536615914517,
      "grad_norm": 4.849119186401367,
      "learning_rate": 1.7992275977245447e-05,
      "loss": 0.8043,
      "step": 36810
    },
    {
      "epoch": 1.9215755345936567,
      "grad_norm": 4.536336421966553,
      "learning_rate": 1.798357775342275e-05,
      "loss": 0.8849,
      "step": 36820
    },
    {
      "epoch": 1.9220974075958615,
      "grad_norm": 4.455996990203857,
      "learning_rate": 1.7974879529600054e-05,
      "loss": 0.9843,
      "step": 36830
    },
    {
      "epoch": 1.9226192805980664,
      "grad_norm": 4.4299702644348145,
      "learning_rate": 1.796618130577736e-05,
      "loss": 0.8628,
      "step": 36840
    },
    {
      "epoch": 1.9231411536002714,
      "grad_norm": 4.920158863067627,
      "learning_rate": 1.7957483081954665e-05,
      "loss": 0.9599,
      "step": 36850
    },
    {
      "epoch": 1.9236630266024761,
      "grad_norm": 4.519651412963867,
      "learning_rate": 1.794878485813197e-05,
      "loss": 0.8294,
      "step": 36860
    },
    {
      "epoch": 1.9241848996046813,
      "grad_norm": 4.617417812347412,
      "learning_rate": 1.7940086634309275e-05,
      "loss": 0.9219,
      "step": 36870
    },
    {
      "epoch": 1.924706772606886,
      "grad_norm": 4.30791711807251,
      "learning_rate": 1.793138841048658e-05,
      "loss": 0.8777,
      "step": 36880
    },
    {
      "epoch": 1.925228645609091,
      "grad_norm": 4.305119037628174,
      "learning_rate": 1.7922690186663886e-05,
      "loss": 0.8095,
      "step": 36890
    },
    {
      "epoch": 1.925750518611296,
      "grad_norm": 4.337164402008057,
      "learning_rate": 1.791399196284119e-05,
      "loss": 0.8646,
      "step": 36900
    },
    {
      "epoch": 1.9262723916135007,
      "grad_norm": 4.679721355438232,
      "learning_rate": 1.7905293739018493e-05,
      "loss": 0.8362,
      "step": 36910
    },
    {
      "epoch": 1.926794264615706,
      "grad_norm": 5.0720319747924805,
      "learning_rate": 1.78965955151958e-05,
      "loss": 0.7882,
      "step": 36920
    },
    {
      "epoch": 1.9273161376179107,
      "grad_norm": 4.892858982086182,
      "learning_rate": 1.78878972913731e-05,
      "loss": 0.8404,
      "step": 36930
    },
    {
      "epoch": 1.9278380106201156,
      "grad_norm": 4.591266632080078,
      "learning_rate": 1.7879199067550407e-05,
      "loss": 0.8509,
      "step": 36940
    },
    {
      "epoch": 1.9283598836223206,
      "grad_norm": 4.76450777053833,
      "learning_rate": 1.787050084372771e-05,
      "loss": 0.8237,
      "step": 36950
    },
    {
      "epoch": 1.9288817566245253,
      "grad_norm": 4.879542827606201,
      "learning_rate": 1.7861802619905018e-05,
      "loss": 0.8653,
      "step": 36960
    },
    {
      "epoch": 1.9294036296267303,
      "grad_norm": 4.354860305786133,
      "learning_rate": 1.785310439608232e-05,
      "loss": 0.8513,
      "step": 36970
    },
    {
      "epoch": 1.9299255026289353,
      "grad_norm": 4.145888805389404,
      "learning_rate": 1.7844406172259625e-05,
      "loss": 0.8878,
      "step": 36980
    },
    {
      "epoch": 1.9304473756311402,
      "grad_norm": 4.7804718017578125,
      "learning_rate": 1.783570794843693e-05,
      "loss": 0.9559,
      "step": 36990
    },
    {
      "epoch": 1.9309692486333452,
      "grad_norm": 4.475903034210205,
      "learning_rate": 1.7827009724614235e-05,
      "loss": 0.7856,
      "step": 37000
    },
    {
      "epoch": 1.93149112163555,
      "grad_norm": 4.678876876831055,
      "learning_rate": 1.781831150079154e-05,
      "loss": 0.9556,
      "step": 37010
    },
    {
      "epoch": 1.9320129946377549,
      "grad_norm": 5.342745780944824,
      "learning_rate": 1.7809613276968846e-05,
      "loss": 0.8273,
      "step": 37020
    },
    {
      "epoch": 1.9325348676399599,
      "grad_norm": 5.061808109283447,
      "learning_rate": 1.7800915053146146e-05,
      "loss": 0.8216,
      "step": 37030
    },
    {
      "epoch": 1.9330567406421646,
      "grad_norm": 4.399825572967529,
      "learning_rate": 1.7792216829323453e-05,
      "loss": 0.9163,
      "step": 37040
    },
    {
      "epoch": 1.9335786136443698,
      "grad_norm": 3.9478373527526855,
      "learning_rate": 1.7783518605500756e-05,
      "loss": 0.8888,
      "step": 37050
    },
    {
      "epoch": 1.9341004866465745,
      "grad_norm": 4.68644905090332,
      "learning_rate": 1.7774820381678063e-05,
      "loss": 0.905,
      "step": 37060
    },
    {
      "epoch": 1.9346223596487795,
      "grad_norm": 5.140381813049316,
      "learning_rate": 1.7766122157855367e-05,
      "loss": 1.0146,
      "step": 37070
    },
    {
      "epoch": 1.9351442326509845,
      "grad_norm": 4.071829319000244,
      "learning_rate": 1.775742393403267e-05,
      "loss": 0.8995,
      "step": 37080
    },
    {
      "epoch": 1.9356661056531892,
      "grad_norm": 4.249318599700928,
      "learning_rate": 1.7748725710209977e-05,
      "loss": 0.8906,
      "step": 37090
    },
    {
      "epoch": 1.9361879786553942,
      "grad_norm": 4.631140232086182,
      "learning_rate": 1.774002748638728e-05,
      "loss": 0.8608,
      "step": 37100
    },
    {
      "epoch": 1.9367098516575991,
      "grad_norm": 4.166442394256592,
      "learning_rate": 1.7731329262564588e-05,
      "loss": 0.8699,
      "step": 37110
    },
    {
      "epoch": 1.937231724659804,
      "grad_norm": 4.776159763336182,
      "learning_rate": 1.772263103874189e-05,
      "loss": 0.8344,
      "step": 37120
    },
    {
      "epoch": 1.937753597662009,
      "grad_norm": 4.890894889831543,
      "learning_rate": 1.771393281491919e-05,
      "loss": 0.9111,
      "step": 37130
    },
    {
      "epoch": 1.9382754706642138,
      "grad_norm": 4.638399124145508,
      "learning_rate": 1.77052345910965e-05,
      "loss": 0.9236,
      "step": 37140
    },
    {
      "epoch": 1.9387973436664188,
      "grad_norm": 3.9281177520751953,
      "learning_rate": 1.7696536367273802e-05,
      "loss": 0.8052,
      "step": 37150
    },
    {
      "epoch": 1.9393192166686237,
      "grad_norm": 4.742191791534424,
      "learning_rate": 1.768783814345111e-05,
      "loss": 0.7999,
      "step": 37160
    },
    {
      "epoch": 1.9398410896708285,
      "grad_norm": 4.647989749908447,
      "learning_rate": 1.7679139919628412e-05,
      "loss": 0.9237,
      "step": 37170
    },
    {
      "epoch": 1.9403629626730337,
      "grad_norm": 3.529801368713379,
      "learning_rate": 1.7670441695805716e-05,
      "loss": 0.7892,
      "step": 37180
    },
    {
      "epoch": 1.9408848356752384,
      "grad_norm": 5.050379276275635,
      "learning_rate": 1.7661743471983023e-05,
      "loss": 0.825,
      "step": 37190
    },
    {
      "epoch": 1.9414067086774434,
      "grad_norm": 4.985114097595215,
      "learning_rate": 1.7653045248160326e-05,
      "loss": 0.9072,
      "step": 37200
    },
    {
      "epoch": 1.9419285816796483,
      "grad_norm": 4.3675761222839355,
      "learning_rate": 1.7644347024337633e-05,
      "loss": 0.8387,
      "step": 37210
    },
    {
      "epoch": 1.942450454681853,
      "grad_norm": 4.224120616912842,
      "learning_rate": 1.7635648800514937e-05,
      "loss": 0.8741,
      "step": 37220
    },
    {
      "epoch": 1.942972327684058,
      "grad_norm": 3.587632179260254,
      "learning_rate": 1.762695057669224e-05,
      "loss": 0.8192,
      "step": 37230
    },
    {
      "epoch": 1.943494200686263,
      "grad_norm": 4.833042621612549,
      "learning_rate": 1.7618252352869544e-05,
      "loss": 0.9201,
      "step": 37240
    },
    {
      "epoch": 1.944016073688468,
      "grad_norm": 4.385997772216797,
      "learning_rate": 1.7609554129046848e-05,
      "loss": 0.9364,
      "step": 37250
    },
    {
      "epoch": 1.944537946690673,
      "grad_norm": 5.139562606811523,
      "learning_rate": 1.7600855905224154e-05,
      "loss": 0.8877,
      "step": 37260
    },
    {
      "epoch": 1.9450598196928777,
      "grad_norm": 5.302883148193359,
      "learning_rate": 1.7592157681401458e-05,
      "loss": 0.8827,
      "step": 37270
    },
    {
      "epoch": 1.9455816926950826,
      "grad_norm": 3.487880229949951,
      "learning_rate": 1.758345945757876e-05,
      "loss": 0.8731,
      "step": 37280
    },
    {
      "epoch": 1.9461035656972876,
      "grad_norm": 4.462958335876465,
      "learning_rate": 1.757476123375607e-05,
      "loss": 0.894,
      "step": 37290
    },
    {
      "epoch": 1.9466254386994923,
      "grad_norm": 3.6538071632385254,
      "learning_rate": 1.7566063009933372e-05,
      "loss": 0.9566,
      "step": 37300
    },
    {
      "epoch": 1.9471473117016975,
      "grad_norm": 4.544818878173828,
      "learning_rate": 1.755736478611068e-05,
      "loss": 0.9319,
      "step": 37310
    },
    {
      "epoch": 1.9476691847039023,
      "grad_norm": 4.396784782409668,
      "learning_rate": 1.7548666562287983e-05,
      "loss": 0.7036,
      "step": 37320
    },
    {
      "epoch": 1.9481910577061072,
      "grad_norm": 4.45294189453125,
      "learning_rate": 1.7539968338465286e-05,
      "loss": 0.8471,
      "step": 37330
    },
    {
      "epoch": 1.9487129307083122,
      "grad_norm": 4.963863372802734,
      "learning_rate": 1.753127011464259e-05,
      "loss": 0.8816,
      "step": 37340
    },
    {
      "epoch": 1.949234803710517,
      "grad_norm": 4.355977535247803,
      "learning_rate": 1.7522571890819893e-05,
      "loss": 0.9283,
      "step": 37350
    },
    {
      "epoch": 1.9497566767127221,
      "grad_norm": 4.074745178222656,
      "learning_rate": 1.75138736669972e-05,
      "loss": 0.8491,
      "step": 37360
    },
    {
      "epoch": 1.9502785497149269,
      "grad_norm": 3.678938627243042,
      "learning_rate": 1.7505175443174504e-05,
      "loss": 0.7836,
      "step": 37370
    },
    {
      "epoch": 1.9508004227171318,
      "grad_norm": 4.130949974060059,
      "learning_rate": 1.749647721935181e-05,
      "loss": 0.9358,
      "step": 37380
    },
    {
      "epoch": 1.9513222957193368,
      "grad_norm": 4.039055347442627,
      "learning_rate": 1.7487778995529114e-05,
      "loss": 0.8873,
      "step": 37390
    },
    {
      "epoch": 1.9518441687215415,
      "grad_norm": 3.953789234161377,
      "learning_rate": 1.7479080771706418e-05,
      "loss": 0.8243,
      "step": 37400
    },
    {
      "epoch": 1.9523660417237465,
      "grad_norm": 4.994103908538818,
      "learning_rate": 1.7470382547883725e-05,
      "loss": 0.9015,
      "step": 37410
    },
    {
      "epoch": 1.9528879147259515,
      "grad_norm": 4.51550817489624,
      "learning_rate": 1.7461684324061028e-05,
      "loss": 0.8317,
      "step": 37420
    },
    {
      "epoch": 1.9534097877281562,
      "grad_norm": 4.744054317474365,
      "learning_rate": 1.7452986100238332e-05,
      "loss": 0.9127,
      "step": 37430
    },
    {
      "epoch": 1.9539316607303614,
      "grad_norm": 4.3795881271362305,
      "learning_rate": 1.744428787641564e-05,
      "loss": 1.0139,
      "step": 37440
    },
    {
      "epoch": 1.9544535337325661,
      "grad_norm": 4.8404860496521,
      "learning_rate": 1.743558965259294e-05,
      "loss": 0.9489,
      "step": 37450
    },
    {
      "epoch": 1.954975406734771,
      "grad_norm": 4.158383846282959,
      "learning_rate": 1.7426891428770246e-05,
      "loss": 0.9163,
      "step": 37460
    },
    {
      "epoch": 1.955497279736976,
      "grad_norm": 4.8628058433532715,
      "learning_rate": 1.741819320494755e-05,
      "loss": 0.904,
      "step": 37470
    },
    {
      "epoch": 1.9560191527391808,
      "grad_norm": 4.796501636505127,
      "learning_rate": 1.7409494981124856e-05,
      "loss": 0.9632,
      "step": 37480
    },
    {
      "epoch": 1.956541025741386,
      "grad_norm": 4.880764961242676,
      "learning_rate": 1.740079675730216e-05,
      "loss": 0.9085,
      "step": 37490
    },
    {
      "epoch": 1.9570628987435907,
      "grad_norm": 4.4203267097473145,
      "learning_rate": 1.7392098533479463e-05,
      "loss": 0.8842,
      "step": 37500
    },
    {
      "epoch": 1.9575847717457957,
      "grad_norm": 4.523249626159668,
      "learning_rate": 1.738340030965677e-05,
      "loss": 0.8996,
      "step": 37510
    },
    {
      "epoch": 1.9581066447480007,
      "grad_norm": 4.841414928436279,
      "learning_rate": 1.7374702085834074e-05,
      "loss": 0.9218,
      "step": 37520
    },
    {
      "epoch": 1.9586285177502054,
      "grad_norm": 4.10693883895874,
      "learning_rate": 1.7366003862011377e-05,
      "loss": 0.892,
      "step": 37530
    },
    {
      "epoch": 1.9591503907524104,
      "grad_norm": 4.252422332763672,
      "learning_rate": 1.7357305638188684e-05,
      "loss": 0.8268,
      "step": 37540
    },
    {
      "epoch": 1.9596722637546153,
      "grad_norm": 4.417975425720215,
      "learning_rate": 1.7348607414365985e-05,
      "loss": 0.8283,
      "step": 37550
    },
    {
      "epoch": 1.96019413675682,
      "grad_norm": 4.957663059234619,
      "learning_rate": 1.733990919054329e-05,
      "loss": 0.8706,
      "step": 37560
    },
    {
      "epoch": 1.9607160097590253,
      "grad_norm": 4.383186340332031,
      "learning_rate": 1.7331210966720595e-05,
      "loss": 0.8605,
      "step": 37570
    },
    {
      "epoch": 1.96123788276123,
      "grad_norm": 4.541569232940674,
      "learning_rate": 1.7322512742897902e-05,
      "loss": 0.9108,
      "step": 37580
    },
    {
      "epoch": 1.961759755763435,
      "grad_norm": 4.483466625213623,
      "learning_rate": 1.7313814519075205e-05,
      "loss": 0.8238,
      "step": 37590
    },
    {
      "epoch": 1.96228162876564,
      "grad_norm": 4.2949090003967285,
      "learning_rate": 1.730511629525251e-05,
      "loss": 0.8662,
      "step": 37600
    },
    {
      "epoch": 1.9628035017678447,
      "grad_norm": 5.687070369720459,
      "learning_rate": 1.7296418071429816e-05,
      "loss": 0.9146,
      "step": 37610
    },
    {
      "epoch": 1.9633253747700499,
      "grad_norm": 5.043203830718994,
      "learning_rate": 1.728771984760712e-05,
      "loss": 1.0275,
      "step": 37620
    },
    {
      "epoch": 1.9638472477722546,
      "grad_norm": 2.978229522705078,
      "learning_rate": 1.7279021623784426e-05,
      "loss": 0.841,
      "step": 37630
    },
    {
      "epoch": 1.9643691207744596,
      "grad_norm": 4.0681986808776855,
      "learning_rate": 1.727032339996173e-05,
      "loss": 0.8873,
      "step": 37640
    },
    {
      "epoch": 1.9648909937766645,
      "grad_norm": 5.417431354522705,
      "learning_rate": 1.726162517613903e-05,
      "loss": 0.8877,
      "step": 37650
    },
    {
      "epoch": 1.9654128667788693,
      "grad_norm": 4.8772172927856445,
      "learning_rate": 1.7252926952316337e-05,
      "loss": 0.8853,
      "step": 37660
    },
    {
      "epoch": 1.9659347397810742,
      "grad_norm": 4.735421180725098,
      "learning_rate": 1.724422872849364e-05,
      "loss": 0.8765,
      "step": 37670
    },
    {
      "epoch": 1.9664566127832792,
      "grad_norm": 4.587630748748779,
      "learning_rate": 1.7235530504670948e-05,
      "loss": 0.8121,
      "step": 37680
    },
    {
      "epoch": 1.966978485785484,
      "grad_norm": 4.948021411895752,
      "learning_rate": 1.722683228084825e-05,
      "loss": 0.8932,
      "step": 37690
    },
    {
      "epoch": 1.9675003587876891,
      "grad_norm": 4.526338577270508,
      "learning_rate": 1.7218134057025555e-05,
      "loss": 0.8619,
      "step": 37700
    },
    {
      "epoch": 1.9680222317898939,
      "grad_norm": 4.506092548370361,
      "learning_rate": 1.720943583320286e-05,
      "loss": 0.9179,
      "step": 37710
    },
    {
      "epoch": 1.9685441047920988,
      "grad_norm": 5.498577117919922,
      "learning_rate": 1.7200737609380165e-05,
      "loss": 0.983,
      "step": 37720
    },
    {
      "epoch": 1.9690659777943038,
      "grad_norm": 4.163431644439697,
      "learning_rate": 1.7192039385557472e-05,
      "loss": 0.8224,
      "step": 37730
    },
    {
      "epoch": 1.9695878507965086,
      "grad_norm": 3.770496368408203,
      "learning_rate": 1.7183341161734776e-05,
      "loss": 0.8775,
      "step": 37740
    },
    {
      "epoch": 1.9701097237987137,
      "grad_norm": 5.476680755615234,
      "learning_rate": 1.717464293791208e-05,
      "loss": 0.8773,
      "step": 37750
    },
    {
      "epoch": 1.9706315968009185,
      "grad_norm": 3.3528969287872314,
      "learning_rate": 1.7165944714089383e-05,
      "loss": 0.796,
      "step": 37760
    },
    {
      "epoch": 1.9711534698031234,
      "grad_norm": 4.4217047691345215,
      "learning_rate": 1.7157246490266686e-05,
      "loss": 0.9234,
      "step": 37770
    },
    {
      "epoch": 1.9716753428053284,
      "grad_norm": 4.767432689666748,
      "learning_rate": 1.7148548266443993e-05,
      "loss": 0.8627,
      "step": 37780
    },
    {
      "epoch": 1.9721972158075332,
      "grad_norm": 4.03415584564209,
      "learning_rate": 1.7139850042621297e-05,
      "loss": 0.8176,
      "step": 37790
    },
    {
      "epoch": 1.9727190888097381,
      "grad_norm": 4.000103950500488,
      "learning_rate": 1.71311518187986e-05,
      "loss": 0.8334,
      "step": 37800
    },
    {
      "epoch": 1.973240961811943,
      "grad_norm": 4.458095073699951,
      "learning_rate": 1.7122453594975907e-05,
      "loss": 0.8256,
      "step": 37810
    },
    {
      "epoch": 1.9737628348141478,
      "grad_norm": 4.275491237640381,
      "learning_rate": 1.711375537115321e-05,
      "loss": 0.8959,
      "step": 37820
    },
    {
      "epoch": 1.974284707816353,
      "grad_norm": 4.666067123413086,
      "learning_rate": 1.7105057147330518e-05,
      "loss": 0.8089,
      "step": 37830
    },
    {
      "epoch": 1.9748065808185578,
      "grad_norm": 4.698145866394043,
      "learning_rate": 1.709635892350782e-05,
      "loss": 0.8525,
      "step": 37840
    },
    {
      "epoch": 1.9753284538207627,
      "grad_norm": 4.28782320022583,
      "learning_rate": 1.7087660699685125e-05,
      "loss": 0.7595,
      "step": 37850
    },
    {
      "epoch": 1.9758503268229677,
      "grad_norm": 4.5539703369140625,
      "learning_rate": 1.707896247586243e-05,
      "loss": 0.8482,
      "step": 37860
    },
    {
      "epoch": 1.9763721998251724,
      "grad_norm": 4.166711330413818,
      "learning_rate": 1.7070264252039732e-05,
      "loss": 0.8698,
      "step": 37870
    },
    {
      "epoch": 1.9768940728273776,
      "grad_norm": 4.576975345611572,
      "learning_rate": 1.706156602821704e-05,
      "loss": 0.7889,
      "step": 37880
    },
    {
      "epoch": 1.9774159458295824,
      "grad_norm": 4.681501388549805,
      "learning_rate": 1.7052867804394342e-05,
      "loss": 0.9242,
      "step": 37890
    },
    {
      "epoch": 1.9779378188317873,
      "grad_norm": 4.463043689727783,
      "learning_rate": 1.704416958057165e-05,
      "loss": 0.9652,
      "step": 37900
    },
    {
      "epoch": 1.9784596918339923,
      "grad_norm": 4.534258842468262,
      "learning_rate": 1.7035471356748953e-05,
      "loss": 0.8673,
      "step": 37910
    },
    {
      "epoch": 1.978981564836197,
      "grad_norm": 3.5104987621307373,
      "learning_rate": 1.7026773132926256e-05,
      "loss": 0.9163,
      "step": 37920
    },
    {
      "epoch": 1.979503437838402,
      "grad_norm": 4.432376384735107,
      "learning_rate": 1.7018074909103563e-05,
      "loss": 0.8885,
      "step": 37930
    },
    {
      "epoch": 1.980025310840607,
      "grad_norm": 5.085388660430908,
      "learning_rate": 1.7009376685280867e-05,
      "loss": 0.7919,
      "step": 37940
    },
    {
      "epoch": 1.9805471838428117,
      "grad_norm": 4.559131622314453,
      "learning_rate": 1.700067846145817e-05,
      "loss": 0.8972,
      "step": 37950
    },
    {
      "epoch": 1.9810690568450169,
      "grad_norm": 4.294698715209961,
      "learning_rate": 1.6991980237635477e-05,
      "loss": 0.8787,
      "step": 37960
    },
    {
      "epoch": 1.9815909298472216,
      "grad_norm": 4.628641128540039,
      "learning_rate": 1.6983282013812778e-05,
      "loss": 0.9126,
      "step": 37970
    },
    {
      "epoch": 1.9821128028494266,
      "grad_norm": 4.4557037353515625,
      "learning_rate": 1.6974583789990085e-05,
      "loss": 0.7993,
      "step": 37980
    },
    {
      "epoch": 1.9826346758516316,
      "grad_norm": 3.9851889610290527,
      "learning_rate": 1.6965885566167388e-05,
      "loss": 0.7561,
      "step": 37990
    },
    {
      "epoch": 1.9831565488538363,
      "grad_norm": 3.8458681106567383,
      "learning_rate": 1.6957187342344695e-05,
      "loss": 0.8636,
      "step": 38000
    },
    {
      "epoch": 1.9836784218560415,
      "grad_norm": 5.05225944519043,
      "learning_rate": 1.6948489118522e-05,
      "loss": 0.9217,
      "step": 38010
    },
    {
      "epoch": 1.9842002948582462,
      "grad_norm": 4.9884138107299805,
      "learning_rate": 1.6939790894699302e-05,
      "loss": 0.9167,
      "step": 38020
    },
    {
      "epoch": 1.9847221678604512,
      "grad_norm": 3.4436936378479004,
      "learning_rate": 1.693109267087661e-05,
      "loss": 0.7566,
      "step": 38030
    },
    {
      "epoch": 1.9852440408626562,
      "grad_norm": 5.332721710205078,
      "learning_rate": 1.6922394447053913e-05,
      "loss": 0.8891,
      "step": 38040
    },
    {
      "epoch": 1.985765913864861,
      "grad_norm": 4.950900077819824,
      "learning_rate": 1.691369622323122e-05,
      "loss": 0.8479,
      "step": 38050
    },
    {
      "epoch": 1.9862877868670659,
      "grad_norm": 4.114470481872559,
      "learning_rate": 1.6904997999408523e-05,
      "loss": 0.9183,
      "step": 38060
    },
    {
      "epoch": 1.9868096598692708,
      "grad_norm": 3.385753870010376,
      "learning_rate": 1.6896299775585823e-05,
      "loss": 0.7463,
      "step": 38070
    },
    {
      "epoch": 1.9873315328714756,
      "grad_norm": 3.9690799713134766,
      "learning_rate": 1.688760155176313e-05,
      "loss": 0.8514,
      "step": 38080
    },
    {
      "epoch": 1.9878534058736808,
      "grad_norm": 4.567313194274902,
      "learning_rate": 1.6878903327940434e-05,
      "loss": 0.8283,
      "step": 38090
    },
    {
      "epoch": 1.9883752788758855,
      "grad_norm": 4.811189651489258,
      "learning_rate": 1.687020510411774e-05,
      "loss": 0.8237,
      "step": 38100
    },
    {
      "epoch": 1.9888971518780905,
      "grad_norm": 3.978425979614258,
      "learning_rate": 1.6861506880295044e-05,
      "loss": 0.7822,
      "step": 38110
    },
    {
      "epoch": 1.9894190248802954,
      "grad_norm": 3.979038953781128,
      "learning_rate": 1.6852808656472348e-05,
      "loss": 0.9407,
      "step": 38120
    },
    {
      "epoch": 1.9899408978825002,
      "grad_norm": 4.590608596801758,
      "learning_rate": 1.6844110432649655e-05,
      "loss": 0.901,
      "step": 38130
    },
    {
      "epoch": 1.9904627708847054,
      "grad_norm": 4.374840259552002,
      "learning_rate": 1.6835412208826958e-05,
      "loss": 0.7998,
      "step": 38140
    },
    {
      "epoch": 1.99098464388691,
      "grad_norm": 5.107889175415039,
      "learning_rate": 1.6826713985004265e-05,
      "loss": 0.8706,
      "step": 38150
    },
    {
      "epoch": 1.991506516889115,
      "grad_norm": 3.4037535190582275,
      "learning_rate": 1.681801576118157e-05,
      "loss": 0.8661,
      "step": 38160
    },
    {
      "epoch": 1.99202838989132,
      "grad_norm": 4.7213311195373535,
      "learning_rate": 1.6809317537358872e-05,
      "loss": 0.9039,
      "step": 38170
    },
    {
      "epoch": 1.9925502628935248,
      "grad_norm": 5.371721267700195,
      "learning_rate": 1.6800619313536176e-05,
      "loss": 0.8349,
      "step": 38180
    },
    {
      "epoch": 1.9930721358957297,
      "grad_norm": 4.6564459800720215,
      "learning_rate": 1.679192108971348e-05,
      "loss": 0.7979,
      "step": 38190
    },
    {
      "epoch": 1.9935940088979347,
      "grad_norm": 4.300957679748535,
      "learning_rate": 1.6783222865890786e-05,
      "loss": 0.8806,
      "step": 38200
    },
    {
      "epoch": 1.9941158819001394,
      "grad_norm": 4.371217250823975,
      "learning_rate": 1.677452464206809e-05,
      "loss": 0.9574,
      "step": 38210
    },
    {
      "epoch": 1.9946377549023446,
      "grad_norm": 3.4002392292022705,
      "learning_rate": 1.6765826418245393e-05,
      "loss": 0.854,
      "step": 38220
    },
    {
      "epoch": 1.9951596279045494,
      "grad_norm": 3.751476526260376,
      "learning_rate": 1.67571281944227e-05,
      "loss": 0.8302,
      "step": 38230
    },
    {
      "epoch": 1.9956815009067543,
      "grad_norm": 5.045434951782227,
      "learning_rate": 1.6748429970600004e-05,
      "loss": 0.8883,
      "step": 38240
    },
    {
      "epoch": 1.9962033739089593,
      "grad_norm": 5.21339225769043,
      "learning_rate": 1.673973174677731e-05,
      "loss": 0.8186,
      "step": 38250
    },
    {
      "epoch": 1.996725246911164,
      "grad_norm": 5.373865604400635,
      "learning_rate": 1.6731033522954614e-05,
      "loss": 0.8951,
      "step": 38260
    },
    {
      "epoch": 1.9972471199133692,
      "grad_norm": 4.533215522766113,
      "learning_rate": 1.6722335299131918e-05,
      "loss": 0.824,
      "step": 38270
    },
    {
      "epoch": 1.997768992915574,
      "grad_norm": 5.196887493133545,
      "learning_rate": 1.671363707530922e-05,
      "loss": 0.9309,
      "step": 38280
    },
    {
      "epoch": 1.998290865917779,
      "grad_norm": 4.908480167388916,
      "learning_rate": 1.6704938851486525e-05,
      "loss": 0.8438,
      "step": 38290
    },
    {
      "epoch": 1.998812738919984,
      "grad_norm": 3.5674476623535156,
      "learning_rate": 1.66971104500461e-05,
      "loss": 0.8875,
      "step": 38300
    },
    {
      "epoch": 1.9993346119221886,
      "grad_norm": 4.489586353302002,
      "learning_rate": 1.668841222622341e-05,
      "loss": 0.9225,
      "step": 38310
    },
    {
      "epoch": 1.9998564849243936,
      "grad_norm": 5.00626802444458,
      "learning_rate": 1.667971400240071e-05,
      "loss": 0.8841,
      "step": 38320
    },
    {
      "epoch": 1.9999608595248346,
      "eval_loss": 0.8694379925727844,
      "eval_runtime": 1019.8732,
      "eval_samples_per_second": 75.153,
      "eval_steps_per_second": 18.789,
      "step": 38322
    },
    {
      "epoch": 2.000417498401764,
      "grad_norm": 4.309261798858643,
      "learning_rate": 1.6671015778578016e-05,
      "loss": 0.9729,
      "step": 38330
    },
    {
      "epoch": 2.0009393714039687,
      "grad_norm": 5.250616073608398,
      "learning_rate": 1.666231755475532e-05,
      "loss": 0.8886,
      "step": 38340
    },
    {
      "epoch": 2.001461244406174,
      "grad_norm": 5.080410003662109,
      "learning_rate": 1.6653619330932623e-05,
      "loss": 0.8473,
      "step": 38350
    },
    {
      "epoch": 2.0019831174083786,
      "grad_norm": 4.991974830627441,
      "learning_rate": 1.664492110710993e-05,
      "loss": 0.8939,
      "step": 38360
    },
    {
      "epoch": 2.0025049904105834,
      "grad_norm": 4.749435901641846,
      "learning_rate": 1.6636222883287233e-05,
      "loss": 0.8467,
      "step": 38370
    },
    {
      "epoch": 2.0030268634127886,
      "grad_norm": 4.87331485748291,
      "learning_rate": 1.6627524659464537e-05,
      "loss": 0.876,
      "step": 38380
    },
    {
      "epoch": 2.0035487364149933,
      "grad_norm": 4.40933895111084,
      "learning_rate": 1.6618826435641844e-05,
      "loss": 0.7662,
      "step": 38390
    },
    {
      "epoch": 2.0040706094171985,
      "grad_norm": 3.635932207107544,
      "learning_rate": 1.6610128211819147e-05,
      "loss": 0.8377,
      "step": 38400
    },
    {
      "epoch": 2.0045924824194032,
      "grad_norm": 4.840478420257568,
      "learning_rate": 1.6601429987996454e-05,
      "loss": 0.7491,
      "step": 38410
    },
    {
      "epoch": 2.005114355421608,
      "grad_norm": 4.9033589363098145,
      "learning_rate": 1.6592731764173758e-05,
      "loss": 0.8254,
      "step": 38420
    },
    {
      "epoch": 2.005636228423813,
      "grad_norm": 5.085372447967529,
      "learning_rate": 1.658403354035106e-05,
      "loss": 0.8312,
      "step": 38430
    },
    {
      "epoch": 2.006158101426018,
      "grad_norm": 5.064331531524658,
      "learning_rate": 1.6575335316528365e-05,
      "loss": 0.8734,
      "step": 38440
    },
    {
      "epoch": 2.006679974428223,
      "grad_norm": 4.136418342590332,
      "learning_rate": 1.656663709270567e-05,
      "loss": 0.8173,
      "step": 38450
    },
    {
      "epoch": 2.007201847430428,
      "grad_norm": 4.076263904571533,
      "learning_rate": 1.6557938868882975e-05,
      "loss": 0.8144,
      "step": 38460
    },
    {
      "epoch": 2.0077237204326326,
      "grad_norm": 4.608588695526123,
      "learning_rate": 1.654924064506028e-05,
      "loss": 0.874,
      "step": 38470
    },
    {
      "epoch": 2.0082455934348378,
      "grad_norm": 4.782345294952393,
      "learning_rate": 1.6540542421237586e-05,
      "loss": 0.8922,
      "step": 38480
    },
    {
      "epoch": 2.0087674664370425,
      "grad_norm": 4.643003940582275,
      "learning_rate": 1.653184419741489e-05,
      "loss": 0.8327,
      "step": 38490
    },
    {
      "epoch": 2.0092893394392473,
      "grad_norm": 4.550434589385986,
      "learning_rate": 1.6523145973592193e-05,
      "loss": 0.7895,
      "step": 38500
    },
    {
      "epoch": 2.0098112124414524,
      "grad_norm": 4.33446741104126,
      "learning_rate": 1.65144477497695e-05,
      "loss": 0.9583,
      "step": 38510
    },
    {
      "epoch": 2.010333085443657,
      "grad_norm": 4.319707870483398,
      "learning_rate": 1.6505749525946803e-05,
      "loss": 0.8702,
      "step": 38520
    },
    {
      "epoch": 2.0108549584458624,
      "grad_norm": 4.0876898765563965,
      "learning_rate": 1.6497051302124107e-05,
      "loss": 0.8003,
      "step": 38530
    },
    {
      "epoch": 2.011376831448067,
      "grad_norm": 4.325161457061768,
      "learning_rate": 1.648835307830141e-05,
      "loss": 0.9098,
      "step": 38540
    },
    {
      "epoch": 2.011898704450272,
      "grad_norm": 4.599072456359863,
      "learning_rate": 1.6479654854478714e-05,
      "loss": 0.8106,
      "step": 38550
    },
    {
      "epoch": 2.012420577452477,
      "grad_norm": 4.574134826660156,
      "learning_rate": 1.647095663065602e-05,
      "loss": 0.9314,
      "step": 38560
    },
    {
      "epoch": 2.012942450454682,
      "grad_norm": 4.699256896972656,
      "learning_rate": 1.6462258406833325e-05,
      "loss": 0.9271,
      "step": 38570
    },
    {
      "epoch": 2.013464323456887,
      "grad_norm": 4.37095308303833,
      "learning_rate": 1.645356018301063e-05,
      "loss": 0.9544,
      "step": 38580
    },
    {
      "epoch": 2.0139861964590917,
      "grad_norm": 4.666642665863037,
      "learning_rate": 1.6444861959187935e-05,
      "loss": 0.9222,
      "step": 38590
    },
    {
      "epoch": 2.0145080694612965,
      "grad_norm": 3.80708384513855,
      "learning_rate": 1.643616373536524e-05,
      "loss": 0.8258,
      "step": 38600
    },
    {
      "epoch": 2.0150299424635016,
      "grad_norm": 4.173478126525879,
      "learning_rate": 1.6427465511542546e-05,
      "loss": 0.8399,
      "step": 38610
    },
    {
      "epoch": 2.0155518154657064,
      "grad_norm": 4.876028537750244,
      "learning_rate": 1.641876728771985e-05,
      "loss": 0.8428,
      "step": 38620
    },
    {
      "epoch": 2.016073688467911,
      "grad_norm": 4.411349773406982,
      "learning_rate": 1.6410069063897156e-05,
      "loss": 0.9744,
      "step": 38630
    },
    {
      "epoch": 2.0165955614701163,
      "grad_norm": 4.131272315979004,
      "learning_rate": 1.6401370840074456e-05,
      "loss": 0.8906,
      "step": 38640
    },
    {
      "epoch": 2.017117434472321,
      "grad_norm": 4.682008266448975,
      "learning_rate": 1.639267261625176e-05,
      "loss": 0.8909,
      "step": 38650
    },
    {
      "epoch": 2.0176393074745262,
      "grad_norm": 5.54680061340332,
      "learning_rate": 1.6383974392429067e-05,
      "loss": 0.948,
      "step": 38660
    },
    {
      "epoch": 2.018161180476731,
      "grad_norm": 4.680761337280273,
      "learning_rate": 1.637527616860637e-05,
      "loss": 0.8886,
      "step": 38670
    },
    {
      "epoch": 2.0186830534789357,
      "grad_norm": 4.342768669128418,
      "learning_rate": 1.6366577944783677e-05,
      "loss": 0.9223,
      "step": 38680
    },
    {
      "epoch": 2.019204926481141,
      "grad_norm": 4.6235456466674805,
      "learning_rate": 1.635787972096098e-05,
      "loss": 0.8845,
      "step": 38690
    },
    {
      "epoch": 2.0197267994833457,
      "grad_norm": 4.989151477813721,
      "learning_rate": 1.6349181497138284e-05,
      "loss": 0.9504,
      "step": 38700
    },
    {
      "epoch": 2.020248672485551,
      "grad_norm": 4.273043632507324,
      "learning_rate": 1.634048327331559e-05,
      "loss": 0.8861,
      "step": 38710
    },
    {
      "epoch": 2.0207705454877556,
      "grad_norm": 4.518545150756836,
      "learning_rate": 1.6331785049492895e-05,
      "loss": 0.8583,
      "step": 38720
    },
    {
      "epoch": 2.0212924184899603,
      "grad_norm": 3.5626978874206543,
      "learning_rate": 1.6323086825670202e-05,
      "loss": 0.9399,
      "step": 38730
    },
    {
      "epoch": 2.0218142914921655,
      "grad_norm": 4.075164794921875,
      "learning_rate": 1.6314388601847502e-05,
      "loss": 0.9136,
      "step": 38740
    },
    {
      "epoch": 2.0223361644943703,
      "grad_norm": 3.945662260055542,
      "learning_rate": 1.630569037802481e-05,
      "loss": 0.9022,
      "step": 38750
    },
    {
      "epoch": 2.022858037496575,
      "grad_norm": 4.870081424713135,
      "learning_rate": 1.6296992154202112e-05,
      "loss": 0.9015,
      "step": 38760
    },
    {
      "epoch": 2.02337991049878,
      "grad_norm": 4.232278347015381,
      "learning_rate": 1.6288293930379416e-05,
      "loss": 0.8331,
      "step": 38770
    },
    {
      "epoch": 2.023901783500985,
      "grad_norm": 5.344335556030273,
      "learning_rate": 1.6279595706556723e-05,
      "loss": 0.9309,
      "step": 38780
    },
    {
      "epoch": 2.02442365650319,
      "grad_norm": 4.238248348236084,
      "learning_rate": 1.6270897482734026e-05,
      "loss": 0.8255,
      "step": 38790
    },
    {
      "epoch": 2.024945529505395,
      "grad_norm": 4.015616416931152,
      "learning_rate": 1.626219925891133e-05,
      "loss": 0.8759,
      "step": 38800
    },
    {
      "epoch": 2.0254674025075996,
      "grad_norm": 4.41179895401001,
      "learning_rate": 1.6253501035088637e-05,
      "loss": 0.9006,
      "step": 38810
    },
    {
      "epoch": 2.025989275509805,
      "grad_norm": 5.295701503753662,
      "learning_rate": 1.624480281126594e-05,
      "loss": 0.7717,
      "step": 38820
    },
    {
      "epoch": 2.0265111485120095,
      "grad_norm": 3.731997489929199,
      "learning_rate": 1.6236104587443247e-05,
      "loss": 0.8753,
      "step": 38830
    },
    {
      "epoch": 2.0270330215142147,
      "grad_norm": 4.805581569671631,
      "learning_rate": 1.6227406363620548e-05,
      "loss": 0.9055,
      "step": 38840
    },
    {
      "epoch": 2.0275548945164195,
      "grad_norm": 4.5933613777160645,
      "learning_rate": 1.6218708139797854e-05,
      "loss": 0.7706,
      "step": 38850
    },
    {
      "epoch": 2.028076767518624,
      "grad_norm": 5.80829381942749,
      "learning_rate": 1.6210009915975158e-05,
      "loss": 0.8406,
      "step": 38860
    },
    {
      "epoch": 2.0285986405208294,
      "grad_norm": 4.550958633422852,
      "learning_rate": 1.620131169215246e-05,
      "loss": 0.8478,
      "step": 38870
    },
    {
      "epoch": 2.029120513523034,
      "grad_norm": 4.336696147918701,
      "learning_rate": 1.619261346832977e-05,
      "loss": 0.7995,
      "step": 38880
    },
    {
      "epoch": 2.029642386525239,
      "grad_norm": 4.588006019592285,
      "learning_rate": 1.6183915244507072e-05,
      "loss": 0.8578,
      "step": 38890
    },
    {
      "epoch": 2.030164259527444,
      "grad_norm": 4.895496845245361,
      "learning_rate": 1.617521702068438e-05,
      "loss": 0.8805,
      "step": 38900
    },
    {
      "epoch": 2.030686132529649,
      "grad_norm": 4.332076072692871,
      "learning_rate": 1.6166518796861683e-05,
      "loss": 0.8233,
      "step": 38910
    },
    {
      "epoch": 2.031208005531854,
      "grad_norm": 4.177633762359619,
      "learning_rate": 1.6157820573038986e-05,
      "loss": 0.9296,
      "step": 38920
    },
    {
      "epoch": 2.0317298785340587,
      "grad_norm": 4.698068618774414,
      "learning_rate": 1.6149122349216293e-05,
      "loss": 0.9019,
      "step": 38930
    },
    {
      "epoch": 2.0322517515362635,
      "grad_norm": 4.368653774261475,
      "learning_rate": 1.6140424125393597e-05,
      "loss": 0.8553,
      "step": 38940
    },
    {
      "epoch": 2.0327736245384687,
      "grad_norm": 5.212513446807861,
      "learning_rate": 1.61317259015709e-05,
      "loss": 0.8718,
      "step": 38950
    },
    {
      "epoch": 2.0332954975406734,
      "grad_norm": 4.987772464752197,
      "learning_rate": 1.6123027677748204e-05,
      "loss": 0.9054,
      "step": 38960
    },
    {
      "epoch": 2.0338173705428786,
      "grad_norm": 4.5671210289001465,
      "learning_rate": 1.6114329453925507e-05,
      "loss": 0.7481,
      "step": 38970
    },
    {
      "epoch": 2.0343392435450833,
      "grad_norm": 3.9302613735198975,
      "learning_rate": 1.6105631230102814e-05,
      "loss": 0.8163,
      "step": 38980
    },
    {
      "epoch": 2.034861116547288,
      "grad_norm": 5.114372253417969,
      "learning_rate": 1.6096933006280118e-05,
      "loss": 0.8872,
      "step": 38990
    },
    {
      "epoch": 2.0353829895494933,
      "grad_norm": 3.8710849285125732,
      "learning_rate": 1.6088234782457425e-05,
      "loss": 0.8567,
      "step": 39000
    },
    {
      "epoch": 2.035904862551698,
      "grad_norm": 4.583016395568848,
      "learning_rate": 1.6079536558634728e-05,
      "loss": 0.8144,
      "step": 39010
    },
    {
      "epoch": 2.0364267355539027,
      "grad_norm": 4.049055099487305,
      "learning_rate": 1.6070838334812032e-05,
      "loss": 0.9072,
      "step": 39020
    },
    {
      "epoch": 2.036948608556108,
      "grad_norm": 5.271353721618652,
      "learning_rate": 1.606214011098934e-05,
      "loss": 0.8578,
      "step": 39030
    },
    {
      "epoch": 2.0374704815583127,
      "grad_norm": 4.420067310333252,
      "learning_rate": 1.6053441887166642e-05,
      "loss": 0.9001,
      "step": 39040
    },
    {
      "epoch": 2.037992354560518,
      "grad_norm": 4.7911458015441895,
      "learning_rate": 1.6044743663343946e-05,
      "loss": 0.747,
      "step": 39050
    },
    {
      "epoch": 2.0385142275627226,
      "grad_norm": 5.014828205108643,
      "learning_rate": 1.603604543952125e-05,
      "loss": 0.9178,
      "step": 39060
    },
    {
      "epoch": 2.0390361005649273,
      "grad_norm": 4.5807576179504395,
      "learning_rate": 1.6027347215698553e-05,
      "loss": 0.8337,
      "step": 39070
    },
    {
      "epoch": 2.0395579735671325,
      "grad_norm": 5.196455001831055,
      "learning_rate": 1.601864899187586e-05,
      "loss": 0.8243,
      "step": 39080
    },
    {
      "epoch": 2.0400798465693373,
      "grad_norm": 4.581164360046387,
      "learning_rate": 1.6009950768053163e-05,
      "loss": 0.8378,
      "step": 39090
    },
    {
      "epoch": 2.0406017195715425,
      "grad_norm": 5.129262447357178,
      "learning_rate": 1.600125254423047e-05,
      "loss": 0.8145,
      "step": 39100
    },
    {
      "epoch": 2.041123592573747,
      "grad_norm": 3.9355690479278564,
      "learning_rate": 1.5992554320407774e-05,
      "loss": 0.8333,
      "step": 39110
    },
    {
      "epoch": 2.041645465575952,
      "grad_norm": 4.0619025230407715,
      "learning_rate": 1.5983856096585077e-05,
      "loss": 0.8568,
      "step": 39120
    },
    {
      "epoch": 2.042167338578157,
      "grad_norm": 4.129268169403076,
      "learning_rate": 1.5975157872762384e-05,
      "loss": 0.8506,
      "step": 39130
    },
    {
      "epoch": 2.042689211580362,
      "grad_norm": 3.992185592651367,
      "learning_rate": 1.5966459648939688e-05,
      "loss": 0.9255,
      "step": 39140
    },
    {
      "epoch": 2.0432110845825666,
      "grad_norm": 4.0865702629089355,
      "learning_rate": 1.5957761425116995e-05,
      "loss": 0.859,
      "step": 39150
    },
    {
      "epoch": 2.043732957584772,
      "grad_norm": 4.254152774810791,
      "learning_rate": 1.5949063201294295e-05,
      "loss": 0.8215,
      "step": 39160
    },
    {
      "epoch": 2.0442548305869765,
      "grad_norm": 3.751955270767212,
      "learning_rate": 1.59403649774716e-05,
      "loss": 0.8788,
      "step": 39170
    },
    {
      "epoch": 2.0447767035891817,
      "grad_norm": 4.262964725494385,
      "learning_rate": 1.5931666753648905e-05,
      "loss": 0.8305,
      "step": 39180
    },
    {
      "epoch": 2.0452985765913865,
      "grad_norm": 5.262465953826904,
      "learning_rate": 1.592296852982621e-05,
      "loss": 0.9371,
      "step": 39190
    },
    {
      "epoch": 2.045820449593591,
      "grad_norm": 3.5720102787017822,
      "learning_rate": 1.5914270306003516e-05,
      "loss": 0.8416,
      "step": 39200
    },
    {
      "epoch": 2.0463423225957964,
      "grad_norm": 4.178874969482422,
      "learning_rate": 1.590557208218082e-05,
      "loss": 0.845,
      "step": 39210
    },
    {
      "epoch": 2.046864195598001,
      "grad_norm": 4.357267379760742,
      "learning_rate": 1.5896873858358123e-05,
      "loss": 0.8397,
      "step": 39220
    },
    {
      "epoch": 2.0473860686002063,
      "grad_norm": 4.2107086181640625,
      "learning_rate": 1.588817563453543e-05,
      "loss": 0.8007,
      "step": 39230
    },
    {
      "epoch": 2.047907941602411,
      "grad_norm": 4.577972412109375,
      "learning_rate": 1.5879477410712734e-05,
      "loss": 0.7522,
      "step": 39240
    },
    {
      "epoch": 2.048429814604616,
      "grad_norm": 4.74308443069458,
      "learning_rate": 1.587077918689004e-05,
      "loss": 0.9517,
      "step": 39250
    },
    {
      "epoch": 2.048951687606821,
      "grad_norm": 4.518247604370117,
      "learning_rate": 1.586208096306734e-05,
      "loss": 0.9286,
      "step": 39260
    },
    {
      "epoch": 2.0494735606090257,
      "grad_norm": 4.374490261077881,
      "learning_rate": 1.5853382739244648e-05,
      "loss": 0.794,
      "step": 39270
    },
    {
      "epoch": 2.0499954336112305,
      "grad_norm": 4.2697224617004395,
      "learning_rate": 1.584468451542195e-05,
      "loss": 0.8277,
      "step": 39280
    },
    {
      "epoch": 2.0505173066134357,
      "grad_norm": 5.0678815841674805,
      "learning_rate": 1.5835986291599255e-05,
      "loss": 0.8135,
      "step": 39290
    },
    {
      "epoch": 2.0510391796156404,
      "grad_norm": 4.146457672119141,
      "learning_rate": 1.582728806777656e-05,
      "loss": 0.8636,
      "step": 39300
    },
    {
      "epoch": 2.0515610526178456,
      "grad_norm": 4.783657550811768,
      "learning_rate": 1.5818589843953865e-05,
      "loss": 0.8288,
      "step": 39310
    },
    {
      "epoch": 2.0520829256200503,
      "grad_norm": 4.41710901260376,
      "learning_rate": 1.580989162013117e-05,
      "loss": 0.8691,
      "step": 39320
    },
    {
      "epoch": 2.052604798622255,
      "grad_norm": 4.7872819900512695,
      "learning_rate": 1.5801193396308476e-05,
      "loss": 0.9839,
      "step": 39330
    },
    {
      "epoch": 2.0531266716244603,
      "grad_norm": 5.251109600067139,
      "learning_rate": 1.579249517248578e-05,
      "loss": 0.9255,
      "step": 39340
    },
    {
      "epoch": 2.053648544626665,
      "grad_norm": 4.743844509124756,
      "learning_rate": 1.5783796948663086e-05,
      "loss": 0.8545,
      "step": 39350
    },
    {
      "epoch": 2.05417041762887,
      "grad_norm": 4.503264427185059,
      "learning_rate": 1.5775098724840386e-05,
      "loss": 0.918,
      "step": 39360
    },
    {
      "epoch": 2.054692290631075,
      "grad_norm": 4.9737043380737305,
      "learning_rate": 1.5766400501017693e-05,
      "loss": 0.8561,
      "step": 39370
    },
    {
      "epoch": 2.0552141636332797,
      "grad_norm": 5.01069450378418,
      "learning_rate": 1.5757702277194997e-05,
      "loss": 0.8627,
      "step": 39380
    },
    {
      "epoch": 2.055736036635485,
      "grad_norm": 4.193811416625977,
      "learning_rate": 1.57490040533723e-05,
      "loss": 0.8645,
      "step": 39390
    },
    {
      "epoch": 2.0562579096376896,
      "grad_norm": 5.640252590179443,
      "learning_rate": 1.5740305829549607e-05,
      "loss": 0.8569,
      "step": 39400
    },
    {
      "epoch": 2.056779782639895,
      "grad_norm": 4.823402404785156,
      "learning_rate": 1.573160760572691e-05,
      "loss": 0.8417,
      "step": 39410
    },
    {
      "epoch": 2.0573016556420995,
      "grad_norm": 4.8743977546691895,
      "learning_rate": 1.5722909381904218e-05,
      "loss": 0.8512,
      "step": 39420
    },
    {
      "epoch": 2.0578235286443043,
      "grad_norm": 3.9890878200531006,
      "learning_rate": 1.571421115808152e-05,
      "loss": 0.8069,
      "step": 39430
    },
    {
      "epoch": 2.0583454016465095,
      "grad_norm": 5.15078592300415,
      "learning_rate": 1.5705512934258825e-05,
      "loss": 0.8604,
      "step": 39440
    },
    {
      "epoch": 2.058867274648714,
      "grad_norm": 3.958005428314209,
      "learning_rate": 1.5696814710436132e-05,
      "loss": 0.8714,
      "step": 39450
    },
    {
      "epoch": 2.059389147650919,
      "grad_norm": 3.6510813236236572,
      "learning_rate": 1.5688116486613435e-05,
      "loss": 0.8778,
      "step": 39460
    },
    {
      "epoch": 2.059911020653124,
      "grad_norm": 4.213191986083984,
      "learning_rate": 1.567941826279074e-05,
      "loss": 0.8051,
      "step": 39470
    },
    {
      "epoch": 2.060432893655329,
      "grad_norm": 4.029824733734131,
      "learning_rate": 1.5670720038968042e-05,
      "loss": 0.8523,
      "step": 39480
    },
    {
      "epoch": 2.060954766657534,
      "grad_norm": 3.9777159690856934,
      "learning_rate": 1.5662021815145346e-05,
      "loss": 0.8411,
      "step": 39490
    },
    {
      "epoch": 2.061476639659739,
      "grad_norm": 4.765551567077637,
      "learning_rate": 1.5653323591322653e-05,
      "loss": 0.8696,
      "step": 39500
    },
    {
      "epoch": 2.0619985126619436,
      "grad_norm": 4.315624713897705,
      "learning_rate": 1.5644625367499956e-05,
      "loss": 0.8796,
      "step": 39510
    },
    {
      "epoch": 2.0625203856641487,
      "grad_norm": 4.521589279174805,
      "learning_rate": 1.5635927143677263e-05,
      "loss": 0.8549,
      "step": 39520
    },
    {
      "epoch": 2.0630422586663535,
      "grad_norm": 4.4419121742248535,
      "learning_rate": 1.5627228919854567e-05,
      "loss": 0.898,
      "step": 39530
    },
    {
      "epoch": 2.0635641316685582,
      "grad_norm": 4.987538814544678,
      "learning_rate": 1.561853069603187e-05,
      "loss": 0.8499,
      "step": 39540
    },
    {
      "epoch": 2.0640860046707634,
      "grad_norm": 4.3541388511657715,
      "learning_rate": 1.5609832472209177e-05,
      "loss": 0.834,
      "step": 39550
    },
    {
      "epoch": 2.064607877672968,
      "grad_norm": 4.555232048034668,
      "learning_rate": 1.560113424838648e-05,
      "loss": 0.8426,
      "step": 39560
    },
    {
      "epoch": 2.0651297506751733,
      "grad_norm": 4.1394219398498535,
      "learning_rate": 1.5592436024563785e-05,
      "loss": 0.8459,
      "step": 39570
    },
    {
      "epoch": 2.065651623677378,
      "grad_norm": 4.316160202026367,
      "learning_rate": 1.5583737800741088e-05,
      "loss": 0.7631,
      "step": 39580
    },
    {
      "epoch": 2.066173496679583,
      "grad_norm": 4.164004802703857,
      "learning_rate": 1.557503957691839e-05,
      "loss": 0.7991,
      "step": 39590
    },
    {
      "epoch": 2.066695369681788,
      "grad_norm": 4.381293296813965,
      "learning_rate": 1.55663413530957e-05,
      "loss": 0.8133,
      "step": 39600
    },
    {
      "epoch": 2.0672172426839928,
      "grad_norm": 4.225502014160156,
      "learning_rate": 1.5557643129273002e-05,
      "loss": 0.89,
      "step": 39610
    },
    {
      "epoch": 2.067739115686198,
      "grad_norm": 4.719217777252197,
      "learning_rate": 1.554894490545031e-05,
      "loss": 0.8546,
      "step": 39620
    },
    {
      "epoch": 2.0682609886884027,
      "grad_norm": 4.734067916870117,
      "learning_rate": 1.5540246681627613e-05,
      "loss": 0.8717,
      "step": 39630
    },
    {
      "epoch": 2.0687828616906074,
      "grad_norm": 5.454977989196777,
      "learning_rate": 1.5531548457804916e-05,
      "loss": 0.9019,
      "step": 39640
    },
    {
      "epoch": 2.0693047346928126,
      "grad_norm": 4.19240665435791,
      "learning_rate": 1.5522850233982223e-05,
      "loss": 0.885,
      "step": 39650
    },
    {
      "epoch": 2.0698266076950174,
      "grad_norm": 4.61730432510376,
      "learning_rate": 1.5514152010159527e-05,
      "loss": 0.8972,
      "step": 39660
    },
    {
      "epoch": 2.0703484806972225,
      "grad_norm": 4.739790916442871,
      "learning_rate": 1.5505453786336834e-05,
      "loss": 0.8397,
      "step": 39670
    },
    {
      "epoch": 2.0708703536994273,
      "grad_norm": 5.220311641693115,
      "learning_rate": 1.5496755562514134e-05,
      "loss": 0.878,
      "step": 39680
    },
    {
      "epoch": 2.071392226701632,
      "grad_norm": 4.385321617126465,
      "learning_rate": 1.548805733869144e-05,
      "loss": 0.9318,
      "step": 39690
    },
    {
      "epoch": 2.071914099703837,
      "grad_norm": 5.0298919677734375,
      "learning_rate": 1.5479359114868744e-05,
      "loss": 0.8556,
      "step": 39700
    },
    {
      "epoch": 2.072435972706042,
      "grad_norm": 4.814258098602295,
      "learning_rate": 1.5470660891046048e-05,
      "loss": 0.9172,
      "step": 39710
    },
    {
      "epoch": 2.0729578457082467,
      "grad_norm": 3.7523465156555176,
      "learning_rate": 1.5461962667223355e-05,
      "loss": 0.8922,
      "step": 39720
    },
    {
      "epoch": 2.073479718710452,
      "grad_norm": 4.966047763824463,
      "learning_rate": 1.5453264443400658e-05,
      "loss": 0.8752,
      "step": 39730
    },
    {
      "epoch": 2.0740015917126566,
      "grad_norm": 4.802694320678711,
      "learning_rate": 1.5444566219577962e-05,
      "loss": 0.8736,
      "step": 39740
    },
    {
      "epoch": 2.074523464714862,
      "grad_norm": 4.729084014892578,
      "learning_rate": 1.543586799575527e-05,
      "loss": 0.8385,
      "step": 39750
    },
    {
      "epoch": 2.0750453377170666,
      "grad_norm": 4.323923587799072,
      "learning_rate": 1.5427169771932572e-05,
      "loss": 0.8216,
      "step": 39760
    },
    {
      "epoch": 2.0755672107192713,
      "grad_norm": 4.585926055908203,
      "learning_rate": 1.541847154810988e-05,
      "loss": 0.9314,
      "step": 39770
    },
    {
      "epoch": 2.0760890837214765,
      "grad_norm": 4.464273452758789,
      "learning_rate": 1.540977332428718e-05,
      "loss": 0.8337,
      "step": 39780
    },
    {
      "epoch": 2.0766109567236812,
      "grad_norm": 4.606903553009033,
      "learning_rate": 1.5401075100464486e-05,
      "loss": 0.867,
      "step": 39790
    },
    {
      "epoch": 2.0771328297258864,
      "grad_norm": 3.601898670196533,
      "learning_rate": 1.539237687664179e-05,
      "loss": 0.8125,
      "step": 39800
    },
    {
      "epoch": 2.077654702728091,
      "grad_norm": 3.4850339889526367,
      "learning_rate": 1.5383678652819093e-05,
      "loss": 0.8356,
      "step": 39810
    },
    {
      "epoch": 2.078176575730296,
      "grad_norm": 4.3088059425354,
      "learning_rate": 1.53749804289964e-05,
      "loss": 0.7649,
      "step": 39820
    },
    {
      "epoch": 2.078698448732501,
      "grad_norm": 4.7415266036987305,
      "learning_rate": 1.5366282205173704e-05,
      "loss": 0.7588,
      "step": 39830
    },
    {
      "epoch": 2.079220321734706,
      "grad_norm": 4.58880615234375,
      "learning_rate": 1.535758398135101e-05,
      "loss": 0.838,
      "step": 39840
    },
    {
      "epoch": 2.0797421947369106,
      "grad_norm": 4.626010417938232,
      "learning_rate": 1.5348885757528314e-05,
      "loss": 0.8611,
      "step": 39850
    },
    {
      "epoch": 2.0802640677391158,
      "grad_norm": 4.499919414520264,
      "learning_rate": 1.5340187533705618e-05,
      "loss": 0.8816,
      "step": 39860
    },
    {
      "epoch": 2.0807859407413205,
      "grad_norm": 4.3133392333984375,
      "learning_rate": 1.5331489309882925e-05,
      "loss": 0.8981,
      "step": 39870
    },
    {
      "epoch": 2.0813078137435257,
      "grad_norm": 4.813652038574219,
      "learning_rate": 1.532279108606023e-05,
      "loss": 0.8561,
      "step": 39880
    },
    {
      "epoch": 2.0818296867457304,
      "grad_norm": 4.230471134185791,
      "learning_rate": 1.5314092862237532e-05,
      "loss": 0.8901,
      "step": 39890
    },
    {
      "epoch": 2.082351559747935,
      "grad_norm": 4.63806676864624,
      "learning_rate": 1.5305394638414836e-05,
      "loss": 0.9858,
      "step": 39900
    },
    {
      "epoch": 2.0828734327501404,
      "grad_norm": 4.87812614440918,
      "learning_rate": 1.529669641459214e-05,
      "loss": 0.8407,
      "step": 39910
    },
    {
      "epoch": 2.083395305752345,
      "grad_norm": 4.972762584686279,
      "learning_rate": 1.5287998190769446e-05,
      "loss": 0.7896,
      "step": 39920
    },
    {
      "epoch": 2.0839171787545503,
      "grad_norm": 5.246470928192139,
      "learning_rate": 1.527929996694675e-05,
      "loss": 0.9076,
      "step": 39930
    },
    {
      "epoch": 2.084439051756755,
      "grad_norm": 5.015084266662598,
      "learning_rate": 1.5270601743124056e-05,
      "loss": 0.8518,
      "step": 39940
    },
    {
      "epoch": 2.0849609247589598,
      "grad_norm": 4.857418060302734,
      "learning_rate": 1.526190351930136e-05,
      "loss": 0.8655,
      "step": 39950
    },
    {
      "epoch": 2.085482797761165,
      "grad_norm": 4.616189956665039,
      "learning_rate": 1.5253205295478665e-05,
      "loss": 0.9229,
      "step": 39960
    },
    {
      "epoch": 2.0860046707633697,
      "grad_norm": 5.3470306396484375,
      "learning_rate": 1.5244507071655969e-05,
      "loss": 0.9574,
      "step": 39970
    },
    {
      "epoch": 2.0865265437655744,
      "grad_norm": 5.5561017990112305,
      "learning_rate": 1.5235808847833274e-05,
      "loss": 0.8909,
      "step": 39980
    },
    {
      "epoch": 2.0870484167677796,
      "grad_norm": 4.309542179107666,
      "learning_rate": 1.5227110624010576e-05,
      "loss": 0.8136,
      "step": 39990
    },
    {
      "epoch": 2.0875702897699844,
      "grad_norm": 4.459062576293945,
      "learning_rate": 1.5218412400187881e-05,
      "loss": 0.7725,
      "step": 40000
    },
    {
      "epoch": 2.0880921627721896,
      "grad_norm": 5.287691116333008,
      "learning_rate": 1.5209714176365186e-05,
      "loss": 0.9126,
      "step": 40010
    },
    {
      "epoch": 2.0886140357743943,
      "grad_norm": 4.252469539642334,
      "learning_rate": 1.5201015952542492e-05,
      "loss": 0.7956,
      "step": 40020
    },
    {
      "epoch": 2.089135908776599,
      "grad_norm": 4.493505001068115,
      "learning_rate": 1.5192317728719795e-05,
      "loss": 0.9361,
      "step": 40030
    },
    {
      "epoch": 2.0896577817788042,
      "grad_norm": 4.566045761108398,
      "learning_rate": 1.51836195048971e-05,
      "loss": 0.8016,
      "step": 40040
    },
    {
      "epoch": 2.090179654781009,
      "grad_norm": 4.732221603393555,
      "learning_rate": 1.5174921281074406e-05,
      "loss": 0.7298,
      "step": 40050
    },
    {
      "epoch": 2.090701527783214,
      "grad_norm": 3.689847707748413,
      "learning_rate": 1.5166223057251711e-05,
      "loss": 0.8973,
      "step": 40060
    },
    {
      "epoch": 2.091223400785419,
      "grad_norm": 4.952366352081299,
      "learning_rate": 1.5157524833429016e-05,
      "loss": 0.707,
      "step": 40070
    },
    {
      "epoch": 2.0917452737876236,
      "grad_norm": 4.086841106414795,
      "learning_rate": 1.514882660960632e-05,
      "loss": 0.8808,
      "step": 40080
    },
    {
      "epoch": 2.092267146789829,
      "grad_norm": 4.711390972137451,
      "learning_rate": 1.5140128385783622e-05,
      "loss": 0.7981,
      "step": 40090
    },
    {
      "epoch": 2.0927890197920336,
      "grad_norm": 4.337270259857178,
      "learning_rate": 1.5131430161960927e-05,
      "loss": 0.8789,
      "step": 40100
    },
    {
      "epoch": 2.0933108927942383,
      "grad_norm": 4.402121067047119,
      "learning_rate": 1.5122731938138232e-05,
      "loss": 0.8334,
      "step": 40110
    },
    {
      "epoch": 2.0938327657964435,
      "grad_norm": 3.8523521423339844,
      "learning_rate": 1.5114033714315537e-05,
      "loss": 0.8541,
      "step": 40120
    },
    {
      "epoch": 2.0943546387986482,
      "grad_norm": 4.403240203857422,
      "learning_rate": 1.5105335490492843e-05,
      "loss": 0.897,
      "step": 40130
    },
    {
      "epoch": 2.0948765118008534,
      "grad_norm": 4.8231916427612305,
      "learning_rate": 1.5096637266670146e-05,
      "loss": 0.9285,
      "step": 40140
    },
    {
      "epoch": 2.095398384803058,
      "grad_norm": 4.147903919219971,
      "learning_rate": 1.5087939042847451e-05,
      "loss": 0.8213,
      "step": 40150
    },
    {
      "epoch": 2.095920257805263,
      "grad_norm": 4.7654805183410645,
      "learning_rate": 1.5079240819024757e-05,
      "loss": 0.8631,
      "step": 40160
    },
    {
      "epoch": 2.096442130807468,
      "grad_norm": 3.631927967071533,
      "learning_rate": 1.5070542595202062e-05,
      "loss": 0.8742,
      "step": 40170
    },
    {
      "epoch": 2.096964003809673,
      "grad_norm": 3.9412729740142822,
      "learning_rate": 1.5061844371379365e-05,
      "loss": 0.8483,
      "step": 40180
    },
    {
      "epoch": 2.097485876811878,
      "grad_norm": 4.376950740814209,
      "learning_rate": 1.505314614755667e-05,
      "loss": 0.8731,
      "step": 40190
    },
    {
      "epoch": 2.098007749814083,
      "grad_norm": 4.2377705574035645,
      "learning_rate": 1.5044447923733972e-05,
      "loss": 0.9034,
      "step": 40200
    },
    {
      "epoch": 2.0985296228162875,
      "grad_norm": 5.245284557342529,
      "learning_rate": 1.5035749699911278e-05,
      "loss": 0.926,
      "step": 40210
    },
    {
      "epoch": 2.0990514958184927,
      "grad_norm": 4.801839351654053,
      "learning_rate": 1.5027051476088583e-05,
      "loss": 0.8945,
      "step": 40220
    },
    {
      "epoch": 2.0995733688206974,
      "grad_norm": 4.188918113708496,
      "learning_rate": 1.5018353252265888e-05,
      "loss": 0.8279,
      "step": 40230
    },
    {
      "epoch": 2.100095241822902,
      "grad_norm": 4.493033409118652,
      "learning_rate": 1.5009655028443192e-05,
      "loss": 0.8745,
      "step": 40240
    },
    {
      "epoch": 2.1006171148251074,
      "grad_norm": 4.238437652587891,
      "learning_rate": 1.5000956804620497e-05,
      "loss": 0.9229,
      "step": 40250
    },
    {
      "epoch": 2.101138987827312,
      "grad_norm": 4.114013671875,
      "learning_rate": 1.4992258580797802e-05,
      "loss": 0.7903,
      "step": 40260
    },
    {
      "epoch": 2.1016608608295173,
      "grad_norm": 3.765549898147583,
      "learning_rate": 1.4983560356975107e-05,
      "loss": 0.8412,
      "step": 40270
    },
    {
      "epoch": 2.102182733831722,
      "grad_norm": 5.110095024108887,
      "learning_rate": 1.4974862133152413e-05,
      "loss": 0.8204,
      "step": 40280
    },
    {
      "epoch": 2.102704606833927,
      "grad_norm": 4.216182231903076,
      "learning_rate": 1.4966163909329716e-05,
      "loss": 0.833,
      "step": 40290
    },
    {
      "epoch": 2.103226479836132,
      "grad_norm": 4.455270767211914,
      "learning_rate": 1.4957465685507018e-05,
      "loss": 0.8333,
      "step": 40300
    },
    {
      "epoch": 2.1037483528383367,
      "grad_norm": 5.064055919647217,
      "learning_rate": 1.4948767461684323e-05,
      "loss": 0.884,
      "step": 40310
    },
    {
      "epoch": 2.104270225840542,
      "grad_norm": 3.8048572540283203,
      "learning_rate": 1.4940069237861629e-05,
      "loss": 0.8447,
      "step": 40320
    },
    {
      "epoch": 2.1047920988427467,
      "grad_norm": 4.031355857849121,
      "learning_rate": 1.4931371014038934e-05,
      "loss": 0.8189,
      "step": 40330
    },
    {
      "epoch": 2.1053139718449514,
      "grad_norm": 4.004607200622559,
      "learning_rate": 1.4922672790216239e-05,
      "loss": 0.882,
      "step": 40340
    },
    {
      "epoch": 2.1058358448471566,
      "grad_norm": 4.517694473266602,
      "learning_rate": 1.4913974566393543e-05,
      "loss": 0.8541,
      "step": 40350
    },
    {
      "epoch": 2.1063577178493613,
      "grad_norm": 4.613595962524414,
      "learning_rate": 1.4905276342570848e-05,
      "loss": 0.8755,
      "step": 40360
    },
    {
      "epoch": 2.106879590851566,
      "grad_norm": 5.181058883666992,
      "learning_rate": 1.4896578118748153e-05,
      "loss": 0.9531,
      "step": 40370
    },
    {
      "epoch": 2.1074014638537713,
      "grad_norm": Infinity,
      "learning_rate": 1.4888749717307726e-05,
      "loss": 0.8097,
      "step": 40380
    },
    {
      "epoch": 2.107923336855976,
      "grad_norm": 4.663949966430664,
      "learning_rate": 1.4880051493485032e-05,
      "loss": 0.9058,
      "step": 40390
    },
    {
      "epoch": 2.108445209858181,
      "grad_norm": 3.9145758152008057,
      "learning_rate": 1.4871353269662335e-05,
      "loss": 0.8509,
      "step": 40400
    },
    {
      "epoch": 2.108967082860386,
      "grad_norm": 4.331654071807861,
      "learning_rate": 1.486265504583964e-05,
      "loss": 0.925,
      "step": 40410
    },
    {
      "epoch": 2.1094889558625907,
      "grad_norm": 4.540279865264893,
      "learning_rate": 1.4853956822016946e-05,
      "loss": 0.7735,
      "step": 40420
    },
    {
      "epoch": 2.110010828864796,
      "grad_norm": 4.60723352432251,
      "learning_rate": 1.484525859819425e-05,
      "loss": 0.8429,
      "step": 40430
    },
    {
      "epoch": 2.1105327018670006,
      "grad_norm": 4.253439903259277,
      "learning_rate": 1.4836560374371556e-05,
      "loss": 0.7822,
      "step": 40440
    },
    {
      "epoch": 2.111054574869206,
      "grad_norm": 4.558870792388916,
      "learning_rate": 1.4827862150548858e-05,
      "loss": 0.8728,
      "step": 40450
    },
    {
      "epoch": 2.1115764478714105,
      "grad_norm": 5.405550479888916,
      "learning_rate": 1.4819163926726161e-05,
      "loss": 0.8446,
      "step": 40460
    },
    {
      "epoch": 2.1120983208736153,
      "grad_norm": 4.648169994354248,
      "learning_rate": 1.4810465702903467e-05,
      "loss": 0.8108,
      "step": 40470
    },
    {
      "epoch": 2.1126201938758205,
      "grad_norm": 5.408065319061279,
      "learning_rate": 1.4801767479080772e-05,
      "loss": 0.8544,
      "step": 40480
    },
    {
      "epoch": 2.113142066878025,
      "grad_norm": 4.313014030456543,
      "learning_rate": 1.4793069255258077e-05,
      "loss": 0.757,
      "step": 40490
    },
    {
      "epoch": 2.11366393988023,
      "grad_norm": 4.14522123336792,
      "learning_rate": 1.4784371031435382e-05,
      "loss": 0.7994,
      "step": 40500
    },
    {
      "epoch": 2.114185812882435,
      "grad_norm": 4.486152648925781,
      "learning_rate": 1.4775672807612686e-05,
      "loss": 0.8654,
      "step": 40510
    },
    {
      "epoch": 2.11470768588464,
      "grad_norm": 4.197316646575928,
      "learning_rate": 1.4766974583789991e-05,
      "loss": 0.8503,
      "step": 40520
    },
    {
      "epoch": 2.115229558886845,
      "grad_norm": 4.188856601715088,
      "learning_rate": 1.4758276359967296e-05,
      "loss": 0.9039,
      "step": 40530
    },
    {
      "epoch": 2.11575143188905,
      "grad_norm": 4.804287910461426,
      "learning_rate": 1.4749578136144602e-05,
      "loss": 0.896,
      "step": 40540
    },
    {
      "epoch": 2.1162733048912545,
      "grad_norm": 4.054283618927002,
      "learning_rate": 1.4740879912321904e-05,
      "loss": 0.896,
      "step": 40550
    },
    {
      "epoch": 2.1167951778934597,
      "grad_norm": 4.4701409339904785,
      "learning_rate": 1.4732181688499209e-05,
      "loss": 0.8927,
      "step": 40560
    },
    {
      "epoch": 2.1173170508956645,
      "grad_norm": 4.665729522705078,
      "learning_rate": 1.4723483464676512e-05,
      "loss": 0.8359,
      "step": 40570
    },
    {
      "epoch": 2.1178389238978697,
      "grad_norm": 4.73729133605957,
      "learning_rate": 1.4714785240853818e-05,
      "loss": 0.8897,
      "step": 40580
    },
    {
      "epoch": 2.1183607969000744,
      "grad_norm": 5.090782642364502,
      "learning_rate": 1.4706087017031123e-05,
      "loss": 0.831,
      "step": 40590
    },
    {
      "epoch": 2.118882669902279,
      "grad_norm": 3.7286856174468994,
      "learning_rate": 1.4697388793208428e-05,
      "loss": 0.7943,
      "step": 40600
    },
    {
      "epoch": 2.1194045429044843,
      "grad_norm": 4.051360130310059,
      "learning_rate": 1.4688690569385732e-05,
      "loss": 0.7948,
      "step": 40610
    },
    {
      "epoch": 2.119926415906689,
      "grad_norm": 4.947723388671875,
      "learning_rate": 1.4679992345563037e-05,
      "loss": 0.9356,
      "step": 40620
    },
    {
      "epoch": 2.120448288908894,
      "grad_norm": 4.124200344085693,
      "learning_rate": 1.4671294121740342e-05,
      "loss": 0.7585,
      "step": 40630
    },
    {
      "epoch": 2.120970161911099,
      "grad_norm": 4.846542835235596,
      "learning_rate": 1.4662595897917647e-05,
      "loss": 0.7461,
      "step": 40640
    },
    {
      "epoch": 2.1214920349133037,
      "grad_norm": 4.758273124694824,
      "learning_rate": 1.4653897674094953e-05,
      "loss": 0.8975,
      "step": 40650
    },
    {
      "epoch": 2.122013907915509,
      "grad_norm": 4.212950229644775,
      "learning_rate": 1.4645199450272254e-05,
      "loss": 0.8501,
      "step": 40660
    },
    {
      "epoch": 2.1225357809177137,
      "grad_norm": 4.175291061401367,
      "learning_rate": 1.4636501226449558e-05,
      "loss": 0.8017,
      "step": 40670
    },
    {
      "epoch": 2.1230576539199184,
      "grad_norm": 4.619266033172607,
      "learning_rate": 1.4627803002626863e-05,
      "loss": 0.9526,
      "step": 40680
    },
    {
      "epoch": 2.1235795269221236,
      "grad_norm": 4.657547473907471,
      "learning_rate": 1.4619104778804169e-05,
      "loss": 0.8554,
      "step": 40690
    },
    {
      "epoch": 2.1241013999243283,
      "grad_norm": 5.1141180992126465,
      "learning_rate": 1.4610406554981474e-05,
      "loss": 0.899,
      "step": 40700
    },
    {
      "epoch": 2.1246232729265335,
      "grad_norm": 4.666840553283691,
      "learning_rate": 1.4601708331158779e-05,
      "loss": 0.8263,
      "step": 40710
    },
    {
      "epoch": 2.1251451459287383,
      "grad_norm": 4.6207475662231445,
      "learning_rate": 1.4593010107336083e-05,
      "loss": 0.8487,
      "step": 40720
    },
    {
      "epoch": 2.125667018930943,
      "grad_norm": 4.845256328582764,
      "learning_rate": 1.4584311883513388e-05,
      "loss": 0.9142,
      "step": 40730
    },
    {
      "epoch": 2.126188891933148,
      "grad_norm": 4.190451622009277,
      "learning_rate": 1.4575613659690693e-05,
      "loss": 0.9011,
      "step": 40740
    },
    {
      "epoch": 2.126710764935353,
      "grad_norm": 4.465723991394043,
      "learning_rate": 1.4566915435867998e-05,
      "loss": 0.9455,
      "step": 40750
    },
    {
      "epoch": 2.127232637937558,
      "grad_norm": 4.684974670410156,
      "learning_rate": 1.45582172120453e-05,
      "loss": 0.8233,
      "step": 40760
    },
    {
      "epoch": 2.127754510939763,
      "grad_norm": 3.765585422515869,
      "learning_rate": 1.4549518988222605e-05,
      "loss": 0.799,
      "step": 40770
    },
    {
      "epoch": 2.1282763839419676,
      "grad_norm": 3.752804756164551,
      "learning_rate": 1.4540820764399909e-05,
      "loss": 0.8337,
      "step": 40780
    },
    {
      "epoch": 2.128798256944173,
      "grad_norm": 4.2702836990356445,
      "learning_rate": 1.4532122540577214e-05,
      "loss": 0.8316,
      "step": 40790
    },
    {
      "epoch": 2.1293201299463775,
      "grad_norm": 4.437561511993408,
      "learning_rate": 1.452342431675452e-05,
      "loss": 0.9482,
      "step": 40800
    },
    {
      "epoch": 2.1298420029485823,
      "grad_norm": 4.213450908660889,
      "learning_rate": 1.4514726092931825e-05,
      "loss": 0.8441,
      "step": 40810
    },
    {
      "epoch": 2.1303638759507875,
      "grad_norm": 4.670012474060059,
      "learning_rate": 1.4506027869109128e-05,
      "loss": 0.836,
      "step": 40820
    },
    {
      "epoch": 2.130885748952992,
      "grad_norm": 4.765969276428223,
      "learning_rate": 1.4497329645286433e-05,
      "loss": 0.8977,
      "step": 40830
    },
    {
      "epoch": 2.1314076219551974,
      "grad_norm": 3.711430311203003,
      "learning_rate": 1.4488631421463739e-05,
      "loss": 0.8289,
      "step": 40840
    },
    {
      "epoch": 2.131929494957402,
      "grad_norm": 5.002388000488281,
      "learning_rate": 1.4479933197641044e-05,
      "loss": 0.9289,
      "step": 40850
    },
    {
      "epoch": 2.132451367959607,
      "grad_norm": 3.8669426441192627,
      "learning_rate": 1.447123497381835e-05,
      "loss": 0.8937,
      "step": 40860
    },
    {
      "epoch": 2.132973240961812,
      "grad_norm": 4.193953990936279,
      "learning_rate": 1.4462536749995651e-05,
      "loss": 0.804,
      "step": 40870
    },
    {
      "epoch": 2.133495113964017,
      "grad_norm": 4.396733283996582,
      "learning_rate": 1.4453838526172955e-05,
      "loss": 0.8122,
      "step": 40880
    },
    {
      "epoch": 2.1340169869662216,
      "grad_norm": 4.889025688171387,
      "learning_rate": 1.444514030235026e-05,
      "loss": 0.7924,
      "step": 40890
    },
    {
      "epoch": 2.1345388599684267,
      "grad_norm": 4.723783016204834,
      "learning_rate": 1.4436442078527565e-05,
      "loss": 0.857,
      "step": 40900
    },
    {
      "epoch": 2.1350607329706315,
      "grad_norm": 4.174337387084961,
      "learning_rate": 1.442774385470487e-05,
      "loss": 0.7672,
      "step": 40910
    },
    {
      "epoch": 2.1355826059728367,
      "grad_norm": 4.854785442352295,
      "learning_rate": 1.4419045630882176e-05,
      "loss": 0.8039,
      "step": 40920
    },
    {
      "epoch": 2.1361044789750414,
      "grad_norm": 4.360519886016846,
      "learning_rate": 1.4410347407059479e-05,
      "loss": 0.813,
      "step": 40930
    },
    {
      "epoch": 2.136626351977246,
      "grad_norm": 5.828193187713623,
      "learning_rate": 1.4401649183236784e-05,
      "loss": 0.9077,
      "step": 40940
    },
    {
      "epoch": 2.1371482249794513,
      "grad_norm": 4.989020347595215,
      "learning_rate": 1.439295095941409e-05,
      "loss": 0.7763,
      "step": 40950
    },
    {
      "epoch": 2.137670097981656,
      "grad_norm": 5.561542987823486,
      "learning_rate": 1.4384252735591395e-05,
      "loss": 0.8968,
      "step": 40960
    },
    {
      "epoch": 2.1381919709838613,
      "grad_norm": 3.9746475219726562,
      "learning_rate": 1.4375554511768697e-05,
      "loss": 0.8848,
      "step": 40970
    },
    {
      "epoch": 2.138713843986066,
      "grad_norm": 4.692822456359863,
      "learning_rate": 1.4366856287946002e-05,
      "loss": 0.8111,
      "step": 40980
    },
    {
      "epoch": 2.1392357169882708,
      "grad_norm": 4.96327543258667,
      "learning_rate": 1.4358158064123305e-05,
      "loss": 0.9278,
      "step": 40990
    },
    {
      "epoch": 2.139757589990476,
      "grad_norm": 4.234142780303955,
      "learning_rate": 1.434945984030061e-05,
      "loss": 0.8643,
      "step": 41000
    },
    {
      "epoch": 2.1402794629926807,
      "grad_norm": 5.418530464172363,
      "learning_rate": 1.4340761616477916e-05,
      "loss": 0.8689,
      "step": 41010
    },
    {
      "epoch": 2.140801335994886,
      "grad_norm": 4.739896297454834,
      "learning_rate": 1.4332063392655221e-05,
      "loss": 0.8238,
      "step": 41020
    },
    {
      "epoch": 2.1413232089970906,
      "grad_norm": 4.912186145782471,
      "learning_rate": 1.4323365168832525e-05,
      "loss": 0.8965,
      "step": 41030
    },
    {
      "epoch": 2.1418450819992954,
      "grad_norm": 4.5516252517700195,
      "learning_rate": 1.431466694500983e-05,
      "loss": 0.8433,
      "step": 41040
    },
    {
      "epoch": 2.1423669550015005,
      "grad_norm": 3.7927396297454834,
      "learning_rate": 1.4305968721187135e-05,
      "loss": 0.8757,
      "step": 41050
    },
    {
      "epoch": 2.1428888280037053,
      "grad_norm": 4.564826011657715,
      "learning_rate": 1.429727049736444e-05,
      "loss": 0.9072,
      "step": 41060
    },
    {
      "epoch": 2.14341070100591,
      "grad_norm": 4.787129878997803,
      "learning_rate": 1.4288572273541742e-05,
      "loss": 0.8479,
      "step": 41070
    },
    {
      "epoch": 2.143932574008115,
      "grad_norm": 4.632583141326904,
      "learning_rate": 1.4279874049719048e-05,
      "loss": 0.8167,
      "step": 41080
    },
    {
      "epoch": 2.14445444701032,
      "grad_norm": 5.224324703216553,
      "learning_rate": 1.4271175825896351e-05,
      "loss": 0.8403,
      "step": 41090
    },
    {
      "epoch": 2.144976320012525,
      "grad_norm": 4.931112289428711,
      "learning_rate": 1.4262477602073656e-05,
      "loss": 0.8208,
      "step": 41100
    },
    {
      "epoch": 2.14549819301473,
      "grad_norm": 4.430501461029053,
      "learning_rate": 1.4253779378250962e-05,
      "loss": 0.9072,
      "step": 41110
    },
    {
      "epoch": 2.1460200660169346,
      "grad_norm": 4.57651424407959,
      "learning_rate": 1.4245081154428267e-05,
      "loss": 0.8317,
      "step": 41120
    },
    {
      "epoch": 2.14654193901914,
      "grad_norm": 4.548994064331055,
      "learning_rate": 1.423638293060557e-05,
      "loss": 0.8247,
      "step": 41130
    },
    {
      "epoch": 2.1470638120213446,
      "grad_norm": 4.5223212242126465,
      "learning_rate": 1.4227684706782876e-05,
      "loss": 0.8298,
      "step": 41140
    },
    {
      "epoch": 2.1475856850235493,
      "grad_norm": 4.8326897621154785,
      "learning_rate": 1.4218986482960181e-05,
      "loss": 0.8358,
      "step": 41150
    },
    {
      "epoch": 2.1481075580257545,
      "grad_norm": 4.212347507476807,
      "learning_rate": 1.4210288259137486e-05,
      "loss": 0.8826,
      "step": 41160
    },
    {
      "epoch": 2.1486294310279592,
      "grad_norm": 4.554798126220703,
      "learning_rate": 1.4201590035314791e-05,
      "loss": 0.7943,
      "step": 41170
    },
    {
      "epoch": 2.1491513040301644,
      "grad_norm": 4.807229518890381,
      "learning_rate": 1.4192891811492093e-05,
      "loss": 0.8733,
      "step": 41180
    },
    {
      "epoch": 2.149673177032369,
      "grad_norm": 3.4990358352661133,
      "learning_rate": 1.4184193587669398e-05,
      "loss": 0.8175,
      "step": 41190
    },
    {
      "epoch": 2.150195050034574,
      "grad_norm": 4.645532131195068,
      "learning_rate": 1.4175495363846702e-05,
      "loss": 0.8879,
      "step": 41200
    },
    {
      "epoch": 2.150716923036779,
      "grad_norm": 4.73109769821167,
      "learning_rate": 1.4166797140024007e-05,
      "loss": 0.8584,
      "step": 41210
    },
    {
      "epoch": 2.151238796038984,
      "grad_norm": 4.8601789474487305,
      "learning_rate": 1.4158098916201313e-05,
      "loss": 0.8122,
      "step": 41220
    },
    {
      "epoch": 2.151760669041189,
      "grad_norm": 4.915182590484619,
      "learning_rate": 1.4149400692378618e-05,
      "loss": 0.8161,
      "step": 41230
    },
    {
      "epoch": 2.1522825420433938,
      "grad_norm": 4.38178825378418,
      "learning_rate": 1.4140702468555921e-05,
      "loss": 0.8431,
      "step": 41240
    },
    {
      "epoch": 2.1528044150455985,
      "grad_norm": 4.2685956954956055,
      "learning_rate": 1.4132004244733227e-05,
      "loss": 0.8021,
      "step": 41250
    },
    {
      "epoch": 2.1533262880478037,
      "grad_norm": 4.078335285186768,
      "learning_rate": 1.4123306020910532e-05,
      "loss": 0.9699,
      "step": 41260
    },
    {
      "epoch": 2.1538481610500084,
      "grad_norm": 4.707082271575928,
      "learning_rate": 1.4114607797087837e-05,
      "loss": 0.832,
      "step": 41270
    },
    {
      "epoch": 2.1543700340522136,
      "grad_norm": 4.246005058288574,
      "learning_rate": 1.4105909573265139e-05,
      "loss": 0.8745,
      "step": 41280
    },
    {
      "epoch": 2.1548919070544184,
      "grad_norm": 4.521274089813232,
      "learning_rate": 1.4097211349442444e-05,
      "loss": 0.8416,
      "step": 41290
    },
    {
      "epoch": 2.155413780056623,
      "grad_norm": 4.7850494384765625,
      "learning_rate": 1.4088513125619748e-05,
      "loss": 0.8709,
      "step": 41300
    },
    {
      "epoch": 2.1559356530588283,
      "grad_norm": 4.164330959320068,
      "learning_rate": 1.4079814901797053e-05,
      "loss": 0.8371,
      "step": 41310
    },
    {
      "epoch": 2.156457526061033,
      "grad_norm": 4.729741096496582,
      "learning_rate": 1.4071116677974358e-05,
      "loss": 0.9516,
      "step": 41320
    },
    {
      "epoch": 2.1569793990632378,
      "grad_norm": 4.986912727355957,
      "learning_rate": 1.4062418454151663e-05,
      "loss": 0.7789,
      "step": 41330
    },
    {
      "epoch": 2.157501272065443,
      "grad_norm": 5.057315349578857,
      "learning_rate": 1.4053720230328967e-05,
      "loss": 0.8162,
      "step": 41340
    },
    {
      "epoch": 2.1580231450676477,
      "grad_norm": 4.995757579803467,
      "learning_rate": 1.4045022006506272e-05,
      "loss": 0.9199,
      "step": 41350
    },
    {
      "epoch": 2.158545018069853,
      "grad_norm": 4.643167018890381,
      "learning_rate": 1.4036323782683577e-05,
      "loss": 0.8764,
      "step": 41360
    },
    {
      "epoch": 2.1590668910720576,
      "grad_norm": 4.742911338806152,
      "learning_rate": 1.4027625558860883e-05,
      "loss": 0.8761,
      "step": 41370
    },
    {
      "epoch": 2.1595887640742624,
      "grad_norm": 4.397409439086914,
      "learning_rate": 1.4018927335038188e-05,
      "loss": 0.8392,
      "step": 41380
    },
    {
      "epoch": 2.1601106370764676,
      "grad_norm": 4.649298667907715,
      "learning_rate": 1.401022911121549e-05,
      "loss": 0.8877,
      "step": 41390
    },
    {
      "epoch": 2.1606325100786723,
      "grad_norm": 5.10193395614624,
      "learning_rate": 1.4001530887392793e-05,
      "loss": 0.8851,
      "step": 41400
    },
    {
      "epoch": 2.161154383080877,
      "grad_norm": 3.3649215698242188,
      "learning_rate": 1.3992832663570099e-05,
      "loss": 0.7901,
      "step": 41410
    },
    {
      "epoch": 2.1616762560830822,
      "grad_norm": 4.631348609924316,
      "learning_rate": 1.3984134439747404e-05,
      "loss": 0.8562,
      "step": 41420
    },
    {
      "epoch": 2.162198129085287,
      "grad_norm": 4.326648235321045,
      "learning_rate": 1.3975436215924709e-05,
      "loss": 1.0106,
      "step": 41430
    },
    {
      "epoch": 2.162720002087492,
      "grad_norm": 4.699954509735107,
      "learning_rate": 1.3966737992102014e-05,
      "loss": 0.9052,
      "step": 41440
    },
    {
      "epoch": 2.163241875089697,
      "grad_norm": 4.045042991638184,
      "learning_rate": 1.3958039768279318e-05,
      "loss": 0.8814,
      "step": 41450
    },
    {
      "epoch": 2.1637637480919016,
      "grad_norm": 5.941722869873047,
      "learning_rate": 1.3949341544456623e-05,
      "loss": 0.8917,
      "step": 41460
    },
    {
      "epoch": 2.164285621094107,
      "grad_norm": 3.78401517868042,
      "learning_rate": 1.3940643320633928e-05,
      "loss": 0.8117,
      "step": 41470
    },
    {
      "epoch": 2.1648074940963116,
      "grad_norm": 4.099809646606445,
      "learning_rate": 1.3931945096811234e-05,
      "loss": 0.8393,
      "step": 41480
    },
    {
      "epoch": 2.1653293670985168,
      "grad_norm": 4.074374675750732,
      "learning_rate": 1.3923246872988535e-05,
      "loss": 0.8074,
      "step": 41490
    },
    {
      "epoch": 2.1658512401007215,
      "grad_norm": 4.35902738571167,
      "learning_rate": 1.391454864916584e-05,
      "loss": 0.8635,
      "step": 41500
    },
    {
      "epoch": 2.1663731131029262,
      "grad_norm": 3.8487181663513184,
      "learning_rate": 1.3905850425343144e-05,
      "loss": 0.8595,
      "step": 41510
    },
    {
      "epoch": 2.1668949861051314,
      "grad_norm": 5.129374980926514,
      "learning_rate": 1.389715220152045e-05,
      "loss": 0.8376,
      "step": 41520
    },
    {
      "epoch": 2.167416859107336,
      "grad_norm": 4.304446220397949,
      "learning_rate": 1.3888453977697755e-05,
      "loss": 0.8089,
      "step": 41530
    },
    {
      "epoch": 2.1679387321095414,
      "grad_norm": 3.7926182746887207,
      "learning_rate": 1.387975575387506e-05,
      "loss": 0.8409,
      "step": 41540
    },
    {
      "epoch": 2.168460605111746,
      "grad_norm": 4.545909881591797,
      "learning_rate": 1.3871057530052364e-05,
      "loss": 0.8399,
      "step": 41550
    },
    {
      "epoch": 2.168982478113951,
      "grad_norm": 4.787650108337402,
      "learning_rate": 1.3862359306229669e-05,
      "loss": 0.8822,
      "step": 41560
    },
    {
      "epoch": 2.169504351116156,
      "grad_norm": 5.100009918212891,
      "learning_rate": 1.3853661082406974e-05,
      "loss": 0.8512,
      "step": 41570
    },
    {
      "epoch": 2.1700262241183608,
      "grad_norm": 3.989701747894287,
      "learning_rate": 1.384496285858428e-05,
      "loss": 0.8149,
      "step": 41580
    },
    {
      "epoch": 2.1705480971205655,
      "grad_norm": 5.231873512268066,
      "learning_rate": 1.3836264634761581e-05,
      "loss": 0.7694,
      "step": 41590
    },
    {
      "epoch": 2.1710699701227707,
      "grad_norm": 4.662689685821533,
      "learning_rate": 1.3827566410938886e-05,
      "loss": 0.9099,
      "step": 41600
    },
    {
      "epoch": 2.1715918431249754,
      "grad_norm": 4.624289035797119,
      "learning_rate": 1.381886818711619e-05,
      "loss": 0.8872,
      "step": 41610
    },
    {
      "epoch": 2.1721137161271806,
      "grad_norm": 4.660060405731201,
      "learning_rate": 1.3810169963293495e-05,
      "loss": 0.8407,
      "step": 41620
    },
    {
      "epoch": 2.1726355891293854,
      "grad_norm": 5.288376808166504,
      "learning_rate": 1.38014717394708e-05,
      "loss": 0.8803,
      "step": 41630
    },
    {
      "epoch": 2.17315746213159,
      "grad_norm": 5.187073707580566,
      "learning_rate": 1.3792773515648106e-05,
      "loss": 0.7693,
      "step": 41640
    },
    {
      "epoch": 2.1736793351337953,
      "grad_norm": 3.7666447162628174,
      "learning_rate": 1.3784075291825411e-05,
      "loss": 0.7561,
      "step": 41650
    },
    {
      "epoch": 2.174201208136,
      "grad_norm": 4.359990119934082,
      "learning_rate": 1.3775377068002714e-05,
      "loss": 0.9416,
      "step": 41660
    },
    {
      "epoch": 2.174723081138205,
      "grad_norm": 5.692201137542725,
      "learning_rate": 1.376667884418002e-05,
      "loss": 0.8645,
      "step": 41670
    },
    {
      "epoch": 2.17524495414041,
      "grad_norm": 4.251439094543457,
      "learning_rate": 1.3757980620357325e-05,
      "loss": 0.8502,
      "step": 41680
    },
    {
      "epoch": 2.1757668271426147,
      "grad_norm": 5.043875694274902,
      "learning_rate": 1.374928239653463e-05,
      "loss": 0.8349,
      "step": 41690
    },
    {
      "epoch": 2.17628870014482,
      "grad_norm": 4.399950981140137,
      "learning_rate": 1.3740584172711932e-05,
      "loss": 0.806,
      "step": 41700
    },
    {
      "epoch": 2.1768105731470246,
      "grad_norm": 4.350290775299072,
      "learning_rate": 1.3731885948889237e-05,
      "loss": 0.8336,
      "step": 41710
    },
    {
      "epoch": 2.1773324461492294,
      "grad_norm": 4.656682014465332,
      "learning_rate": 1.372318772506654e-05,
      "loss": 0.9471,
      "step": 41720
    },
    {
      "epoch": 2.1778543191514346,
      "grad_norm": 4.550047874450684,
      "learning_rate": 1.3714489501243846e-05,
      "loss": 0.8865,
      "step": 41730
    },
    {
      "epoch": 2.1783761921536393,
      "grad_norm": 4.716023921966553,
      "learning_rate": 1.3705791277421151e-05,
      "loss": 0.8109,
      "step": 41740
    },
    {
      "epoch": 2.1788980651558445,
      "grad_norm": 4.34673547744751,
      "learning_rate": 1.3697093053598457e-05,
      "loss": 0.7961,
      "step": 41750
    },
    {
      "epoch": 2.1794199381580492,
      "grad_norm": 4.528900623321533,
      "learning_rate": 1.368839482977576e-05,
      "loss": 0.9673,
      "step": 41760
    },
    {
      "epoch": 2.179941811160254,
      "grad_norm": 5.057579517364502,
      "learning_rate": 1.3679696605953065e-05,
      "loss": 0.8765,
      "step": 41770
    },
    {
      "epoch": 2.180463684162459,
      "grad_norm": 4.239817142486572,
      "learning_rate": 1.367099838213037e-05,
      "loss": 0.8337,
      "step": 41780
    },
    {
      "epoch": 2.180985557164664,
      "grad_norm": 4.042840480804443,
      "learning_rate": 1.3662300158307676e-05,
      "loss": 0.7771,
      "step": 41790
    },
    {
      "epoch": 2.181507430166869,
      "grad_norm": 4.2809882164001465,
      "learning_rate": 1.3653601934484978e-05,
      "loss": 0.8277,
      "step": 41800
    },
    {
      "epoch": 2.182029303169074,
      "grad_norm": 4.925380229949951,
      "learning_rate": 1.3644903710662283e-05,
      "loss": 0.7936,
      "step": 41810
    },
    {
      "epoch": 2.1825511761712786,
      "grad_norm": 4.859100818634033,
      "learning_rate": 1.3636205486839586e-05,
      "loss": 0.889,
      "step": 41820
    },
    {
      "epoch": 2.1830730491734838,
      "grad_norm": 4.373752117156982,
      "learning_rate": 1.3627507263016892e-05,
      "loss": 0.8795,
      "step": 41830
    },
    {
      "epoch": 2.1835949221756885,
      "grad_norm": 4.386699199676514,
      "learning_rate": 1.3618809039194197e-05,
      "loss": 0.8568,
      "step": 41840
    },
    {
      "epoch": 2.1841167951778933,
      "grad_norm": 4.457125186920166,
      "learning_rate": 1.3610110815371502e-05,
      "loss": 0.8455,
      "step": 41850
    },
    {
      "epoch": 2.1846386681800984,
      "grad_norm": 4.67177677154541,
      "learning_rate": 1.3601412591548807e-05,
      "loss": 0.8918,
      "step": 41860
    },
    {
      "epoch": 2.185160541182303,
      "grad_norm": 4.000875949859619,
      "learning_rate": 1.3592714367726111e-05,
      "loss": 0.8467,
      "step": 41870
    },
    {
      "epoch": 2.1856824141845084,
      "grad_norm": 4.082926273345947,
      "learning_rate": 1.3584016143903416e-05,
      "loss": 0.9017,
      "step": 41880
    },
    {
      "epoch": 2.186204287186713,
      "grad_norm": 5.025332450866699,
      "learning_rate": 1.3575317920080721e-05,
      "loss": 0.8546,
      "step": 41890
    },
    {
      "epoch": 2.186726160188918,
      "grad_norm": 5.894900321960449,
      "learning_rate": 1.3566619696258027e-05,
      "loss": 0.9221,
      "step": 41900
    },
    {
      "epoch": 2.187248033191123,
      "grad_norm": 4.88790225982666,
      "learning_rate": 1.3557921472435329e-05,
      "loss": 0.8346,
      "step": 41910
    },
    {
      "epoch": 2.187769906193328,
      "grad_norm": 4.582520008087158,
      "learning_rate": 1.3549223248612634e-05,
      "loss": 0.7833,
      "step": 41920
    },
    {
      "epoch": 2.188291779195533,
      "grad_norm": 4.520956039428711,
      "learning_rate": 1.3540525024789937e-05,
      "loss": 0.8349,
      "step": 41930
    },
    {
      "epoch": 2.1888136521977377,
      "grad_norm": 4.155943393707275,
      "learning_rate": 1.3531826800967243e-05,
      "loss": 0.8673,
      "step": 41940
    },
    {
      "epoch": 2.1893355251999425,
      "grad_norm": 5.220656871795654,
      "learning_rate": 1.3523128577144548e-05,
      "loss": 0.8425,
      "step": 41950
    },
    {
      "epoch": 2.1898573982021476,
      "grad_norm": 4.286341190338135,
      "learning_rate": 1.3514430353321853e-05,
      "loss": 0.8341,
      "step": 41960
    },
    {
      "epoch": 2.1903792712043524,
      "grad_norm": 4.639873504638672,
      "learning_rate": 1.3505732129499157e-05,
      "loss": 0.8135,
      "step": 41970
    },
    {
      "epoch": 2.190901144206557,
      "grad_norm": 4.079619884490967,
      "learning_rate": 1.3497033905676462e-05,
      "loss": 0.849,
      "step": 41980
    },
    {
      "epoch": 2.1914230172087623,
      "grad_norm": 4.7368693351745605,
      "learning_rate": 1.3488335681853767e-05,
      "loss": 0.8895,
      "step": 41990
    },
    {
      "epoch": 2.191944890210967,
      "grad_norm": 4.7927374839782715,
      "learning_rate": 1.3479637458031072e-05,
      "loss": 0.8479,
      "step": 42000
    },
    {
      "epoch": 2.1924667632131722,
      "grad_norm": 3.8185038566589355,
      "learning_rate": 1.3470939234208374e-05,
      "loss": 0.8754,
      "step": 42010
    },
    {
      "epoch": 2.192988636215377,
      "grad_norm": 5.174910068511963,
      "learning_rate": 1.346224101038568e-05,
      "loss": 0.7716,
      "step": 42020
    },
    {
      "epoch": 2.1935105092175817,
      "grad_norm": 4.159278869628906,
      "learning_rate": 1.3453542786562983e-05,
      "loss": 0.8385,
      "step": 42030
    },
    {
      "epoch": 2.194032382219787,
      "grad_norm": 4.511744499206543,
      "learning_rate": 1.3444844562740288e-05,
      "loss": 0.8157,
      "step": 42040
    },
    {
      "epoch": 2.1945542552219917,
      "grad_norm": 4.461395740509033,
      "learning_rate": 1.3436146338917593e-05,
      "loss": 0.8866,
      "step": 42050
    },
    {
      "epoch": 2.195076128224197,
      "grad_norm": 4.4088897705078125,
      "learning_rate": 1.3427448115094899e-05,
      "loss": 0.7515,
      "step": 42060
    },
    {
      "epoch": 2.1955980012264016,
      "grad_norm": 5.069671154022217,
      "learning_rate": 1.3418749891272204e-05,
      "loss": 0.8688,
      "step": 42070
    },
    {
      "epoch": 2.1961198742286063,
      "grad_norm": 3.8884406089782715,
      "learning_rate": 1.3410051667449508e-05,
      "loss": 0.8091,
      "step": 42080
    },
    {
      "epoch": 2.1966417472308115,
      "grad_norm": 5.616210460662842,
      "learning_rate": 1.3401353443626813e-05,
      "loss": 0.8893,
      "step": 42090
    },
    {
      "epoch": 2.1971636202330163,
      "grad_norm": 3.7862274646759033,
      "learning_rate": 1.3392655219804118e-05,
      "loss": 0.798,
      "step": 42100
    },
    {
      "epoch": 2.1976854932352214,
      "grad_norm": 5.153355598449707,
      "learning_rate": 1.3383956995981423e-05,
      "loss": 0.8314,
      "step": 42110
    },
    {
      "epoch": 2.198207366237426,
      "grad_norm": 6.799341201782227,
      "learning_rate": 1.3375258772158725e-05,
      "loss": 0.8891,
      "step": 42120
    },
    {
      "epoch": 2.198729239239631,
      "grad_norm": 4.815983772277832,
      "learning_rate": 1.336656054833603e-05,
      "loss": 0.7801,
      "step": 42130
    },
    {
      "epoch": 2.199251112241836,
      "grad_norm": 4.385842323303223,
      "learning_rate": 1.3357862324513334e-05,
      "loss": 0.8615,
      "step": 42140
    },
    {
      "epoch": 2.199772985244041,
      "grad_norm": 4.88196325302124,
      "learning_rate": 1.3349164100690639e-05,
      "loss": 0.871,
      "step": 42150
    },
    {
      "epoch": 2.2002948582462456,
      "grad_norm": 4.085361957550049,
      "learning_rate": 1.3340465876867944e-05,
      "loss": 0.9003,
      "step": 42160
    },
    {
      "epoch": 2.200816731248451,
      "grad_norm": 4.585059642791748,
      "learning_rate": 1.333176765304525e-05,
      "loss": 0.9525,
      "step": 42170
    },
    {
      "epoch": 2.2013386042506555,
      "grad_norm": 3.7774736881256104,
      "learning_rate": 1.3323069429222553e-05,
      "loss": 0.7574,
      "step": 42180
    },
    {
      "epoch": 2.2018604772528607,
      "grad_norm": 4.89665412902832,
      "learning_rate": 1.3314371205399858e-05,
      "loss": 0.8425,
      "step": 42190
    },
    {
      "epoch": 2.2023823502550655,
      "grad_norm": 4.900243282318115,
      "learning_rate": 1.3305672981577164e-05,
      "loss": 0.8596,
      "step": 42200
    },
    {
      "epoch": 2.20290422325727,
      "grad_norm": 4.743905544281006,
      "learning_rate": 1.3296974757754469e-05,
      "loss": 0.872,
      "step": 42210
    },
    {
      "epoch": 2.2034260962594754,
      "grad_norm": 4.264583110809326,
      "learning_rate": 1.328827653393177e-05,
      "loss": 0.8427,
      "step": 42220
    },
    {
      "epoch": 2.20394796926168,
      "grad_norm": 4.205019950866699,
      "learning_rate": 1.3279578310109076e-05,
      "loss": 0.8533,
      "step": 42230
    },
    {
      "epoch": 2.204469842263885,
      "grad_norm": 4.670494079589844,
      "learning_rate": 1.327088008628638e-05,
      "loss": 0.8669,
      "step": 42240
    },
    {
      "epoch": 2.20499171526609,
      "grad_norm": 4.736777305603027,
      "learning_rate": 1.3262181862463685e-05,
      "loss": 0.807,
      "step": 42250
    },
    {
      "epoch": 2.205513588268295,
      "grad_norm": 4.834983825683594,
      "learning_rate": 1.325348363864099e-05,
      "loss": 0.843,
      "step": 42260
    },
    {
      "epoch": 2.2060354612705,
      "grad_norm": 4.696111679077148,
      "learning_rate": 1.3244785414818295e-05,
      "loss": 0.8532,
      "step": 42270
    },
    {
      "epoch": 2.2065573342727047,
      "grad_norm": 4.82494592666626,
      "learning_rate": 1.32360871909956e-05,
      "loss": 0.8908,
      "step": 42280
    },
    {
      "epoch": 2.2070792072749095,
      "grad_norm": 4.8504767417907715,
      "learning_rate": 1.3227388967172904e-05,
      "loss": 0.8466,
      "step": 42290
    },
    {
      "epoch": 2.2076010802771147,
      "grad_norm": 4.21274995803833,
      "learning_rate": 1.321869074335021e-05,
      "loss": 0.8054,
      "step": 42300
    },
    {
      "epoch": 2.2081229532793194,
      "grad_norm": 5.345791339874268,
      "learning_rate": 1.3209992519527515e-05,
      "loss": 0.8836,
      "step": 42310
    },
    {
      "epoch": 2.2086448262815246,
      "grad_norm": 4.477675437927246,
      "learning_rate": 1.3201294295704816e-05,
      "loss": 0.9866,
      "step": 42320
    },
    {
      "epoch": 2.2091666992837293,
      "grad_norm": 4.547488212585449,
      "learning_rate": 1.3192596071882122e-05,
      "loss": 0.8412,
      "step": 42330
    },
    {
      "epoch": 2.209688572285934,
      "grad_norm": 4.363590717315674,
      "learning_rate": 1.3183897848059427e-05,
      "loss": 0.9109,
      "step": 42340
    },
    {
      "epoch": 2.2102104452881393,
      "grad_norm": 5.8626885414123535,
      "learning_rate": 1.317519962423673e-05,
      "loss": 1.0235,
      "step": 42350
    },
    {
      "epoch": 2.210732318290344,
      "grad_norm": 4.958362102508545,
      "learning_rate": 1.3166501400414036e-05,
      "loss": 0.7854,
      "step": 42360
    },
    {
      "epoch": 2.211254191292549,
      "grad_norm": 5.46560525894165,
      "learning_rate": 1.3157803176591341e-05,
      "loss": 0.8773,
      "step": 42370
    },
    {
      "epoch": 2.211776064294754,
      "grad_norm": 4.186777114868164,
      "learning_rate": 1.3149104952768646e-05,
      "loss": 0.8604,
      "step": 42380
    },
    {
      "epoch": 2.2122979372969587,
      "grad_norm": 4.547131061553955,
      "learning_rate": 1.314040672894595e-05,
      "loss": 0.8523,
      "step": 42390
    },
    {
      "epoch": 2.212819810299164,
      "grad_norm": 3.7450098991394043,
      "learning_rate": 1.3131708505123255e-05,
      "loss": 0.8338,
      "step": 42400
    },
    {
      "epoch": 2.2133416833013686,
      "grad_norm": 5.084625244140625,
      "learning_rate": 1.312301028130056e-05,
      "loss": 0.8844,
      "step": 42410
    },
    {
      "epoch": 2.2138635563035733,
      "grad_norm": 4.388841152191162,
      "learning_rate": 1.3114312057477865e-05,
      "loss": 0.736,
      "step": 42420
    },
    {
      "epoch": 2.2143854293057785,
      "grad_norm": 4.903294563293457,
      "learning_rate": 1.3105613833655167e-05,
      "loss": 0.8546,
      "step": 42430
    },
    {
      "epoch": 2.2149073023079833,
      "grad_norm": 4.715198516845703,
      "learning_rate": 1.3096915609832473e-05,
      "loss": 0.7846,
      "step": 42440
    },
    {
      "epoch": 2.2154291753101885,
      "grad_norm": 3.933685302734375,
      "learning_rate": 1.3088217386009776e-05,
      "loss": 0.8115,
      "step": 42450
    },
    {
      "epoch": 2.215951048312393,
      "grad_norm": 4.773382663726807,
      "learning_rate": 1.3079519162187081e-05,
      "loss": 0.8171,
      "step": 42460
    },
    {
      "epoch": 2.216472921314598,
      "grad_norm": 4.434498310089111,
      "learning_rate": 1.3070820938364387e-05,
      "loss": 0.7824,
      "step": 42470
    },
    {
      "epoch": 2.216994794316803,
      "grad_norm": 4.546049118041992,
      "learning_rate": 1.3062122714541692e-05,
      "loss": 0.8768,
      "step": 42480
    },
    {
      "epoch": 2.217516667319008,
      "grad_norm": 4.741227149963379,
      "learning_rate": 1.3053424490718995e-05,
      "loss": 0.8875,
      "step": 42490
    },
    {
      "epoch": 2.2180385403212126,
      "grad_norm": 5.235148906707764,
      "learning_rate": 1.30447262668963e-05,
      "loss": 0.8538,
      "step": 42500
    },
    {
      "epoch": 2.218560413323418,
      "grad_norm": 4.466457843780518,
      "learning_rate": 1.3036028043073606e-05,
      "loss": 0.9105,
      "step": 42510
    },
    {
      "epoch": 2.2190822863256225,
      "grad_norm": 4.43363094329834,
      "learning_rate": 1.3027329819250911e-05,
      "loss": 0.8877,
      "step": 42520
    },
    {
      "epoch": 2.2196041593278277,
      "grad_norm": 4.390783786773682,
      "learning_rate": 1.3018631595428213e-05,
      "loss": 0.7993,
      "step": 42530
    },
    {
      "epoch": 2.2201260323300325,
      "grad_norm": 4.684945583343506,
      "learning_rate": 1.3009933371605518e-05,
      "loss": 0.833,
      "step": 42540
    },
    {
      "epoch": 2.220647905332237,
      "grad_norm": 4.538883686065674,
      "learning_rate": 1.3001235147782822e-05,
      "loss": 0.8791,
      "step": 42550
    },
    {
      "epoch": 2.2211697783344424,
      "grad_norm": 5.77951192855835,
      "learning_rate": 1.2992536923960127e-05,
      "loss": 0.8205,
      "step": 42560
    },
    {
      "epoch": 2.221691651336647,
      "grad_norm": 4.4166436195373535,
      "learning_rate": 1.2983838700137432e-05,
      "loss": 0.8849,
      "step": 42570
    },
    {
      "epoch": 2.2222135243388523,
      "grad_norm": 4.328357219696045,
      "learning_rate": 1.2975140476314737e-05,
      "loss": 0.8295,
      "step": 42580
    },
    {
      "epoch": 2.222735397341057,
      "grad_norm": 4.507734775543213,
      "learning_rate": 1.2966442252492043e-05,
      "loss": 0.8555,
      "step": 42590
    },
    {
      "epoch": 2.223257270343262,
      "grad_norm": 4.102365970611572,
      "learning_rate": 1.2957744028669346e-05,
      "loss": 0.8762,
      "step": 42600
    },
    {
      "epoch": 2.223779143345467,
      "grad_norm": 4.573253631591797,
      "learning_rate": 1.2949045804846652e-05,
      "loss": 0.849,
      "step": 42610
    },
    {
      "epoch": 2.2243010163476717,
      "grad_norm": 4.908905506134033,
      "learning_rate": 1.2940347581023957e-05,
      "loss": 0.8839,
      "step": 42620
    },
    {
      "epoch": 2.224822889349877,
      "grad_norm": 4.816678524017334,
      "learning_rate": 1.2931649357201262e-05,
      "loss": 0.8759,
      "step": 42630
    },
    {
      "epoch": 2.2253447623520817,
      "grad_norm": 3.557504653930664,
      "learning_rate": 1.2922951133378564e-05,
      "loss": 0.8954,
      "step": 42640
    },
    {
      "epoch": 2.2258666353542864,
      "grad_norm": 4.629965305328369,
      "learning_rate": 1.2914252909555869e-05,
      "loss": 0.8643,
      "step": 42650
    },
    {
      "epoch": 2.2263885083564916,
      "grad_norm": 4.485665321350098,
      "learning_rate": 1.2905554685733173e-05,
      "loss": 0.8122,
      "step": 42660
    },
    {
      "epoch": 2.2269103813586963,
      "grad_norm": 4.801854610443115,
      "learning_rate": 1.2896856461910478e-05,
      "loss": 0.8449,
      "step": 42670
    },
    {
      "epoch": 2.227432254360901,
      "grad_norm": 4.400117874145508,
      "learning_rate": 1.2888158238087783e-05,
      "loss": 0.7361,
      "step": 42680
    },
    {
      "epoch": 2.2279541273631063,
      "grad_norm": 3.667255163192749,
      "learning_rate": 1.2879460014265088e-05,
      "loss": 0.8202,
      "step": 42690
    },
    {
      "epoch": 2.228476000365311,
      "grad_norm": 4.036479949951172,
      "learning_rate": 1.2870761790442392e-05,
      "loss": 0.8005,
      "step": 42700
    },
    {
      "epoch": 2.228997873367516,
      "grad_norm": 4.652451992034912,
      "learning_rate": 1.2862063566619697e-05,
      "loss": 0.8072,
      "step": 42710
    },
    {
      "epoch": 2.229519746369721,
      "grad_norm": 3.6873345375061035,
      "learning_rate": 1.2853365342797002e-05,
      "loss": 0.7306,
      "step": 42720
    },
    {
      "epoch": 2.2300416193719257,
      "grad_norm": 4.102584362030029,
      "learning_rate": 1.2844667118974308e-05,
      "loss": 0.8066,
      "step": 42730
    },
    {
      "epoch": 2.230563492374131,
      "grad_norm": 4.6896162033081055,
      "learning_rate": 1.283596889515161e-05,
      "loss": 0.8471,
      "step": 42740
    },
    {
      "epoch": 2.2310853653763356,
      "grad_norm": 5.577451229095459,
      "learning_rate": 1.2827270671328915e-05,
      "loss": 0.856,
      "step": 42750
    },
    {
      "epoch": 2.2316072383785404,
      "grad_norm": 4.871917724609375,
      "learning_rate": 1.2818572447506218e-05,
      "loss": 0.9335,
      "step": 42760
    },
    {
      "epoch": 2.2321291113807455,
      "grad_norm": 4.485328674316406,
      "learning_rate": 1.2809874223683524e-05,
      "loss": 0.7969,
      "step": 42770
    },
    {
      "epoch": 2.2326509843829503,
      "grad_norm": 4.897760391235352,
      "learning_rate": 1.2801175999860829e-05,
      "loss": 0.9094,
      "step": 42780
    },
    {
      "epoch": 2.2331728573851555,
      "grad_norm": 4.832489013671875,
      "learning_rate": 1.2792477776038134e-05,
      "loss": 0.8643,
      "step": 42790
    },
    {
      "epoch": 2.23369473038736,
      "grad_norm": 4.609407901763916,
      "learning_rate": 1.278377955221544e-05,
      "loss": 0.8234,
      "step": 42800
    },
    {
      "epoch": 2.234216603389565,
      "grad_norm": 4.030649185180664,
      "learning_rate": 1.2775081328392743e-05,
      "loss": 0.8493,
      "step": 42810
    },
    {
      "epoch": 2.23473847639177,
      "grad_norm": 4.893064022064209,
      "learning_rate": 1.2766383104570048e-05,
      "loss": 0.8001,
      "step": 42820
    },
    {
      "epoch": 2.235260349393975,
      "grad_norm": 4.356618404388428,
      "learning_rate": 1.2757684880747353e-05,
      "loss": 0.8411,
      "step": 42830
    },
    {
      "epoch": 2.23578222239618,
      "grad_norm": 5.122181415557861,
      "learning_rate": 1.2748986656924655e-05,
      "loss": 0.8202,
      "step": 42840
    },
    {
      "epoch": 2.236304095398385,
      "grad_norm": 4.482151985168457,
      "learning_rate": 1.274028843310196e-05,
      "loss": 0.887,
      "step": 42850
    },
    {
      "epoch": 2.2368259684005896,
      "grad_norm": 3.7707886695861816,
      "learning_rate": 1.2732460031661535e-05,
      "loss": 0.8027,
      "step": 42860
    },
    {
      "epoch": 2.2373478414027947,
      "grad_norm": 4.4157843589782715,
      "learning_rate": 1.272376180783884e-05,
      "loss": 0.8137,
      "step": 42870
    },
    {
      "epoch": 2.2378697144049995,
      "grad_norm": 4.963201522827148,
      "learning_rate": 1.2715063584016146e-05,
      "loss": 0.7858,
      "step": 42880
    },
    {
      "epoch": 2.2383915874072047,
      "grad_norm": 4.258410453796387,
      "learning_rate": 1.2706365360193448e-05,
      "loss": 0.7865,
      "step": 42890
    },
    {
      "epoch": 2.2389134604094094,
      "grad_norm": 5.556198596954346,
      "learning_rate": 1.2697667136370753e-05,
      "loss": 0.8198,
      "step": 42900
    },
    {
      "epoch": 2.239435333411614,
      "grad_norm": 4.101747989654541,
      "learning_rate": 1.2688968912548058e-05,
      "loss": 0.837,
      "step": 42910
    },
    {
      "epoch": 2.2399572064138193,
      "grad_norm": 4.5894551277160645,
      "learning_rate": 1.2680270688725362e-05,
      "loss": 0.9618,
      "step": 42920
    },
    {
      "epoch": 2.240479079416024,
      "grad_norm": 4.605129241943359,
      "learning_rate": 1.2671572464902667e-05,
      "loss": 0.8654,
      "step": 42930
    },
    {
      "epoch": 2.241000952418229,
      "grad_norm": 4.303630352020264,
      "learning_rate": 1.2662874241079972e-05,
      "loss": 0.8453,
      "step": 42940
    },
    {
      "epoch": 2.241522825420434,
      "grad_norm": 3.717639923095703,
      "learning_rate": 1.2654176017257277e-05,
      "loss": 0.8337,
      "step": 42950
    },
    {
      "epoch": 2.2420446984226388,
      "grad_norm": 5.187131881713867,
      "learning_rate": 1.2645477793434583e-05,
      "loss": 0.8929,
      "step": 42960
    },
    {
      "epoch": 2.242566571424844,
      "grad_norm": 4.232997894287109,
      "learning_rate": 1.2636779569611886e-05,
      "loss": 0.7576,
      "step": 42970
    },
    {
      "epoch": 2.2430884444270487,
      "grad_norm": 3.948146104812622,
      "learning_rate": 1.2628081345789191e-05,
      "loss": 0.845,
      "step": 42980
    },
    {
      "epoch": 2.2436103174292534,
      "grad_norm": 4.2839202880859375,
      "learning_rate": 1.2619383121966493e-05,
      "loss": 0.8582,
      "step": 42990
    },
    {
      "epoch": 2.2441321904314586,
      "grad_norm": 4.385918617248535,
      "learning_rate": 1.2610684898143799e-05,
      "loss": 0.7935,
      "step": 43000
    },
    {
      "epoch": 2.2446540634336634,
      "grad_norm": 4.433445453643799,
      "learning_rate": 1.2601986674321104e-05,
      "loss": 0.8224,
      "step": 43010
    },
    {
      "epoch": 2.245175936435868,
      "grad_norm": 4.104354381561279,
      "learning_rate": 1.2593288450498409e-05,
      "loss": 0.9425,
      "step": 43020
    },
    {
      "epoch": 2.2456978094380733,
      "grad_norm": 4.834204196929932,
      "learning_rate": 1.2584590226675713e-05,
      "loss": 0.8084,
      "step": 43030
    },
    {
      "epoch": 2.246219682440278,
      "grad_norm": 5.447824001312256,
      "learning_rate": 1.2575892002853018e-05,
      "loss": 0.9028,
      "step": 43040
    },
    {
      "epoch": 2.246741555442483,
      "grad_norm": 4.910114288330078,
      "learning_rate": 1.2567193779030323e-05,
      "loss": 0.8595,
      "step": 43050
    },
    {
      "epoch": 2.247263428444688,
      "grad_norm": 4.584636688232422,
      "learning_rate": 1.2558495555207628e-05,
      "loss": 0.8592,
      "step": 43060
    },
    {
      "epoch": 2.2477853014468927,
      "grad_norm": 4.170896530151367,
      "learning_rate": 1.2549797331384932e-05,
      "loss": 0.7993,
      "step": 43070
    },
    {
      "epoch": 2.248307174449098,
      "grad_norm": 4.416297435760498,
      "learning_rate": 1.2541099107562237e-05,
      "loss": 0.7337,
      "step": 43080
    },
    {
      "epoch": 2.2488290474513026,
      "grad_norm": 4.712620258331299,
      "learning_rate": 1.2532400883739542e-05,
      "loss": 0.9166,
      "step": 43090
    },
    {
      "epoch": 2.249350920453508,
      "grad_norm": 4.719457149505615,
      "learning_rate": 1.2523702659916844e-05,
      "loss": 0.8631,
      "step": 43100
    },
    {
      "epoch": 2.2498727934557126,
      "grad_norm": 3.8715269565582275,
      "learning_rate": 1.251500443609415e-05,
      "loss": 0.7129,
      "step": 43110
    },
    {
      "epoch": 2.2503946664579173,
      "grad_norm": 4.705843448638916,
      "learning_rate": 1.2506306212271455e-05,
      "loss": 0.8805,
      "step": 43120
    },
    {
      "epoch": 2.2509165394601225,
      "grad_norm": 3.789607524871826,
      "learning_rate": 1.2497607988448758e-05,
      "loss": 0.8973,
      "step": 43130
    },
    {
      "epoch": 2.2514384124623272,
      "grad_norm": 5.019685745239258,
      "learning_rate": 1.2488909764626063e-05,
      "loss": 0.8323,
      "step": 43140
    },
    {
      "epoch": 2.2519602854645324,
      "grad_norm": 4.3112101554870605,
      "learning_rate": 1.2480211540803369e-05,
      "loss": 0.8721,
      "step": 43150
    },
    {
      "epoch": 2.252482158466737,
      "grad_norm": 4.546873092651367,
      "learning_rate": 1.2471513316980674e-05,
      "loss": 0.7636,
      "step": 43160
    },
    {
      "epoch": 2.253004031468942,
      "grad_norm": 4.786699295043945,
      "learning_rate": 1.2462815093157978e-05,
      "loss": 0.8408,
      "step": 43170
    },
    {
      "epoch": 2.253525904471147,
      "grad_norm": 3.8574917316436768,
      "learning_rate": 1.2454116869335283e-05,
      "loss": 0.8019,
      "step": 43180
    },
    {
      "epoch": 2.254047777473352,
      "grad_norm": 5.135489463806152,
      "learning_rate": 1.2445418645512586e-05,
      "loss": 0.9519,
      "step": 43190
    },
    {
      "epoch": 2.254569650475557,
      "grad_norm": 4.599483013153076,
      "learning_rate": 1.2436720421689892e-05,
      "loss": 0.8688,
      "step": 43200
    },
    {
      "epoch": 2.2550915234777618,
      "grad_norm": 4.67548942565918,
      "learning_rate": 1.2428022197867197e-05,
      "loss": 0.7836,
      "step": 43210
    },
    {
      "epoch": 2.2556133964799665,
      "grad_norm": 5.064045429229736,
      "learning_rate": 1.2419323974044502e-05,
      "loss": 0.8568,
      "step": 43220
    },
    {
      "epoch": 2.2561352694821717,
      "grad_norm": 4.668240547180176,
      "learning_rate": 1.2410625750221806e-05,
      "loss": 0.9034,
      "step": 43230
    },
    {
      "epoch": 2.2566571424843764,
      "grad_norm": 4.5272297859191895,
      "learning_rate": 1.2401927526399109e-05,
      "loss": 0.965,
      "step": 43240
    },
    {
      "epoch": 2.257179015486581,
      "grad_norm": 4.547530174255371,
      "learning_rate": 1.2393229302576414e-05,
      "loss": 0.8706,
      "step": 43250
    },
    {
      "epoch": 2.2577008884887864,
      "grad_norm": 4.789585590362549,
      "learning_rate": 1.238453107875372e-05,
      "loss": 0.9047,
      "step": 43260
    },
    {
      "epoch": 2.258222761490991,
      "grad_norm": 5.187839031219482,
      "learning_rate": 1.2375832854931025e-05,
      "loss": 0.8605,
      "step": 43270
    },
    {
      "epoch": 2.258744634493196,
      "grad_norm": 4.487208843231201,
      "learning_rate": 1.2367134631108328e-05,
      "loss": 0.815,
      "step": 43280
    },
    {
      "epoch": 2.259266507495401,
      "grad_norm": 4.887330055236816,
      "learning_rate": 1.2358436407285632e-05,
      "loss": 0.8002,
      "step": 43290
    },
    {
      "epoch": 2.2597883804976058,
      "grad_norm": 4.959173202514648,
      "learning_rate": 1.2349738183462937e-05,
      "loss": 0.8643,
      "step": 43300
    },
    {
      "epoch": 2.260310253499811,
      "grad_norm": 4.682300567626953,
      "learning_rate": 1.2341039959640242e-05,
      "loss": 0.8335,
      "step": 43310
    },
    {
      "epoch": 2.2608321265020157,
      "grad_norm": 4.677105903625488,
      "learning_rate": 1.2332341735817548e-05,
      "loss": 0.7824,
      "step": 43320
    },
    {
      "epoch": 2.2613539995042204,
      "grad_norm": 4.332658767700195,
      "learning_rate": 1.2323643511994851e-05,
      "loss": 0.784,
      "step": 43330
    },
    {
      "epoch": 2.2618758725064256,
      "grad_norm": 4.304277420043945,
      "learning_rate": 1.2314945288172155e-05,
      "loss": 0.8261,
      "step": 43340
    },
    {
      "epoch": 2.2623977455086304,
      "grad_norm": 3.9705092906951904,
      "learning_rate": 1.230624706434946e-05,
      "loss": 0.8143,
      "step": 43350
    },
    {
      "epoch": 2.2629196185108356,
      "grad_norm": 5.1328935623168945,
      "learning_rate": 1.2297548840526765e-05,
      "loss": 0.8221,
      "step": 43360
    },
    {
      "epoch": 2.2634414915130403,
      "grad_norm": 4.0851521492004395,
      "learning_rate": 1.228885061670407e-05,
      "loss": 0.8219,
      "step": 43370
    },
    {
      "epoch": 2.263963364515245,
      "grad_norm": 4.5890326499938965,
      "learning_rate": 1.2280152392881374e-05,
      "loss": 0.8499,
      "step": 43380
    },
    {
      "epoch": 2.2644852375174502,
      "grad_norm": 5.396193981170654,
      "learning_rate": 1.2271454169058678e-05,
      "loss": 0.844,
      "step": 43390
    },
    {
      "epoch": 2.265007110519655,
      "grad_norm": 4.456335544586182,
      "learning_rate": 1.2262755945235983e-05,
      "loss": 0.861,
      "step": 43400
    },
    {
      "epoch": 2.26552898352186,
      "grad_norm": 4.362011909484863,
      "learning_rate": 1.2254057721413288e-05,
      "loss": 0.8943,
      "step": 43410
    },
    {
      "epoch": 2.266050856524065,
      "grad_norm": 5.408658981323242,
      "learning_rate": 1.2245359497590593e-05,
      "loss": 0.9582,
      "step": 43420
    },
    {
      "epoch": 2.2665727295262696,
      "grad_norm": 5.453146934509277,
      "learning_rate": 1.2236661273767897e-05,
      "loss": 0.9518,
      "step": 43430
    },
    {
      "epoch": 2.267094602528475,
      "grad_norm": 4.373194217681885,
      "learning_rate": 1.2227963049945202e-05,
      "loss": 0.7542,
      "step": 43440
    },
    {
      "epoch": 2.2676164755306796,
      "grad_norm": 4.201480388641357,
      "learning_rate": 1.2219264826122506e-05,
      "loss": 0.839,
      "step": 43450
    },
    {
      "epoch": 2.2681383485328848,
      "grad_norm": 4.659914970397949,
      "learning_rate": 1.2210566602299811e-05,
      "loss": 0.842,
      "step": 43460
    },
    {
      "epoch": 2.2686602215350895,
      "grad_norm": 5.093718528747559,
      "learning_rate": 1.2201868378477116e-05,
      "loss": 0.7886,
      "step": 43470
    },
    {
      "epoch": 2.2691820945372942,
      "grad_norm": 4.569889068603516,
      "learning_rate": 1.2193170154654421e-05,
      "loss": 0.7629,
      "step": 43480
    },
    {
      "epoch": 2.2697039675394994,
      "grad_norm": 4.683718204498291,
      "learning_rate": 1.2184471930831725e-05,
      "loss": 0.8374,
      "step": 43490
    },
    {
      "epoch": 2.270225840541704,
      "grad_norm": 4.486150741577148,
      "learning_rate": 1.2175773707009029e-05,
      "loss": 0.7913,
      "step": 43500
    },
    {
      "epoch": 2.270747713543909,
      "grad_norm": 4.4651570320129395,
      "learning_rate": 1.2167075483186334e-05,
      "loss": 0.916,
      "step": 43510
    },
    {
      "epoch": 2.271269586546114,
      "grad_norm": 4.750875473022461,
      "learning_rate": 1.2158377259363639e-05,
      "loss": 0.9099,
      "step": 43520
    },
    {
      "epoch": 2.271791459548319,
      "grad_norm": 5.558835983276367,
      "learning_rate": 1.2149679035540944e-05,
      "loss": 0.8874,
      "step": 43530
    },
    {
      "epoch": 2.2723133325505236,
      "grad_norm": 3.86090087890625,
      "learning_rate": 1.2140980811718248e-05,
      "loss": 0.815,
      "step": 43540
    },
    {
      "epoch": 2.2728352055527288,
      "grad_norm": 4.848601818084717,
      "learning_rate": 1.2132282587895551e-05,
      "loss": 0.8461,
      "step": 43550
    },
    {
      "epoch": 2.2733570785549335,
      "grad_norm": 4.909501075744629,
      "learning_rate": 1.2123584364072857e-05,
      "loss": 0.8251,
      "step": 43560
    },
    {
      "epoch": 2.2738789515571387,
      "grad_norm": 4.867955207824707,
      "learning_rate": 1.2114886140250162e-05,
      "loss": 0.8316,
      "step": 43570
    },
    {
      "epoch": 2.2744008245593434,
      "grad_norm": 4.07324743270874,
      "learning_rate": 1.2106187916427467e-05,
      "loss": 0.8758,
      "step": 43580
    },
    {
      "epoch": 2.274922697561548,
      "grad_norm": 4.704155921936035,
      "learning_rate": 1.209748969260477e-05,
      "loss": 0.8685,
      "step": 43590
    },
    {
      "epoch": 2.2754445705637534,
      "grad_norm": 4.392178535461426,
      "learning_rate": 1.2088791468782074e-05,
      "loss": 0.8954,
      "step": 43600
    },
    {
      "epoch": 2.275966443565958,
      "grad_norm": 4.64475679397583,
      "learning_rate": 1.208009324495938e-05,
      "loss": 0.8668,
      "step": 43610
    },
    {
      "epoch": 2.2764883165681633,
      "grad_norm": 4.533618450164795,
      "learning_rate": 1.2071395021136685e-05,
      "loss": 0.8415,
      "step": 43620
    },
    {
      "epoch": 2.277010189570368,
      "grad_norm": 4.364458084106445,
      "learning_rate": 1.206269679731399e-05,
      "loss": 0.7998,
      "step": 43630
    },
    {
      "epoch": 2.277532062572573,
      "grad_norm": 5.602291107177734,
      "learning_rate": 1.2053998573491293e-05,
      "loss": 0.7863,
      "step": 43640
    },
    {
      "epoch": 2.278053935574778,
      "grad_norm": 5.490383148193359,
      "learning_rate": 1.2045300349668599e-05,
      "loss": 0.8803,
      "step": 43650
    },
    {
      "epoch": 2.2785758085769827,
      "grad_norm": 4.690001010894775,
      "learning_rate": 1.2036602125845902e-05,
      "loss": 1.0384,
      "step": 43660
    },
    {
      "epoch": 2.279097681579188,
      "grad_norm": 4.953339099884033,
      "learning_rate": 1.2027903902023207e-05,
      "loss": 0.7672,
      "step": 43670
    },
    {
      "epoch": 2.2796195545813926,
      "grad_norm": 4.522282123565674,
      "learning_rate": 1.2019205678200513e-05,
      "loss": 0.924,
      "step": 43680
    },
    {
      "epoch": 2.2801414275835974,
      "grad_norm": 4.677001953125,
      "learning_rate": 1.2010507454377816e-05,
      "loss": 0.8267,
      "step": 43690
    },
    {
      "epoch": 2.2806633005858026,
      "grad_norm": 4.197201728820801,
      "learning_rate": 1.2001809230555122e-05,
      "loss": 0.8697,
      "step": 43700
    },
    {
      "epoch": 2.2811851735880073,
      "grad_norm": 4.595710754394531,
      "learning_rate": 1.1993111006732425e-05,
      "loss": 0.7928,
      "step": 43710
    },
    {
      "epoch": 2.2817070465902125,
      "grad_norm": 4.865716457366943,
      "learning_rate": 1.198441278290973e-05,
      "loss": 0.8867,
      "step": 43720
    },
    {
      "epoch": 2.2822289195924172,
      "grad_norm": 3.642939567565918,
      "learning_rate": 1.1975714559087036e-05,
      "loss": 0.8753,
      "step": 43730
    },
    {
      "epoch": 2.282750792594622,
      "grad_norm": 4.555642127990723,
      "learning_rate": 1.196701633526434e-05,
      "loss": 0.8664,
      "step": 43740
    },
    {
      "epoch": 2.283272665596827,
      "grad_norm": 4.478678226470947,
      "learning_rate": 1.1958318111441644e-05,
      "loss": 0.908,
      "step": 43750
    },
    {
      "epoch": 2.283794538599032,
      "grad_norm": 4.26621675491333,
      "learning_rate": 1.1949619887618948e-05,
      "loss": 0.7743,
      "step": 43760
    },
    {
      "epoch": 2.2843164116012367,
      "grad_norm": 5.135541915893555,
      "learning_rate": 1.1940921663796253e-05,
      "loss": 0.9066,
      "step": 43770
    },
    {
      "epoch": 2.284838284603442,
      "grad_norm": 5.406551361083984,
      "learning_rate": 1.1932223439973558e-05,
      "loss": 0.8114,
      "step": 43780
    },
    {
      "epoch": 2.2853601576056466,
      "grad_norm": 4.616453647613525,
      "learning_rate": 1.1923525216150864e-05,
      "loss": 0.7832,
      "step": 43790
    },
    {
      "epoch": 2.2858820306078513,
      "grad_norm": 5.171697616577148,
      "learning_rate": 1.1914826992328167e-05,
      "loss": 0.7959,
      "step": 43800
    },
    {
      "epoch": 2.2864039036100565,
      "grad_norm": 3.8426389694213867,
      "learning_rate": 1.190612876850547e-05,
      "loss": 0.8637,
      "step": 43810
    },
    {
      "epoch": 2.2869257766122613,
      "grad_norm": 4.592321872711182,
      "learning_rate": 1.1897430544682776e-05,
      "loss": 0.9942,
      "step": 43820
    },
    {
      "epoch": 2.2874476496144664,
      "grad_norm": 5.802555084228516,
      "learning_rate": 1.1888732320860081e-05,
      "loss": 0.9132,
      "step": 43830
    },
    {
      "epoch": 2.287969522616671,
      "grad_norm": 5.306292533874512,
      "learning_rate": 1.1880034097037386e-05,
      "loss": 0.8406,
      "step": 43840
    },
    {
      "epoch": 2.288491395618876,
      "grad_norm": 4.61872673034668,
      "learning_rate": 1.187133587321469e-05,
      "loss": 0.8632,
      "step": 43850
    },
    {
      "epoch": 2.289013268621081,
      "grad_norm": 4.209206581115723,
      "learning_rate": 1.1862637649391995e-05,
      "loss": 0.783,
      "step": 43860
    },
    {
      "epoch": 2.289535141623286,
      "grad_norm": 4.745046138763428,
      "learning_rate": 1.1853939425569299e-05,
      "loss": 0.8726,
      "step": 43870
    },
    {
      "epoch": 2.290057014625491,
      "grad_norm": 4.269333362579346,
      "learning_rate": 1.1845241201746604e-05,
      "loss": 0.8281,
      "step": 43880
    },
    {
      "epoch": 2.290578887627696,
      "grad_norm": 5.051189422607422,
      "learning_rate": 1.183654297792391e-05,
      "loss": 0.8567,
      "step": 43890
    },
    {
      "epoch": 2.2911007606299005,
      "grad_norm": 4.398382663726807,
      "learning_rate": 1.1827844754101213e-05,
      "loss": 0.867,
      "step": 43900
    },
    {
      "epoch": 2.2916226336321057,
      "grad_norm": 3.2480080127716064,
      "learning_rate": 1.1819146530278518e-05,
      "loss": 0.8755,
      "step": 43910
    },
    {
      "epoch": 2.2921445066343105,
      "grad_norm": 5.089642524719238,
      "learning_rate": 1.1810448306455822e-05,
      "loss": 0.8268,
      "step": 43920
    },
    {
      "epoch": 2.2926663796365157,
      "grad_norm": 4.239922523498535,
      "learning_rate": 1.1801750082633127e-05,
      "loss": 0.89,
      "step": 43930
    },
    {
      "epoch": 2.2931882526387204,
      "grad_norm": 4.168720722198486,
      "learning_rate": 1.1793051858810432e-05,
      "loss": 0.8255,
      "step": 43940
    },
    {
      "epoch": 2.293710125640925,
      "grad_norm": 5.236614227294922,
      "learning_rate": 1.1784353634987737e-05,
      "loss": 0.925,
      "step": 43950
    },
    {
      "epoch": 2.2942319986431303,
      "grad_norm": 4.268024444580078,
      "learning_rate": 1.1775655411165041e-05,
      "loss": 0.8895,
      "step": 43960
    },
    {
      "epoch": 2.294753871645335,
      "grad_norm": 4.550628662109375,
      "learning_rate": 1.1766957187342344e-05,
      "loss": 0.9298,
      "step": 43970
    },
    {
      "epoch": 2.2952757446475403,
      "grad_norm": 5.596626281738281,
      "learning_rate": 1.175825896351965e-05,
      "loss": 0.9018,
      "step": 43980
    },
    {
      "epoch": 2.295797617649745,
      "grad_norm": 5.263992786407471,
      "learning_rate": 1.1749560739696955e-05,
      "loss": 0.8768,
      "step": 43990
    },
    {
      "epoch": 2.2963194906519497,
      "grad_norm": 4.777750015258789,
      "learning_rate": 1.174086251587426e-05,
      "loss": 0.9409,
      "step": 44000
    },
    {
      "epoch": 2.296841363654155,
      "grad_norm": 4.949932098388672,
      "learning_rate": 1.1732164292051564e-05,
      "loss": 0.9151,
      "step": 44010
    },
    {
      "epoch": 2.2973632366563597,
      "grad_norm": 4.23900842666626,
      "learning_rate": 1.1723466068228867e-05,
      "loss": 0.7473,
      "step": 44020
    },
    {
      "epoch": 2.2978851096585644,
      "grad_norm": 2.977057695388794,
      "learning_rate": 1.1714767844406173e-05,
      "loss": 0.7632,
      "step": 44030
    },
    {
      "epoch": 2.2984069826607696,
      "grad_norm": 4.3285231590271,
      "learning_rate": 1.1706069620583478e-05,
      "loss": 0.8898,
      "step": 44040
    },
    {
      "epoch": 2.2989288556629743,
      "grad_norm": 4.344976425170898,
      "learning_rate": 1.1697371396760783e-05,
      "loss": 0.7663,
      "step": 44050
    },
    {
      "epoch": 2.2994507286651795,
      "grad_norm": 4.137269973754883,
      "learning_rate": 1.1688673172938087e-05,
      "loss": 0.8637,
      "step": 44060
    },
    {
      "epoch": 2.2999726016673843,
      "grad_norm": 5.125828742980957,
      "learning_rate": 1.167997494911539e-05,
      "loss": 0.7851,
      "step": 44070
    },
    {
      "epoch": 2.300494474669589,
      "grad_norm": 5.181545257568359,
      "learning_rate": 1.1671276725292695e-05,
      "loss": 0.8444,
      "step": 44080
    },
    {
      "epoch": 2.301016347671794,
      "grad_norm": 5.30572509765625,
      "learning_rate": 1.166257850147e-05,
      "loss": 0.9744,
      "step": 44090
    },
    {
      "epoch": 2.301538220673999,
      "grad_norm": 5.457449436187744,
      "learning_rate": 1.1653880277647306e-05,
      "loss": 0.7865,
      "step": 44100
    },
    {
      "epoch": 2.3020600936762037,
      "grad_norm": 5.257630348205566,
      "learning_rate": 1.164518205382461e-05,
      "loss": 0.8344,
      "step": 44110
    },
    {
      "epoch": 2.302581966678409,
      "grad_norm": 4.281796932220459,
      "learning_rate": 1.1636483830001915e-05,
      "loss": 0.7498,
      "step": 44120
    },
    {
      "epoch": 2.3031038396806136,
      "grad_norm": 4.651681900024414,
      "learning_rate": 1.1627785606179218e-05,
      "loss": 0.8554,
      "step": 44130
    },
    {
      "epoch": 2.303625712682819,
      "grad_norm": 4.838383197784424,
      "learning_rate": 1.1619087382356523e-05,
      "loss": 0.8866,
      "step": 44140
    },
    {
      "epoch": 2.3041475856850235,
      "grad_norm": 4.999988555908203,
      "learning_rate": 1.1610389158533829e-05,
      "loss": 0.8921,
      "step": 44150
    },
    {
      "epoch": 2.3046694586872283,
      "grad_norm": 4.765493392944336,
      "learning_rate": 1.1601690934711132e-05,
      "loss": 0.8267,
      "step": 44160
    },
    {
      "epoch": 2.3051913316894335,
      "grad_norm": 4.6968865394592285,
      "learning_rate": 1.1592992710888437e-05,
      "loss": 0.9291,
      "step": 44170
    },
    {
      "epoch": 2.305713204691638,
      "grad_norm": 5.034526348114014,
      "learning_rate": 1.1584294487065741e-05,
      "loss": 0.9291,
      "step": 44180
    },
    {
      "epoch": 2.3062350776938434,
      "grad_norm": 5.0380964279174805,
      "learning_rate": 1.1575596263243046e-05,
      "loss": 0.7504,
      "step": 44190
    },
    {
      "epoch": 2.306756950696048,
      "grad_norm": 5.648591995239258,
      "learning_rate": 1.1566898039420351e-05,
      "loss": 0.8186,
      "step": 44200
    },
    {
      "epoch": 2.307278823698253,
      "grad_norm": 4.436723232269287,
      "learning_rate": 1.1558199815597657e-05,
      "loss": 0.8195,
      "step": 44210
    },
    {
      "epoch": 2.307800696700458,
      "grad_norm": 3.961237668991089,
      "learning_rate": 1.154950159177496e-05,
      "loss": 0.8742,
      "step": 44220
    },
    {
      "epoch": 2.308322569702663,
      "grad_norm": 4.348249912261963,
      "learning_rate": 1.1540803367952264e-05,
      "loss": 0.761,
      "step": 44230
    },
    {
      "epoch": 2.308844442704868,
      "grad_norm": 4.879753112792969,
      "learning_rate": 1.1532105144129569e-05,
      "loss": 0.8007,
      "step": 44240
    },
    {
      "epoch": 2.3093663157070727,
      "grad_norm": 4.223365306854248,
      "learning_rate": 1.1523406920306874e-05,
      "loss": 0.8447,
      "step": 44250
    },
    {
      "epoch": 2.3098881887092775,
      "grad_norm": 4.3289618492126465,
      "learning_rate": 1.151470869648418e-05,
      "loss": 0.8842,
      "step": 44260
    },
    {
      "epoch": 2.3104100617114827,
      "grad_norm": 4.482003211975098,
      "learning_rate": 1.1506010472661483e-05,
      "loss": 0.8193,
      "step": 44270
    },
    {
      "epoch": 2.3109319347136874,
      "grad_norm": 5.089869976043701,
      "learning_rate": 1.1497312248838787e-05,
      "loss": 0.8355,
      "step": 44280
    },
    {
      "epoch": 2.311453807715892,
      "grad_norm": 4.229124069213867,
      "learning_rate": 1.1488614025016092e-05,
      "loss": 0.8472,
      "step": 44290
    },
    {
      "epoch": 2.3119756807180973,
      "grad_norm": 5.115238666534424,
      "learning_rate": 1.1479915801193397e-05,
      "loss": 0.8419,
      "step": 44300
    },
    {
      "epoch": 2.312497553720302,
      "grad_norm": 3.4308066368103027,
      "learning_rate": 1.1471217577370702e-05,
      "loss": 0.8588,
      "step": 44310
    },
    {
      "epoch": 2.3130194267225073,
      "grad_norm": 4.23746395111084,
      "learning_rate": 1.1462519353548006e-05,
      "loss": 0.7484,
      "step": 44320
    },
    {
      "epoch": 2.313541299724712,
      "grad_norm": 4.276455402374268,
      "learning_rate": 1.1453821129725311e-05,
      "loss": 0.9488,
      "step": 44330
    },
    {
      "epoch": 2.3140631727269168,
      "grad_norm": 4.039317607879639,
      "learning_rate": 1.1445122905902615e-05,
      "loss": 0.8634,
      "step": 44340
    },
    {
      "epoch": 2.314585045729122,
      "grad_norm": 4.788503170013428,
      "learning_rate": 1.143642468207992e-05,
      "loss": 0.8537,
      "step": 44350
    },
    {
      "epoch": 2.3151069187313267,
      "grad_norm": 4.924009323120117,
      "learning_rate": 1.1427726458257225e-05,
      "loss": 0.848,
      "step": 44360
    },
    {
      "epoch": 2.3156287917335314,
      "grad_norm": 3.5934770107269287,
      "learning_rate": 1.1419028234434529e-05,
      "loss": 0.8552,
      "step": 44370
    },
    {
      "epoch": 2.3161506647357366,
      "grad_norm": 5.3742499351501465,
      "learning_rate": 1.1410330010611834e-05,
      "loss": 0.8877,
      "step": 44380
    },
    {
      "epoch": 2.3166725377379414,
      "grad_norm": 4.485698223114014,
      "learning_rate": 1.1401631786789138e-05,
      "loss": 0.8776,
      "step": 44390
    },
    {
      "epoch": 2.3171944107401465,
      "grad_norm": 3.8911383152008057,
      "learning_rate": 1.1392933562966443e-05,
      "loss": 0.8292,
      "step": 44400
    },
    {
      "epoch": 2.3177162837423513,
      "grad_norm": 4.717197418212891,
      "learning_rate": 1.1384235339143748e-05,
      "loss": 0.8752,
      "step": 44410
    },
    {
      "epoch": 2.318238156744556,
      "grad_norm": 4.678369998931885,
      "learning_rate": 1.1375537115321052e-05,
      "loss": 0.8196,
      "step": 44420
    },
    {
      "epoch": 2.318760029746761,
      "grad_norm": 4.399313449859619,
      "learning_rate": 1.1366838891498357e-05,
      "loss": 0.8586,
      "step": 44430
    },
    {
      "epoch": 2.319281902748966,
      "grad_norm": 4.295538902282715,
      "learning_rate": 1.135814066767566e-05,
      "loss": 0.8132,
      "step": 44440
    },
    {
      "epoch": 2.319803775751171,
      "grad_norm": 4.485230922698975,
      "learning_rate": 1.1349442443852966e-05,
      "loss": 0.7649,
      "step": 44450
    },
    {
      "epoch": 2.320325648753376,
      "grad_norm": 4.179284572601318,
      "learning_rate": 1.134074422003027e-05,
      "loss": 0.8773,
      "step": 44460
    },
    {
      "epoch": 2.3208475217555806,
      "grad_norm": 4.356411457061768,
      "learning_rate": 1.1332045996207576e-05,
      "loss": 0.8674,
      "step": 44470
    },
    {
      "epoch": 2.321369394757786,
      "grad_norm": 4.522469520568848,
      "learning_rate": 1.132334777238488e-05,
      "loss": 0.8915,
      "step": 44480
    },
    {
      "epoch": 2.3218912677599906,
      "grad_norm": 4.885197162628174,
      "learning_rate": 1.1314649548562183e-05,
      "loss": 0.8217,
      "step": 44490
    },
    {
      "epoch": 2.3224131407621957,
      "grad_norm": 3.9254684448242188,
      "learning_rate": 1.1305951324739488e-05,
      "loss": 0.8335,
      "step": 44500
    },
    {
      "epoch": 2.3229350137644005,
      "grad_norm": 5.143261432647705,
      "learning_rate": 1.1297253100916794e-05,
      "loss": 0.8289,
      "step": 44510
    },
    {
      "epoch": 2.3234568867666052,
      "grad_norm": 4.837714195251465,
      "learning_rate": 1.1288554877094099e-05,
      "loss": 0.8107,
      "step": 44520
    },
    {
      "epoch": 2.3239787597688104,
      "grad_norm": 3.9121904373168945,
      "learning_rate": 1.1279856653271402e-05,
      "loss": 0.8669,
      "step": 44530
    },
    {
      "epoch": 2.324500632771015,
      "grad_norm": 4.217336177825928,
      "learning_rate": 1.1271158429448708e-05,
      "loss": 0.8466,
      "step": 44540
    },
    {
      "epoch": 2.32502250577322,
      "grad_norm": 5.724931716918945,
      "learning_rate": 1.1262460205626011e-05,
      "loss": 0.8577,
      "step": 44550
    },
    {
      "epoch": 2.325544378775425,
      "grad_norm": 4.441133499145508,
      "learning_rate": 1.1253761981803317e-05,
      "loss": 0.886,
      "step": 44560
    },
    {
      "epoch": 2.32606625177763,
      "grad_norm": 4.498212814331055,
      "learning_rate": 1.1245063757980622e-05,
      "loss": 0.8627,
      "step": 44570
    },
    {
      "epoch": 2.326588124779835,
      "grad_norm": 4.8540730476379395,
      "learning_rate": 1.1236365534157925e-05,
      "loss": 0.8234,
      "step": 44580
    },
    {
      "epoch": 2.3271099977820398,
      "grad_norm": 5.3251142501831055,
      "learning_rate": 1.122766731033523e-05,
      "loss": 0.7724,
      "step": 44590
    },
    {
      "epoch": 2.3276318707842445,
      "grad_norm": 3.900095224380493,
      "learning_rate": 1.1218969086512534e-05,
      "loss": 0.8312,
      "step": 44600
    },
    {
      "epoch": 2.3281537437864497,
      "grad_norm": 5.04805326461792,
      "learning_rate": 1.121027086268984e-05,
      "loss": 0.8746,
      "step": 44610
    },
    {
      "epoch": 2.3286756167886544,
      "grad_norm": 4.272241115570068,
      "learning_rate": 1.1201572638867145e-05,
      "loss": 0.8145,
      "step": 44620
    },
    {
      "epoch": 2.329197489790859,
      "grad_norm": 4.254430770874023,
      "learning_rate": 1.1192874415044448e-05,
      "loss": 0.7839,
      "step": 44630
    },
    {
      "epoch": 2.3297193627930644,
      "grad_norm": 4.389260768890381,
      "learning_rate": 1.1184176191221753e-05,
      "loss": 0.8204,
      "step": 44640
    },
    {
      "epoch": 2.330241235795269,
      "grad_norm": 4.7202534675598145,
      "learning_rate": 1.1175477967399057e-05,
      "loss": 0.8266,
      "step": 44650
    },
    {
      "epoch": 2.3307631087974743,
      "grad_norm": 4.629241943359375,
      "learning_rate": 1.1166779743576362e-05,
      "loss": 0.9085,
      "step": 44660
    },
    {
      "epoch": 2.331284981799679,
      "grad_norm": 4.189459323883057,
      "learning_rate": 1.1158081519753667e-05,
      "loss": 0.8789,
      "step": 44670
    },
    {
      "epoch": 2.3318068548018838,
      "grad_norm": 4.387894630432129,
      "learning_rate": 1.1149383295930971e-05,
      "loss": 0.877,
      "step": 44680
    },
    {
      "epoch": 2.332328727804089,
      "grad_norm": 4.841796875,
      "learning_rate": 1.1140685072108276e-05,
      "loss": 0.8909,
      "step": 44690
    },
    {
      "epoch": 2.3328506008062937,
      "grad_norm": 5.081906318664551,
      "learning_rate": 1.113198684828558e-05,
      "loss": 0.8595,
      "step": 44700
    },
    {
      "epoch": 2.333372473808499,
      "grad_norm": 3.895993232727051,
      "learning_rate": 1.1123288624462885e-05,
      "loss": 0.7166,
      "step": 44710
    },
    {
      "epoch": 2.3338943468107036,
      "grad_norm": 4.368941783905029,
      "learning_rate": 1.111459040064019e-05,
      "loss": 0.9085,
      "step": 44720
    },
    {
      "epoch": 2.3344162198129084,
      "grad_norm": 5.4311842918396,
      "learning_rate": 1.1105892176817495e-05,
      "loss": 0.8508,
      "step": 44730
    },
    {
      "epoch": 2.3349380928151136,
      "grad_norm": 4.1455979347229,
      "learning_rate": 1.1097193952994799e-05,
      "loss": 0.7846,
      "step": 44740
    },
    {
      "epoch": 2.3354599658173183,
      "grad_norm": 5.130970001220703,
      "learning_rate": 1.1088495729172103e-05,
      "loss": 0.8423,
      "step": 44750
    },
    {
      "epoch": 2.3359818388195235,
      "grad_norm": 5.27645206451416,
      "learning_rate": 1.1079797505349408e-05,
      "loss": 0.8926,
      "step": 44760
    },
    {
      "epoch": 2.3365037118217282,
      "grad_norm": 4.747747421264648,
      "learning_rate": 1.1071099281526713e-05,
      "loss": 0.8387,
      "step": 44770
    },
    {
      "epoch": 2.337025584823933,
      "grad_norm": 5.380115509033203,
      "learning_rate": 1.1062401057704018e-05,
      "loss": 0.8026,
      "step": 44780
    },
    {
      "epoch": 2.337547457826138,
      "grad_norm": 3.9603826999664307,
      "learning_rate": 1.1053702833881322e-05,
      "loss": 0.8218,
      "step": 44790
    },
    {
      "epoch": 2.338069330828343,
      "grad_norm": 4.916608810424805,
      "learning_rate": 1.1045004610058627e-05,
      "loss": 0.9332,
      "step": 44800
    },
    {
      "epoch": 2.338591203830548,
      "grad_norm": 5.049692153930664,
      "learning_rate": 1.103630638623593e-05,
      "loss": 0.8425,
      "step": 44810
    },
    {
      "epoch": 2.339113076832753,
      "grad_norm": 4.012604236602783,
      "learning_rate": 1.1027608162413236e-05,
      "loss": 0.755,
      "step": 44820
    },
    {
      "epoch": 2.3396349498349576,
      "grad_norm": 4.252133846282959,
      "learning_rate": 1.1018909938590541e-05,
      "loss": 0.8806,
      "step": 44830
    },
    {
      "epoch": 2.3401568228371628,
      "grad_norm": 4.786374568939209,
      "learning_rate": 1.1010211714767845e-05,
      "loss": 0.8719,
      "step": 44840
    },
    {
      "epoch": 2.3406786958393675,
      "grad_norm": 4.78605318069458,
      "learning_rate": 1.100151349094515e-05,
      "loss": 0.8629,
      "step": 44850
    },
    {
      "epoch": 2.3412005688415722,
      "grad_norm": 4.132243633270264,
      "learning_rate": 1.0992815267122453e-05,
      "loss": 0.8997,
      "step": 44860
    },
    {
      "epoch": 2.3417224418437774,
      "grad_norm": 6.077686309814453,
      "learning_rate": 1.0984117043299759e-05,
      "loss": 0.934,
      "step": 44870
    },
    {
      "epoch": 2.342244314845982,
      "grad_norm": 5.472658157348633,
      "learning_rate": 1.0975418819477064e-05,
      "loss": 0.9136,
      "step": 44880
    },
    {
      "epoch": 2.342766187848187,
      "grad_norm": 4.281106948852539,
      "learning_rate": 1.0966720595654367e-05,
      "loss": 0.7761,
      "step": 44890
    },
    {
      "epoch": 2.343288060850392,
      "grad_norm": 4.711915016174316,
      "learning_rate": 1.0958022371831673e-05,
      "loss": 0.7929,
      "step": 44900
    },
    {
      "epoch": 2.343809933852597,
      "grad_norm": 5.134189128875732,
      "learning_rate": 1.0949324148008976e-05,
      "loss": 0.862,
      "step": 44910
    },
    {
      "epoch": 2.344331806854802,
      "grad_norm": 4.566989898681641,
      "learning_rate": 1.0940625924186282e-05,
      "loss": 0.9592,
      "step": 44920
    },
    {
      "epoch": 2.3448536798570068,
      "grad_norm": 3.954798698425293,
      "learning_rate": 1.0931927700363587e-05,
      "loss": 0.806,
      "step": 44930
    },
    {
      "epoch": 2.3453755528592115,
      "grad_norm": 4.791798114776611,
      "learning_rate": 1.092322947654089e-05,
      "loss": 0.9124,
      "step": 44940
    },
    {
      "epoch": 2.3458974258614167,
      "grad_norm": 4.480344772338867,
      "learning_rate": 1.0914531252718196e-05,
      "loss": 0.8415,
      "step": 44950
    },
    {
      "epoch": 2.3464192988636214,
      "grad_norm": 4.905373573303223,
      "learning_rate": 1.0905833028895499e-05,
      "loss": 0.853,
      "step": 44960
    },
    {
      "epoch": 2.3469411718658266,
      "grad_norm": 5.409114837646484,
      "learning_rate": 1.0898004627455074e-05,
      "loss": 0.7597,
      "step": 44970
    },
    {
      "epoch": 2.3474630448680314,
      "grad_norm": 3.8300106525421143,
      "learning_rate": 1.088930640363238e-05,
      "loss": 0.7853,
      "step": 44980
    },
    {
      "epoch": 2.347984917870236,
      "grad_norm": 4.6402716636657715,
      "learning_rate": 1.0880608179809683e-05,
      "loss": 0.7808,
      "step": 44990
    },
    {
      "epoch": 2.3485067908724413,
      "grad_norm": 4.599558353424072,
      "learning_rate": 1.0871909955986988e-05,
      "loss": 0.9285,
      "step": 45000
    },
    {
      "epoch": 2.349028663874646,
      "grad_norm": 4.814142227172852,
      "learning_rate": 1.0863211732164293e-05,
      "loss": 0.9362,
      "step": 45010
    },
    {
      "epoch": 2.3495505368768512,
      "grad_norm": 4.805395126342773,
      "learning_rate": 1.0854513508341597e-05,
      "loss": 0.8428,
      "step": 45020
    },
    {
      "epoch": 2.350072409879056,
      "grad_norm": 4.808755874633789,
      "learning_rate": 1.0845815284518902e-05,
      "loss": 0.8949,
      "step": 45030
    },
    {
      "epoch": 2.3505942828812607,
      "grad_norm": 3.960869550704956,
      "learning_rate": 1.0837117060696206e-05,
      "loss": 0.816,
      "step": 45040
    },
    {
      "epoch": 2.351116155883466,
      "grad_norm": 4.064445495605469,
      "learning_rate": 1.0828418836873511e-05,
      "loss": 0.8806,
      "step": 45050
    },
    {
      "epoch": 2.3516380288856706,
      "grad_norm": 5.317488670349121,
      "learning_rate": 1.0819720613050816e-05,
      "loss": 0.8128,
      "step": 45060
    },
    {
      "epoch": 2.352159901887876,
      "grad_norm": 4.440893650054932,
      "learning_rate": 1.081102238922812e-05,
      "loss": 0.8981,
      "step": 45070
    },
    {
      "epoch": 2.3526817748900806,
      "grad_norm": 4.187063217163086,
      "learning_rate": 1.0802324165405425e-05,
      "loss": 0.8277,
      "step": 45080
    },
    {
      "epoch": 2.3532036478922853,
      "grad_norm": 5.435405731201172,
      "learning_rate": 1.0793625941582728e-05,
      "loss": 0.7894,
      "step": 45090
    },
    {
      "epoch": 2.3537255208944905,
      "grad_norm": 4.515104293823242,
      "learning_rate": 1.0784927717760034e-05,
      "loss": 0.7734,
      "step": 45100
    },
    {
      "epoch": 2.3542473938966952,
      "grad_norm": 4.513147830963135,
      "learning_rate": 1.0776229493937339e-05,
      "loss": 0.876,
      "step": 45110
    },
    {
      "epoch": 2.3547692668989,
      "grad_norm": 4.582851409912109,
      "learning_rate": 1.0767531270114642e-05,
      "loss": 0.8793,
      "step": 45120
    },
    {
      "epoch": 2.355291139901105,
      "grad_norm": 5.38122034072876,
      "learning_rate": 1.0758833046291948e-05,
      "loss": 0.8593,
      "step": 45130
    },
    {
      "epoch": 2.35581301290331,
      "grad_norm": 4.4902520179748535,
      "learning_rate": 1.0750134822469251e-05,
      "loss": 0.8144,
      "step": 45140
    },
    {
      "epoch": 2.3563348859055147,
      "grad_norm": 4.579915523529053,
      "learning_rate": 1.0741436598646557e-05,
      "loss": 0.8032,
      "step": 45150
    },
    {
      "epoch": 2.35685675890772,
      "grad_norm": 4.93419885635376,
      "learning_rate": 1.0732738374823862e-05,
      "loss": 0.8606,
      "step": 45160
    },
    {
      "epoch": 2.3573786319099246,
      "grad_norm": 4.5612311363220215,
      "learning_rate": 1.0724040151001167e-05,
      "loss": 0.9475,
      "step": 45170
    },
    {
      "epoch": 2.3579005049121298,
      "grad_norm": 4.336228847503662,
      "learning_rate": 1.071534192717847e-05,
      "loss": 0.8797,
      "step": 45180
    },
    {
      "epoch": 2.3584223779143345,
      "grad_norm": 4.666836261749268,
      "learning_rate": 1.0706643703355776e-05,
      "loss": 0.8446,
      "step": 45190
    },
    {
      "epoch": 2.3589442509165393,
      "grad_norm": 4.981455326080322,
      "learning_rate": 1.069794547953308e-05,
      "loss": 0.8231,
      "step": 45200
    },
    {
      "epoch": 2.3594661239187444,
      "grad_norm": 3.909261465072632,
      "learning_rate": 1.0689247255710385e-05,
      "loss": 0.7903,
      "step": 45210
    },
    {
      "epoch": 2.359987996920949,
      "grad_norm": 4.176263332366943,
      "learning_rate": 1.068054903188769e-05,
      "loss": 0.8235,
      "step": 45220
    },
    {
      "epoch": 2.3605098699231544,
      "grad_norm": 4.593632221221924,
      "learning_rate": 1.0671850808064993e-05,
      "loss": 0.879,
      "step": 45230
    },
    {
      "epoch": 2.361031742925359,
      "grad_norm": 4.799843788146973,
      "learning_rate": 1.0663152584242299e-05,
      "loss": 0.9185,
      "step": 45240
    },
    {
      "epoch": 2.361553615927564,
      "grad_norm": 5.11152458190918,
      "learning_rate": 1.0654454360419602e-05,
      "loss": 0.8237,
      "step": 45250
    },
    {
      "epoch": 2.362075488929769,
      "grad_norm": 4.176076412200928,
      "learning_rate": 1.0645756136596907e-05,
      "loss": 0.8409,
      "step": 45260
    },
    {
      "epoch": 2.362597361931974,
      "grad_norm": 4.860118865966797,
      "learning_rate": 1.0637057912774213e-05,
      "loss": 0.9049,
      "step": 45270
    },
    {
      "epoch": 2.363119234934179,
      "grad_norm": 4.613216400146484,
      "learning_rate": 1.0628359688951516e-05,
      "loss": 0.8657,
      "step": 45280
    },
    {
      "epoch": 2.3636411079363837,
      "grad_norm": 4.387754917144775,
      "learning_rate": 1.0619661465128821e-05,
      "loss": 0.7962,
      "step": 45290
    },
    {
      "epoch": 2.3641629809385885,
      "grad_norm": 4.9106831550598145,
      "learning_rate": 1.0610963241306125e-05,
      "loss": 0.7734,
      "step": 45300
    },
    {
      "epoch": 2.3646848539407936,
      "grad_norm": 5.105258464813232,
      "learning_rate": 1.060226501748343e-05,
      "loss": 0.8738,
      "step": 45310
    },
    {
      "epoch": 2.3652067269429984,
      "grad_norm": 4.69437313079834,
      "learning_rate": 1.0593566793660735e-05,
      "loss": 0.8144,
      "step": 45320
    },
    {
      "epoch": 2.3657285999452036,
      "grad_norm": 4.174917221069336,
      "learning_rate": 1.0584868569838039e-05,
      "loss": 0.8674,
      "step": 45330
    },
    {
      "epoch": 2.3662504729474083,
      "grad_norm": 4.3571977615356445,
      "learning_rate": 1.0576170346015344e-05,
      "loss": 0.8871,
      "step": 45340
    },
    {
      "epoch": 2.366772345949613,
      "grad_norm": 4.025551795959473,
      "learning_rate": 1.0567472122192648e-05,
      "loss": 0.8165,
      "step": 45350
    },
    {
      "epoch": 2.3672942189518182,
      "grad_norm": 4.650361061096191,
      "learning_rate": 1.0558773898369953e-05,
      "loss": 0.9123,
      "step": 45360
    },
    {
      "epoch": 2.367816091954023,
      "grad_norm": 4.290279865264893,
      "learning_rate": 1.0550075674547258e-05,
      "loss": 0.807,
      "step": 45370
    },
    {
      "epoch": 2.3683379649562277,
      "grad_norm": 5.367039680480957,
      "learning_rate": 1.0541377450724564e-05,
      "loss": 0.8697,
      "step": 45380
    },
    {
      "epoch": 2.368859837958433,
      "grad_norm": 4.616590976715088,
      "learning_rate": 1.0532679226901867e-05,
      "loss": 0.8167,
      "step": 45390
    },
    {
      "epoch": 2.3693817109606377,
      "grad_norm": 4.785085678100586,
      "learning_rate": 1.052398100307917e-05,
      "loss": 0.8218,
      "step": 45400
    },
    {
      "epoch": 2.3699035839628424,
      "grad_norm": 4.6199798583984375,
      "learning_rate": 1.0515282779256476e-05,
      "loss": 0.9288,
      "step": 45410
    },
    {
      "epoch": 2.3704254569650476,
      "grad_norm": 4.8986334800720215,
      "learning_rate": 1.0506584555433781e-05,
      "loss": 0.9495,
      "step": 45420
    },
    {
      "epoch": 2.3709473299672523,
      "grad_norm": 4.509947776794434,
      "learning_rate": 1.0497886331611086e-05,
      "loss": 0.88,
      "step": 45430
    },
    {
      "epoch": 2.3714692029694575,
      "grad_norm": 4.053955554962158,
      "learning_rate": 1.048918810778839e-05,
      "loss": 0.8461,
      "step": 45440
    },
    {
      "epoch": 2.3719910759716623,
      "grad_norm": 4.34589147567749,
      "learning_rate": 1.0480489883965695e-05,
      "loss": 0.7552,
      "step": 45450
    },
    {
      "epoch": 2.372512948973867,
      "grad_norm": 4.915953159332275,
      "learning_rate": 1.0471791660142999e-05,
      "loss": 0.9139,
      "step": 45460
    },
    {
      "epoch": 2.373034821976072,
      "grad_norm": 4.297044277191162,
      "learning_rate": 1.0463093436320304e-05,
      "loss": 0.8838,
      "step": 45470
    },
    {
      "epoch": 2.373556694978277,
      "grad_norm": 4.072773456573486,
      "learning_rate": 1.045439521249761e-05,
      "loss": 0.8908,
      "step": 45480
    },
    {
      "epoch": 2.374078567980482,
      "grad_norm": 4.31909704208374,
      "learning_rate": 1.0445696988674913e-05,
      "loss": 0.8072,
      "step": 45490
    },
    {
      "epoch": 2.374600440982687,
      "grad_norm": 4.324347972869873,
      "learning_rate": 1.0436998764852218e-05,
      "loss": 0.8209,
      "step": 45500
    },
    {
      "epoch": 2.3751223139848916,
      "grad_norm": 3.8076257705688477,
      "learning_rate": 1.0428300541029522e-05,
      "loss": 0.7608,
      "step": 45510
    },
    {
      "epoch": 2.375644186987097,
      "grad_norm": 4.222960948944092,
      "learning_rate": 1.0419602317206827e-05,
      "loss": 0.8858,
      "step": 45520
    },
    {
      "epoch": 2.3761660599893015,
      "grad_norm": 4.717966079711914,
      "learning_rate": 1.0410904093384132e-05,
      "loss": 0.8178,
      "step": 45530
    },
    {
      "epoch": 2.3766879329915067,
      "grad_norm": 4.931864261627197,
      "learning_rate": 1.0402205869561436e-05,
      "loss": 0.8821,
      "step": 45540
    },
    {
      "epoch": 2.3772098059937115,
      "grad_norm": 5.341585159301758,
      "learning_rate": 1.039350764573874e-05,
      "loss": 0.9095,
      "step": 45550
    },
    {
      "epoch": 2.377731678995916,
      "grad_norm": 3.9104905128479004,
      "learning_rate": 1.0384809421916044e-05,
      "loss": 0.7785,
      "step": 45560
    },
    {
      "epoch": 2.3782535519981214,
      "grad_norm": 4.85235595703125,
      "learning_rate": 1.037611119809335e-05,
      "loss": 0.8757,
      "step": 45570
    },
    {
      "epoch": 2.378775425000326,
      "grad_norm": 4.086758613586426,
      "learning_rate": 1.0367412974270655e-05,
      "loss": 0.8346,
      "step": 45580
    },
    {
      "epoch": 2.3792972980025313,
      "grad_norm": 4.816257953643799,
      "learning_rate": 1.0358714750447958e-05,
      "loss": 0.924,
      "step": 45590
    },
    {
      "epoch": 2.379819171004736,
      "grad_norm": 4.930220603942871,
      "learning_rate": 1.0350016526625264e-05,
      "loss": 0.7581,
      "step": 45600
    },
    {
      "epoch": 2.380341044006941,
      "grad_norm": 4.2546539306640625,
      "learning_rate": 1.0341318302802567e-05,
      "loss": 0.8698,
      "step": 45610
    },
    {
      "epoch": 2.380862917009146,
      "grad_norm": 4.369176864624023,
      "learning_rate": 1.0332620078979872e-05,
      "loss": 0.8109,
      "step": 45620
    },
    {
      "epoch": 2.3813847900113507,
      "grad_norm": 5.434571266174316,
      "learning_rate": 1.0323921855157178e-05,
      "loss": 0.9016,
      "step": 45630
    },
    {
      "epoch": 2.3819066630135555,
      "grad_norm": 4.560544490814209,
      "learning_rate": 1.0315223631334483e-05,
      "loss": 0.8031,
      "step": 45640
    },
    {
      "epoch": 2.3824285360157607,
      "grad_norm": 5.269442081451416,
      "learning_rate": 1.0306525407511786e-05,
      "loss": 0.8221,
      "step": 45650
    },
    {
      "epoch": 2.3829504090179654,
      "grad_norm": 4.670684337615967,
      "learning_rate": 1.029782718368909e-05,
      "loss": 0.8524,
      "step": 45660
    },
    {
      "epoch": 2.3834722820201706,
      "grad_norm": 5.046187877655029,
      "learning_rate": 1.0289128959866395e-05,
      "loss": 0.8165,
      "step": 45670
    },
    {
      "epoch": 2.3839941550223753,
      "grad_norm": 4.663987636566162,
      "learning_rate": 1.02804307360437e-05,
      "loss": 0.8329,
      "step": 45680
    },
    {
      "epoch": 2.38451602802458,
      "grad_norm": 4.5871806144714355,
      "learning_rate": 1.0271732512221006e-05,
      "loss": 0.828,
      "step": 45690
    },
    {
      "epoch": 2.3850379010267853,
      "grad_norm": 3.8381271362304688,
      "learning_rate": 1.026303428839831e-05,
      "loss": 0.8634,
      "step": 45700
    },
    {
      "epoch": 2.38555977402899,
      "grad_norm": 3.5788967609405518,
      "learning_rate": 1.0254336064575615e-05,
      "loss": 0.8762,
      "step": 45710
    },
    {
      "epoch": 2.3860816470311947,
      "grad_norm": 4.0739665031433105,
      "learning_rate": 1.0245637840752918e-05,
      "loss": 0.7998,
      "step": 45720
    },
    {
      "epoch": 2.3866035200334,
      "grad_norm": 4.819591522216797,
      "learning_rate": 1.0236939616930223e-05,
      "loss": 0.8578,
      "step": 45730
    },
    {
      "epoch": 2.3871253930356047,
      "grad_norm": 5.257519245147705,
      "learning_rate": 1.0228241393107529e-05,
      "loss": 0.8236,
      "step": 45740
    },
    {
      "epoch": 2.38764726603781,
      "grad_norm": 5.012480735778809,
      "learning_rate": 1.0219543169284832e-05,
      "loss": 0.8913,
      "step": 45750
    },
    {
      "epoch": 2.3881691390400146,
      "grad_norm": 4.82662296295166,
      "learning_rate": 1.0210844945462137e-05,
      "loss": 0.9549,
      "step": 45760
    },
    {
      "epoch": 2.3886910120422193,
      "grad_norm": 5.140161037445068,
      "learning_rate": 1.0202146721639441e-05,
      "loss": 0.8992,
      "step": 45770
    },
    {
      "epoch": 2.3892128850444245,
      "grad_norm": 4.806689262390137,
      "learning_rate": 1.0193448497816746e-05,
      "loss": 0.8563,
      "step": 45780
    },
    {
      "epoch": 2.3897347580466293,
      "grad_norm": 4.5310139656066895,
      "learning_rate": 1.0184750273994051e-05,
      "loss": 0.8261,
      "step": 45790
    },
    {
      "epoch": 2.3902566310488345,
      "grad_norm": 5.493623733520508,
      "learning_rate": 1.0176052050171355e-05,
      "loss": 0.7924,
      "step": 45800
    },
    {
      "epoch": 2.390778504051039,
      "grad_norm": 4.690816402435303,
      "learning_rate": 1.016735382634866e-05,
      "loss": 0.8873,
      "step": 45810
    },
    {
      "epoch": 2.391300377053244,
      "grad_norm": 4.8229079246521,
      "learning_rate": 1.0158655602525964e-05,
      "loss": 0.8563,
      "step": 45820
    },
    {
      "epoch": 2.391822250055449,
      "grad_norm": 4.63078498840332,
      "learning_rate": 1.0149957378703269e-05,
      "loss": 0.8717,
      "step": 45830
    },
    {
      "epoch": 2.392344123057654,
      "grad_norm": 5.56025505065918,
      "learning_rate": 1.0141259154880574e-05,
      "loss": 0.8464,
      "step": 45840
    },
    {
      "epoch": 2.392865996059859,
      "grad_norm": 5.462845802307129,
      "learning_rate": 1.013256093105788e-05,
      "loss": 0.8426,
      "step": 45850
    },
    {
      "epoch": 2.393387869062064,
      "grad_norm": 4.80664587020874,
      "learning_rate": 1.0123862707235183e-05,
      "loss": 0.9061,
      "step": 45860
    },
    {
      "epoch": 2.3939097420642685,
      "grad_norm": 4.125568389892578,
      "learning_rate": 1.0115164483412487e-05,
      "loss": 0.8423,
      "step": 45870
    },
    {
      "epoch": 2.3944316150664737,
      "grad_norm": 4.083433151245117,
      "learning_rate": 1.0106466259589792e-05,
      "loss": 0.9358,
      "step": 45880
    },
    {
      "epoch": 2.3949534880686785,
      "grad_norm": 4.8126349449157715,
      "learning_rate": 1.0097768035767097e-05,
      "loss": 0.7861,
      "step": 45890
    },
    {
      "epoch": 2.395475361070883,
      "grad_norm": 5.565248489379883,
      "learning_rate": 1.0089069811944402e-05,
      "loss": 0.8276,
      "step": 45900
    },
    {
      "epoch": 2.3959972340730884,
      "grad_norm": 4.621598720550537,
      "learning_rate": 1.0080371588121706e-05,
      "loss": 0.9018,
      "step": 45910
    },
    {
      "epoch": 2.396519107075293,
      "grad_norm": 4.549753189086914,
      "learning_rate": 1.0071673364299011e-05,
      "loss": 0.8546,
      "step": 45920
    },
    {
      "epoch": 2.3970409800774983,
      "grad_norm": 4.507687091827393,
      "learning_rate": 1.0062975140476315e-05,
      "loss": 0.9512,
      "step": 45930
    },
    {
      "epoch": 2.397562853079703,
      "grad_norm": 4.021927356719971,
      "learning_rate": 1.005427691665362e-05,
      "loss": 0.8426,
      "step": 45940
    },
    {
      "epoch": 2.398084726081908,
      "grad_norm": 4.784265995025635,
      "learning_rate": 1.0045578692830925e-05,
      "loss": 0.7726,
      "step": 45950
    },
    {
      "epoch": 2.398606599084113,
      "grad_norm": 3.900653839111328,
      "learning_rate": 1.0036880469008229e-05,
      "loss": 0.8868,
      "step": 45960
    },
    {
      "epoch": 2.3991284720863177,
      "grad_norm": 4.936992168426514,
      "learning_rate": 1.0028182245185534e-05,
      "loss": 0.8372,
      "step": 45970
    },
    {
      "epoch": 2.3996503450885225,
      "grad_norm": 4.739911079406738,
      "learning_rate": 1.0019484021362837e-05,
      "loss": 0.8922,
      "step": 45980
    },
    {
      "epoch": 2.4001722180907277,
      "grad_norm": 5.305142879486084,
      "learning_rate": 1.0010785797540143e-05,
      "loss": 0.8967,
      "step": 45990
    },
    {
      "epoch": 2.4006940910929324,
      "grad_norm": 4.607773780822754,
      "learning_rate": 1.0002087573717448e-05,
      "loss": 0.8504,
      "step": 46000
    },
    {
      "epoch": 2.4012159640951376,
      "grad_norm": 4.629996299743652,
      "learning_rate": 9.993389349894752e-06,
      "loss": 0.7292,
      "step": 46010
    },
    {
      "epoch": 2.4017378370973423,
      "grad_norm": 4.8730244636535645,
      "learning_rate": 9.984691126072057e-06,
      "loss": 0.8572,
      "step": 46020
    },
    {
      "epoch": 2.402259710099547,
      "grad_norm": 5.101653575897217,
      "learning_rate": 9.97599290224936e-06,
      "loss": 0.8224,
      "step": 46030
    },
    {
      "epoch": 2.4027815831017523,
      "grad_norm": 4.749131202697754,
      "learning_rate": 9.967294678426666e-06,
      "loss": 0.8858,
      "step": 46040
    },
    {
      "epoch": 2.403303456103957,
      "grad_norm": 5.418212890625,
      "learning_rate": 9.95859645460397e-06,
      "loss": 0.8012,
      "step": 46050
    },
    {
      "epoch": 2.403825329106162,
      "grad_norm": 4.493061542510986,
      "learning_rate": 9.949898230781276e-06,
      "loss": 0.8481,
      "step": 46060
    },
    {
      "epoch": 2.404347202108367,
      "grad_norm": 4.645411968231201,
      "learning_rate": 9.94120000695858e-06,
      "loss": 0.8332,
      "step": 46070
    },
    {
      "epoch": 2.4048690751105717,
      "grad_norm": 3.9896957874298096,
      "learning_rate": 9.932501783135883e-06,
      "loss": 0.8599,
      "step": 46080
    },
    {
      "epoch": 2.405390948112777,
      "grad_norm": 3.93151593208313,
      "learning_rate": 9.923803559313188e-06,
      "loss": 0.7992,
      "step": 46090
    },
    {
      "epoch": 2.4059128211149816,
      "grad_norm": 4.131801605224609,
      "learning_rate": 9.915105335490494e-06,
      "loss": 0.9015,
      "step": 46100
    },
    {
      "epoch": 2.406434694117187,
      "grad_norm": 4.15811014175415,
      "learning_rate": 9.906407111667799e-06,
      "loss": 0.87,
      "step": 46110
    },
    {
      "epoch": 2.4069565671193915,
      "grad_norm": 4.955303192138672,
      "learning_rate": 9.897708887845102e-06,
      "loss": 0.817,
      "step": 46120
    },
    {
      "epoch": 2.4074784401215963,
      "grad_norm": 5.769273281097412,
      "learning_rate": 9.889010664022406e-06,
      "loss": 0.8571,
      "step": 46130
    },
    {
      "epoch": 2.4080003131238015,
      "grad_norm": 4.870203971862793,
      "learning_rate": 9.880312440199711e-06,
      "loss": 0.8553,
      "step": 46140
    },
    {
      "epoch": 2.408522186126006,
      "grad_norm": 4.153956413269043,
      "learning_rate": 9.871614216377016e-06,
      "loss": 0.8302,
      "step": 46150
    },
    {
      "epoch": 2.409044059128211,
      "grad_norm": 4.5803680419921875,
      "learning_rate": 9.862915992554322e-06,
      "loss": 0.7971,
      "step": 46160
    },
    {
      "epoch": 2.409565932130416,
      "grad_norm": 4.2795281410217285,
      "learning_rate": 9.854217768731625e-06,
      "loss": 0.7492,
      "step": 46170
    },
    {
      "epoch": 2.410087805132621,
      "grad_norm": 4.070359230041504,
      "learning_rate": 9.84551954490893e-06,
      "loss": 0.8491,
      "step": 46180
    },
    {
      "epoch": 2.410609678134826,
      "grad_norm": 4.58406400680542,
      "learning_rate": 9.836821321086234e-06,
      "loss": 0.8856,
      "step": 46190
    },
    {
      "epoch": 2.411131551137031,
      "grad_norm": 4.46503210067749,
      "learning_rate": 9.82812309726354e-06,
      "loss": 0.8386,
      "step": 46200
    },
    {
      "epoch": 2.4116534241392356,
      "grad_norm": 4.012300491333008,
      "learning_rate": 9.819424873440845e-06,
      "loss": 0.8709,
      "step": 46210
    },
    {
      "epoch": 2.4121752971414407,
      "grad_norm": 4.358165740966797,
      "learning_rate": 9.810726649618148e-06,
      "loss": 0.8404,
      "step": 46220
    },
    {
      "epoch": 2.4126971701436455,
      "grad_norm": 4.421132564544678,
      "learning_rate": 9.802028425795453e-06,
      "loss": 0.7921,
      "step": 46230
    },
    {
      "epoch": 2.4132190431458502,
      "grad_norm": 4.39117956161499,
      "learning_rate": 9.793330201972757e-06,
      "loss": 0.876,
      "step": 46240
    },
    {
      "epoch": 2.4137409161480554,
      "grad_norm": 5.003467082977295,
      "learning_rate": 9.784631978150062e-06,
      "loss": 0.8199,
      "step": 46250
    },
    {
      "epoch": 2.41426278915026,
      "grad_norm": 4.943498134613037,
      "learning_rate": 9.775933754327367e-06,
      "loss": 0.8545,
      "step": 46260
    },
    {
      "epoch": 2.4147846621524653,
      "grad_norm": 4.277771949768066,
      "learning_rate": 9.767235530504671e-06,
      "loss": 0.9072,
      "step": 46270
    },
    {
      "epoch": 2.41530653515467,
      "grad_norm": 4.296119213104248,
      "learning_rate": 9.758537306681976e-06,
      "loss": 0.8369,
      "step": 46280
    },
    {
      "epoch": 2.415828408156875,
      "grad_norm": 5.281744003295898,
      "learning_rate": 9.74983908285928e-06,
      "loss": 0.9034,
      "step": 46290
    },
    {
      "epoch": 2.41635028115908,
      "grad_norm": 4.729063510894775,
      "learning_rate": 9.741140859036585e-06,
      "loss": 0.897,
      "step": 46300
    },
    {
      "epoch": 2.4168721541612848,
      "grad_norm": 4.232228755950928,
      "learning_rate": 9.73244263521389e-06,
      "loss": 0.8814,
      "step": 46310
    },
    {
      "epoch": 2.41739402716349,
      "grad_norm": 4.154237747192383,
      "learning_rate": 9.723744411391195e-06,
      "loss": 0.8268,
      "step": 46320
    },
    {
      "epoch": 2.4179159001656947,
      "grad_norm": 3.9359097480773926,
      "learning_rate": 9.715046187568499e-06,
      "loss": 0.825,
      "step": 46330
    },
    {
      "epoch": 2.4184377731678994,
      "grad_norm": 5.155876159667969,
      "learning_rate": 9.706347963745803e-06,
      "loss": 0.7805,
      "step": 46340
    },
    {
      "epoch": 2.4189596461701046,
      "grad_norm": 5.921510219573975,
      "learning_rate": 9.697649739923108e-06,
      "loss": 0.9535,
      "step": 46350
    },
    {
      "epoch": 2.4194815191723094,
      "grad_norm": 4.092256546020508,
      "learning_rate": 9.688951516100413e-06,
      "loss": 0.8051,
      "step": 46360
    },
    {
      "epoch": 2.4200033921745145,
      "grad_norm": 4.575960159301758,
      "learning_rate": 9.680253292277718e-06,
      "loss": 0.7911,
      "step": 46370
    },
    {
      "epoch": 2.4205252651767193,
      "grad_norm": 4.7127861976623535,
      "learning_rate": 9.671555068455022e-06,
      "loss": 0.8489,
      "step": 46380
    },
    {
      "epoch": 2.421047138178924,
      "grad_norm": 4.559453010559082,
      "learning_rate": 9.662856844632325e-06,
      "loss": 0.8363,
      "step": 46390
    },
    {
      "epoch": 2.421569011181129,
      "grad_norm": 4.421194553375244,
      "learning_rate": 9.65415862080963e-06,
      "loss": 0.8416,
      "step": 46400
    },
    {
      "epoch": 2.422090884183334,
      "grad_norm": 4.5899858474731445,
      "learning_rate": 9.645460396986936e-06,
      "loss": 0.7519,
      "step": 46410
    },
    {
      "epoch": 2.422612757185539,
      "grad_norm": 4.997866630554199,
      "learning_rate": 9.636762173164241e-06,
      "loss": 0.8849,
      "step": 46420
    },
    {
      "epoch": 2.423134630187744,
      "grad_norm": 3.964270830154419,
      "learning_rate": 9.628063949341545e-06,
      "loss": 0.7937,
      "step": 46430
    },
    {
      "epoch": 2.4236565031899486,
      "grad_norm": 4.263418674468994,
      "learning_rate": 9.61936572551885e-06,
      "loss": 0.8777,
      "step": 46440
    },
    {
      "epoch": 2.424178376192154,
      "grad_norm": 4.356073379516602,
      "learning_rate": 9.610667501696153e-06,
      "loss": 0.8176,
      "step": 46450
    },
    {
      "epoch": 2.4247002491943586,
      "grad_norm": 5.5605363845825195,
      "learning_rate": 9.601969277873459e-06,
      "loss": 0.8874,
      "step": 46460
    },
    {
      "epoch": 2.4252221221965633,
      "grad_norm": 3.8034040927886963,
      "learning_rate": 9.593271054050764e-06,
      "loss": 0.8365,
      "step": 46470
    },
    {
      "epoch": 2.4257439951987685,
      "grad_norm": 4.793369770050049,
      "learning_rate": 9.584572830228067e-06,
      "loss": 0.798,
      "step": 46480
    },
    {
      "epoch": 2.4262658682009732,
      "grad_norm": 5.2095947265625,
      "learning_rate": 9.575874606405373e-06,
      "loss": 0.8554,
      "step": 46490
    },
    {
      "epoch": 2.426787741203178,
      "grad_norm": 4.628747940063477,
      "learning_rate": 9.567176382582676e-06,
      "loss": 0.8299,
      "step": 46500
    },
    {
      "epoch": 2.427309614205383,
      "grad_norm": 4.631915092468262,
      "learning_rate": 9.558478158759981e-06,
      "loss": 0.9559,
      "step": 46510
    },
    {
      "epoch": 2.427831487207588,
      "grad_norm": 4.387731075286865,
      "learning_rate": 9.549779934937287e-06,
      "loss": 0.8394,
      "step": 46520
    },
    {
      "epoch": 2.428353360209793,
      "grad_norm": 4.265388488769531,
      "learning_rate": 9.541081711114592e-06,
      "loss": 0.8842,
      "step": 46530
    },
    {
      "epoch": 2.428875233211998,
      "grad_norm": 4.271225929260254,
      "learning_rate": 9.532383487291896e-06,
      "loss": 0.8132,
      "step": 46540
    },
    {
      "epoch": 2.4293971062142026,
      "grad_norm": 4.528648376464844,
      "learning_rate": 9.523685263469199e-06,
      "loss": 0.967,
      "step": 46550
    },
    {
      "epoch": 2.4299189792164078,
      "grad_norm": 4.799872398376465,
      "learning_rate": 9.514987039646504e-06,
      "loss": 0.7933,
      "step": 46560
    },
    {
      "epoch": 2.4304408522186125,
      "grad_norm": 4.559734344482422,
      "learning_rate": 9.50628881582381e-06,
      "loss": 0.8128,
      "step": 46570
    },
    {
      "epoch": 2.4309627252208177,
      "grad_norm": 4.745882511138916,
      "learning_rate": 9.497590592001115e-06,
      "loss": 0.8281,
      "step": 46580
    },
    {
      "epoch": 2.4314845982230224,
      "grad_norm": 5.188348293304443,
      "learning_rate": 9.488892368178418e-06,
      "loss": 0.8842,
      "step": 46590
    },
    {
      "epoch": 2.432006471225227,
      "grad_norm": 5.347267150878906,
      "learning_rate": 9.480194144355722e-06,
      "loss": 0.8145,
      "step": 46600
    },
    {
      "epoch": 2.4325283442274324,
      "grad_norm": 4.497206687927246,
      "learning_rate": 9.471495920533027e-06,
      "loss": 0.8668,
      "step": 46610
    },
    {
      "epoch": 2.433050217229637,
      "grad_norm": 5.00794792175293,
      "learning_rate": 9.462797696710332e-06,
      "loss": 0.8941,
      "step": 46620
    },
    {
      "epoch": 2.4335720902318423,
      "grad_norm": 4.45596981048584,
      "learning_rate": 9.454099472887638e-06,
      "loss": 0.8836,
      "step": 46630
    },
    {
      "epoch": 2.434093963234047,
      "grad_norm": 5.342909812927246,
      "learning_rate": 9.445401249064941e-06,
      "loss": 0.8232,
      "step": 46640
    },
    {
      "epoch": 2.4346158362362518,
      "grad_norm": 4.740204811096191,
      "learning_rate": 9.436703025242245e-06,
      "loss": 0.7989,
      "step": 46650
    },
    {
      "epoch": 2.435137709238457,
      "grad_norm": 4.9261698722839355,
      "learning_rate": 9.42800480141955e-06,
      "loss": 0.9589,
      "step": 46660
    },
    {
      "epoch": 2.4356595822406617,
      "grad_norm": 5.0437822341918945,
      "learning_rate": 9.419306577596855e-06,
      "loss": 0.8357,
      "step": 46670
    },
    {
      "epoch": 2.436181455242867,
      "grad_norm": 5.825997352600098,
      "learning_rate": 9.41060835377416e-06,
      "loss": 0.8176,
      "step": 46680
    },
    {
      "epoch": 2.4367033282450716,
      "grad_norm": 4.537435054779053,
      "learning_rate": 9.401910129951464e-06,
      "loss": 0.8592,
      "step": 46690
    },
    {
      "epoch": 2.4372252012472764,
      "grad_norm": 4.381600856781006,
      "learning_rate": 9.39321190612877e-06,
      "loss": 0.8895,
      "step": 46700
    },
    {
      "epoch": 2.4377470742494816,
      "grad_norm": 5.162294387817383,
      "learning_rate": 9.384513682306073e-06,
      "loss": 0.8238,
      "step": 46710
    },
    {
      "epoch": 2.4382689472516863,
      "grad_norm": 3.989022731781006,
      "learning_rate": 9.375815458483378e-06,
      "loss": 0.7956,
      "step": 46720
    },
    {
      "epoch": 2.438790820253891,
      "grad_norm": 5.27136754989624,
      "learning_rate": 9.367117234660683e-06,
      "loss": 0.9031,
      "step": 46730
    },
    {
      "epoch": 2.4393126932560962,
      "grad_norm": 4.0960822105407715,
      "learning_rate": 9.358419010837987e-06,
      "loss": 0.8485,
      "step": 46740
    },
    {
      "epoch": 2.439834566258301,
      "grad_norm": 4.458786487579346,
      "learning_rate": 9.349720787015292e-06,
      "loss": 0.875,
      "step": 46750
    },
    {
      "epoch": 2.4403564392605057,
      "grad_norm": 4.593235492706299,
      "learning_rate": 9.341022563192596e-06,
      "loss": 0.8333,
      "step": 46760
    },
    {
      "epoch": 2.440878312262711,
      "grad_norm": 5.226797580718994,
      "learning_rate": 9.332324339369901e-06,
      "loss": 0.8285,
      "step": 46770
    },
    {
      "epoch": 2.4414001852649156,
      "grad_norm": 4.6180739402771,
      "learning_rate": 9.323626115547206e-06,
      "loss": 0.9592,
      "step": 46780
    },
    {
      "epoch": 2.441922058267121,
      "grad_norm": 4.608670234680176,
      "learning_rate": 9.314927891724511e-06,
      "loss": 0.7841,
      "step": 46790
    },
    {
      "epoch": 2.4424439312693256,
      "grad_norm": 5.255168914794922,
      "learning_rate": 9.306229667901815e-06,
      "loss": 0.9811,
      "step": 46800
    },
    {
      "epoch": 2.4429658042715303,
      "grad_norm": 5.18467378616333,
      "learning_rate": 9.297531444079118e-06,
      "loss": 0.9378,
      "step": 46810
    },
    {
      "epoch": 2.4434876772737355,
      "grad_norm": 4.8503098487854,
      "learning_rate": 9.288833220256424e-06,
      "loss": 0.8653,
      "step": 46820
    },
    {
      "epoch": 2.4440095502759402,
      "grad_norm": 4.567663669586182,
      "learning_rate": 9.280134996433729e-06,
      "loss": 0.8823,
      "step": 46830
    },
    {
      "epoch": 2.4445314232781454,
      "grad_norm": 5.035424709320068,
      "learning_rate": 9.271436772611034e-06,
      "loss": 0.7948,
      "step": 46840
    },
    {
      "epoch": 2.44505329628035,
      "grad_norm": 4.697155475616455,
      "learning_rate": 9.262738548788338e-06,
      "loss": 0.8539,
      "step": 46850
    },
    {
      "epoch": 2.445575169282555,
      "grad_norm": 4.566757678985596,
      "learning_rate": 9.254040324965641e-06,
      "loss": 0.8198,
      "step": 46860
    },
    {
      "epoch": 2.44609704228476,
      "grad_norm": 4.213109493255615,
      "learning_rate": 9.245342101142947e-06,
      "loss": 0.8599,
      "step": 46870
    },
    {
      "epoch": 2.446618915286965,
      "grad_norm": 4.274679183959961,
      "learning_rate": 9.236643877320252e-06,
      "loss": 0.7776,
      "step": 46880
    },
    {
      "epoch": 2.44714078828917,
      "grad_norm": 4.565354347229004,
      "learning_rate": 9.227945653497557e-06,
      "loss": 0.8415,
      "step": 46890
    },
    {
      "epoch": 2.4476626612913748,
      "grad_norm": 4.210756301879883,
      "learning_rate": 9.21924742967486e-06,
      "loss": 0.8628,
      "step": 46900
    },
    {
      "epoch": 2.4481845342935795,
      "grad_norm": 4.392947196960449,
      "learning_rate": 9.210549205852164e-06,
      "loss": 0.7615,
      "step": 46910
    },
    {
      "epoch": 2.4487064072957847,
      "grad_norm": 4.911371231079102,
      "learning_rate": 9.20185098202947e-06,
      "loss": 0.9212,
      "step": 46920
    },
    {
      "epoch": 2.4492282802979894,
      "grad_norm": 5.018588542938232,
      "learning_rate": 9.193152758206775e-06,
      "loss": 0.7677,
      "step": 46930
    },
    {
      "epoch": 2.4497501533001946,
      "grad_norm": 4.843943119049072,
      "learning_rate": 9.18445453438408e-06,
      "loss": 0.8318,
      "step": 46940
    },
    {
      "epoch": 2.4502720263023994,
      "grad_norm": 4.594460964202881,
      "learning_rate": 9.175756310561383e-06,
      "loss": 0.8886,
      "step": 46950
    },
    {
      "epoch": 2.450793899304604,
      "grad_norm": 5.193377494812012,
      "learning_rate": 9.167058086738689e-06,
      "loss": 0.8799,
      "step": 46960
    },
    {
      "epoch": 2.4513157723068093,
      "grad_norm": 4.550282001495361,
      "learning_rate": 9.158359862915992e-06,
      "loss": 0.7931,
      "step": 46970
    },
    {
      "epoch": 2.451837645309014,
      "grad_norm": 5.200767993927002,
      "learning_rate": 9.149661639093297e-06,
      "loss": 0.8837,
      "step": 46980
    },
    {
      "epoch": 2.452359518311219,
      "grad_norm": 4.574306488037109,
      "learning_rate": 9.140963415270603e-06,
      "loss": 0.7442,
      "step": 46990
    },
    {
      "epoch": 2.452881391313424,
      "grad_norm": 5.5286946296691895,
      "learning_rate": 9.132265191447908e-06,
      "loss": 0.8031,
      "step": 47000
    },
    {
      "epoch": 2.4534032643156287,
      "grad_norm": 3.9891297817230225,
      "learning_rate": 9.123566967625211e-06,
      "loss": 0.8999,
      "step": 47010
    },
    {
      "epoch": 2.4539251373178335,
      "grad_norm": 5.112333297729492,
      "learning_rate": 9.114868743802515e-06,
      "loss": 0.8898,
      "step": 47020
    },
    {
      "epoch": 2.4544470103200386,
      "grad_norm": 4.137390613555908,
      "learning_rate": 9.10617051997982e-06,
      "loss": 0.8225,
      "step": 47030
    },
    {
      "epoch": 2.4549688833222434,
      "grad_norm": 4.430299282073975,
      "learning_rate": 9.097472296157125e-06,
      "loss": 0.8959,
      "step": 47040
    },
    {
      "epoch": 2.4554907563244486,
      "grad_norm": 4.5107879638671875,
      "learning_rate": 9.08877407233443e-06,
      "loss": 0.7575,
      "step": 47050
    },
    {
      "epoch": 2.4560126293266533,
      "grad_norm": 3.966832160949707,
      "learning_rate": 9.080075848511734e-06,
      "loss": 0.7635,
      "step": 47060
    },
    {
      "epoch": 2.456534502328858,
      "grad_norm": 4.44509744644165,
      "learning_rate": 9.071377624689038e-06,
      "loss": 0.7658,
      "step": 47070
    },
    {
      "epoch": 2.4570563753310632,
      "grad_norm": 4.108308792114258,
      "learning_rate": 9.062679400866343e-06,
      "loss": 0.8047,
      "step": 47080
    },
    {
      "epoch": 2.457578248333268,
      "grad_norm": 5.088905334472656,
      "learning_rate": 9.053981177043648e-06,
      "loss": 0.9294,
      "step": 47090
    },
    {
      "epoch": 2.458100121335473,
      "grad_norm": 4.425708770751953,
      "learning_rate": 9.045282953220954e-06,
      "loss": 0.8228,
      "step": 47100
    },
    {
      "epoch": 2.458621994337678,
      "grad_norm": 5.177389621734619,
      "learning_rate": 9.036584729398257e-06,
      "loss": 0.859,
      "step": 47110
    },
    {
      "epoch": 2.4591438673398827,
      "grad_norm": 3.9503910541534424,
      "learning_rate": 9.02788650557556e-06,
      "loss": 0.84,
      "step": 47120
    },
    {
      "epoch": 2.459665740342088,
      "grad_norm": 5.076488018035889,
      "learning_rate": 9.019188281752866e-06,
      "loss": 0.9343,
      "step": 47130
    },
    {
      "epoch": 2.4601876133442926,
      "grad_norm": 5.556741714477539,
      "learning_rate": 9.010490057930171e-06,
      "loss": 0.9229,
      "step": 47140
    },
    {
      "epoch": 2.4607094863464978,
      "grad_norm": 5.181064605712891,
      "learning_rate": 9.001791834107476e-06,
      "loss": 0.8376,
      "step": 47150
    },
    {
      "epoch": 2.4612313593487025,
      "grad_norm": 4.49398136138916,
      "learning_rate": 8.99309361028478e-06,
      "loss": 0.9469,
      "step": 47160
    },
    {
      "epoch": 2.4617532323509073,
      "grad_norm": 3.6939165592193604,
      "learning_rate": 8.984395386462085e-06,
      "loss": 0.7655,
      "step": 47170
    },
    {
      "epoch": 2.4622751053531124,
      "grad_norm": 4.076066970825195,
      "learning_rate": 8.975697162639389e-06,
      "loss": 0.8619,
      "step": 47180
    },
    {
      "epoch": 2.462796978355317,
      "grad_norm": 5.0287909507751465,
      "learning_rate": 8.966998938816694e-06,
      "loss": 0.9177,
      "step": 47190
    },
    {
      "epoch": 2.4633188513575224,
      "grad_norm": 4.259435176849365,
      "learning_rate": 8.958300714994e-06,
      "loss": 0.7523,
      "step": 47200
    },
    {
      "epoch": 2.463840724359727,
      "grad_norm": 4.999093532562256,
      "learning_rate": 8.949602491171304e-06,
      "loss": 0.8289,
      "step": 47210
    },
    {
      "epoch": 2.464362597361932,
      "grad_norm": 4.541708946228027,
      "learning_rate": 8.940904267348608e-06,
      "loss": 0.815,
      "step": 47220
    },
    {
      "epoch": 2.464884470364137,
      "grad_norm": 4.626346588134766,
      "learning_rate": 8.932206043525912e-06,
      "loss": 0.8674,
      "step": 47230
    },
    {
      "epoch": 2.465406343366342,
      "grad_norm": 4.883955001831055,
      "learning_rate": 8.923507819703217e-06,
      "loss": 0.8901,
      "step": 47240
    },
    {
      "epoch": 2.4659282163685465,
      "grad_norm": 4.571277141571045,
      "learning_rate": 8.914809595880522e-06,
      "loss": 0.8648,
      "step": 47250
    },
    {
      "epoch": 2.4664500893707517,
      "grad_norm": 4.687793731689453,
      "learning_rate": 8.906111372057827e-06,
      "loss": 0.9116,
      "step": 47260
    },
    {
      "epoch": 2.4669719623729565,
      "grad_norm": 4.466038227081299,
      "learning_rate": 8.89741314823513e-06,
      "loss": 0.8941,
      "step": 47270
    },
    {
      "epoch": 2.4674938353751616,
      "grad_norm": 4.980353832244873,
      "learning_rate": 8.888714924412434e-06,
      "loss": 0.7744,
      "step": 47280
    },
    {
      "epoch": 2.4680157083773664,
      "grad_norm": 4.649091720581055,
      "learning_rate": 8.88001670058974e-06,
      "loss": 0.8996,
      "step": 47290
    },
    {
      "epoch": 2.468537581379571,
      "grad_norm": 4.051578521728516,
      "learning_rate": 8.871318476767045e-06,
      "loss": 0.8545,
      "step": 47300
    },
    {
      "epoch": 2.4690594543817763,
      "grad_norm": 4.694708824157715,
      "learning_rate": 8.86262025294435e-06,
      "loss": 0.8244,
      "step": 47310
    },
    {
      "epoch": 2.469581327383981,
      "grad_norm": 4.341275215148926,
      "learning_rate": 8.853922029121654e-06,
      "loss": 0.8344,
      "step": 47320
    },
    {
      "epoch": 2.470103200386186,
      "grad_norm": 5.220919609069824,
      "learning_rate": 8.845223805298957e-06,
      "loss": 0.8069,
      "step": 47330
    },
    {
      "epoch": 2.470625073388391,
      "grad_norm": 5.0540056228637695,
      "learning_rate": 8.836525581476262e-06,
      "loss": 0.7744,
      "step": 47340
    },
    {
      "epoch": 2.4711469463905957,
      "grad_norm": 3.730137586593628,
      "learning_rate": 8.827827357653568e-06,
      "loss": 0.773,
      "step": 47350
    },
    {
      "epoch": 2.471668819392801,
      "grad_norm": 5.246461391448975,
      "learning_rate": 8.819129133830873e-06,
      "loss": 0.8895,
      "step": 47360
    },
    {
      "epoch": 2.4721906923950057,
      "grad_norm": 5.050443172454834,
      "learning_rate": 8.810430910008176e-06,
      "loss": 0.8433,
      "step": 47370
    },
    {
      "epoch": 2.4727125653972104,
      "grad_norm": 4.2505717277526855,
      "learning_rate": 8.80173268618548e-06,
      "loss": 0.7995,
      "step": 47380
    },
    {
      "epoch": 2.4732344383994156,
      "grad_norm": 4.18707799911499,
      "learning_rate": 8.793034462362785e-06,
      "loss": 0.7925,
      "step": 47390
    },
    {
      "epoch": 2.4737563114016203,
      "grad_norm": 3.867565870285034,
      "learning_rate": 8.78433623854009e-06,
      "loss": 0.9187,
      "step": 47400
    },
    {
      "epoch": 2.4742781844038255,
      "grad_norm": 4.915382385253906,
      "learning_rate": 8.775638014717396e-06,
      "loss": 0.8464,
      "step": 47410
    },
    {
      "epoch": 2.4748000574060303,
      "grad_norm": 4.765434265136719,
      "learning_rate": 8.7669397908947e-06,
      "loss": 0.7962,
      "step": 47420
    },
    {
      "epoch": 2.475321930408235,
      "grad_norm": 4.386533737182617,
      "learning_rate": 8.758241567072005e-06,
      "loss": 0.8895,
      "step": 47430
    },
    {
      "epoch": 2.47584380341044,
      "grad_norm": 4.757742404937744,
      "learning_rate": 8.749543343249308e-06,
      "loss": 0.8502,
      "step": 47440
    },
    {
      "epoch": 2.476365676412645,
      "grad_norm": 5.16074800491333,
      "learning_rate": 8.741714941808883e-06,
      "loss": 0.8109,
      "step": 47450
    },
    {
      "epoch": 2.47688754941485,
      "grad_norm": 4.737411022186279,
      "learning_rate": 8.733016717986188e-06,
      "loss": 0.8433,
      "step": 47460
    },
    {
      "epoch": 2.477409422417055,
      "grad_norm": 4.231345176696777,
      "learning_rate": 8.724318494163493e-06,
      "loss": 0.8349,
      "step": 47470
    },
    {
      "epoch": 2.4779312954192596,
      "grad_norm": 4.588478088378906,
      "learning_rate": 8.715620270340797e-06,
      "loss": 0.8634,
      "step": 47480
    },
    {
      "epoch": 2.478453168421465,
      "grad_norm": 4.387319564819336,
      "learning_rate": 8.7069220465181e-06,
      "loss": 0.785,
      "step": 47490
    },
    {
      "epoch": 2.4789750414236695,
      "grad_norm": 4.637214660644531,
      "learning_rate": 8.698223822695406e-06,
      "loss": 0.759,
      "step": 47500
    },
    {
      "epoch": 2.4794969144258743,
      "grad_norm": 4.823127746582031,
      "learning_rate": 8.689525598872711e-06,
      "loss": 0.8577,
      "step": 47510
    },
    {
      "epoch": 2.4800187874280795,
      "grad_norm": 5.125249862670898,
      "learning_rate": 8.680827375050016e-06,
      "loss": 0.8517,
      "step": 47520
    },
    {
      "epoch": 2.480540660430284,
      "grad_norm": 4.932933330535889,
      "learning_rate": 8.67212915122732e-06,
      "loss": 0.8261,
      "step": 47530
    },
    {
      "epoch": 2.4810625334324894,
      "grad_norm": 4.471384525299072,
      "learning_rate": 8.663430927404623e-06,
      "loss": 0.798,
      "step": 47540
    },
    {
      "epoch": 2.481584406434694,
      "grad_norm": 5.225184917449951,
      "learning_rate": 8.654732703581929e-06,
      "loss": 0.8851,
      "step": 47550
    },
    {
      "epoch": 2.482106279436899,
      "grad_norm": 5.422306060791016,
      "learning_rate": 8.646034479759234e-06,
      "loss": 0.8955,
      "step": 47560
    },
    {
      "epoch": 2.482628152439104,
      "grad_norm": 5.0475897789001465,
      "learning_rate": 8.637336255936539e-06,
      "loss": 0.9274,
      "step": 47570
    },
    {
      "epoch": 2.483150025441309,
      "grad_norm": 4.606947898864746,
      "learning_rate": 8.628638032113843e-06,
      "loss": 0.8853,
      "step": 47580
    },
    {
      "epoch": 2.4836718984435135,
      "grad_norm": 4.84367561340332,
      "learning_rate": 8.619939808291146e-06,
      "loss": 0.8762,
      "step": 47590
    },
    {
      "epoch": 2.4841937714457187,
      "grad_norm": 4.663933277130127,
      "learning_rate": 8.611241584468451e-06,
      "loss": 0.824,
      "step": 47600
    },
    {
      "epoch": 2.4847156444479235,
      "grad_norm": 5.6439056396484375,
      "learning_rate": 8.602543360645757e-06,
      "loss": 0.8095,
      "step": 47610
    },
    {
      "epoch": 2.4852375174501287,
      "grad_norm": 4.989801406860352,
      "learning_rate": 8.593845136823062e-06,
      "loss": 0.8952,
      "step": 47620
    },
    {
      "epoch": 2.4857593904523334,
      "grad_norm": 4.8617262840271,
      "learning_rate": 8.585146913000365e-06,
      "loss": 0.9469,
      "step": 47630
    },
    {
      "epoch": 2.486281263454538,
      "grad_norm": 5.295025825500488,
      "learning_rate": 8.57644868917767e-06,
      "loss": 0.8471,
      "step": 47640
    },
    {
      "epoch": 2.4868031364567433,
      "grad_norm": 4.756930351257324,
      "learning_rate": 8.567750465354974e-06,
      "loss": 0.8191,
      "step": 47650
    },
    {
      "epoch": 2.487325009458948,
      "grad_norm": 4.632411479949951,
      "learning_rate": 8.55905224153228e-06,
      "loss": 0.8456,
      "step": 47660
    },
    {
      "epoch": 2.4878468824611533,
      "grad_norm": 3.8826026916503906,
      "learning_rate": 8.550354017709585e-06,
      "loss": 0.853,
      "step": 47670
    },
    {
      "epoch": 2.488368755463358,
      "grad_norm": 4.99505615234375,
      "learning_rate": 8.54165579388689e-06,
      "loss": 0.8921,
      "step": 47680
    },
    {
      "epoch": 2.4888906284655627,
      "grad_norm": 4.6782145500183105,
      "learning_rate": 8.532957570064194e-06,
      "loss": 0.8725,
      "step": 47690
    },
    {
      "epoch": 2.489412501467768,
      "grad_norm": 4.412452697753906,
      "learning_rate": 8.524259346241497e-06,
      "loss": 0.8419,
      "step": 47700
    },
    {
      "epoch": 2.4899343744699727,
      "grad_norm": 4.983505725860596,
      "learning_rate": 8.515561122418802e-06,
      "loss": 0.9443,
      "step": 47710
    },
    {
      "epoch": 2.490456247472178,
      "grad_norm": 4.027759075164795,
      "learning_rate": 8.506862898596108e-06,
      "loss": 0.8671,
      "step": 47720
    },
    {
      "epoch": 2.4909781204743826,
      "grad_norm": 3.8231658935546875,
      "learning_rate": 8.498164674773413e-06,
      "loss": 0.8214,
      "step": 47730
    },
    {
      "epoch": 2.4914999934765873,
      "grad_norm": 4.395551681518555,
      "learning_rate": 8.489466450950716e-06,
      "loss": 0.8223,
      "step": 47740
    },
    {
      "epoch": 2.4920218664787925,
      "grad_norm": 4.2386674880981445,
      "learning_rate": 8.48076822712802e-06,
      "loss": 0.8466,
      "step": 47750
    },
    {
      "epoch": 2.4925437394809973,
      "grad_norm": 5.036769390106201,
      "learning_rate": 8.472070003305325e-06,
      "loss": 0.9236,
      "step": 47760
    },
    {
      "epoch": 2.493065612483202,
      "grad_norm": 4.213712692260742,
      "learning_rate": 8.46337177948263e-06,
      "loss": 0.8487,
      "step": 47770
    },
    {
      "epoch": 2.493587485485407,
      "grad_norm": 4.9279303550720215,
      "learning_rate": 8.454673555659936e-06,
      "loss": 0.8559,
      "step": 47780
    },
    {
      "epoch": 2.494109358487612,
      "grad_norm": 4.007989883422852,
      "learning_rate": 8.44597533183724e-06,
      "loss": 0.7801,
      "step": 47790
    },
    {
      "epoch": 2.494631231489817,
      "grad_norm": 4.4249043464660645,
      "learning_rate": 8.437277108014543e-06,
      "loss": 0.8334,
      "step": 47800
    },
    {
      "epoch": 2.495153104492022,
      "grad_norm": 4.692965984344482,
      "learning_rate": 8.428578884191848e-06,
      "loss": 0.8261,
      "step": 47810
    },
    {
      "epoch": 2.4956749774942266,
      "grad_norm": 4.8328142166137695,
      "learning_rate": 8.419880660369153e-06,
      "loss": 0.8166,
      "step": 47820
    },
    {
      "epoch": 2.496196850496432,
      "grad_norm": 4.961799621582031,
      "learning_rate": 8.411182436546458e-06,
      "loss": 0.8099,
      "step": 47830
    },
    {
      "epoch": 2.4967187234986365,
      "grad_norm": 4.7272138595581055,
      "learning_rate": 8.402484212723762e-06,
      "loss": 0.8535,
      "step": 47840
    },
    {
      "epoch": 2.4972405965008413,
      "grad_norm": 4.596590042114258,
      "learning_rate": 8.393785988901066e-06,
      "loss": 0.9069,
      "step": 47850
    },
    {
      "epoch": 2.4977624695030465,
      "grad_norm": 4.334568500518799,
      "learning_rate": 8.38508776507837e-06,
      "loss": 0.8065,
      "step": 47860
    },
    {
      "epoch": 2.498284342505251,
      "grad_norm": 4.315751552581787,
      "learning_rate": 8.376389541255676e-06,
      "loss": 0.7849,
      "step": 47870
    },
    {
      "epoch": 2.4988062155074564,
      "grad_norm": 4.79513692855835,
      "learning_rate": 8.367691317432981e-06,
      "loss": 0.9099,
      "step": 47880
    },
    {
      "epoch": 2.499328088509661,
      "grad_norm": 4.417939186096191,
      "learning_rate": 8.358993093610285e-06,
      "loss": 0.834,
      "step": 47890
    },
    {
      "epoch": 2.499849961511866,
      "grad_norm": 3.7520864009857178,
      "learning_rate": 8.35029486978759e-06,
      "loss": 0.8178,
      "step": 47900
    },
    {
      "epoch": 2.500371834514071,
      "grad_norm": 4.334723949432373,
      "learning_rate": 8.341596645964894e-06,
      "loss": 0.7688,
      "step": 47910
    },
    {
      "epoch": 2.500893707516276,
      "grad_norm": 4.685100555419922,
      "learning_rate": 8.332898422142199e-06,
      "loss": 0.8925,
      "step": 47920
    },
    {
      "epoch": 2.501415580518481,
      "grad_norm": 6.025994300842285,
      "learning_rate": 8.324200198319504e-06,
      "loss": 0.84,
      "step": 47930
    },
    {
      "epoch": 2.5019374535206858,
      "grad_norm": 4.950231075286865,
      "learning_rate": 8.31550197449681e-06,
      "loss": 0.9374,
      "step": 47940
    },
    {
      "epoch": 2.5024593265228905,
      "grad_norm": 4.211717128753662,
      "learning_rate": 8.306803750674113e-06,
      "loss": 0.8013,
      "step": 47950
    },
    {
      "epoch": 2.5029811995250957,
      "grad_norm": 4.135283946990967,
      "learning_rate": 8.298105526851416e-06,
      "loss": 0.9395,
      "step": 47960
    },
    {
      "epoch": 2.5035030725273004,
      "grad_norm": 4.711575984954834,
      "learning_rate": 8.289407303028722e-06,
      "loss": 0.7932,
      "step": 47970
    },
    {
      "epoch": 2.5040249455295056,
      "grad_norm": 4.698774814605713,
      "learning_rate": 8.280709079206027e-06,
      "loss": 0.8894,
      "step": 47980
    },
    {
      "epoch": 2.5045468185317104,
      "grad_norm": 4.498941421508789,
      "learning_rate": 8.272010855383332e-06,
      "loss": 0.7406,
      "step": 47990
    },
    {
      "epoch": 2.505068691533915,
      "grad_norm": 4.3060526847839355,
      "learning_rate": 8.263312631560636e-06,
      "loss": 0.8454,
      "step": 48000
    },
    {
      "epoch": 2.5055905645361203,
      "grad_norm": 5.361184597015381,
      "learning_rate": 8.25461440773794e-06,
      "loss": 0.7854,
      "step": 48010
    },
    {
      "epoch": 2.506112437538325,
      "grad_norm": 4.638546943664551,
      "learning_rate": 8.245916183915245e-06,
      "loss": 0.7877,
      "step": 48020
    },
    {
      "epoch": 2.50663431054053,
      "grad_norm": 4.090638160705566,
      "learning_rate": 8.23721796009255e-06,
      "loss": 0.9312,
      "step": 48030
    },
    {
      "epoch": 2.507156183542735,
      "grad_norm": 4.501407623291016,
      "learning_rate": 8.228519736269855e-06,
      "loss": 0.9466,
      "step": 48040
    },
    {
      "epoch": 2.5076780565449397,
      "grad_norm": 4.337854385375977,
      "learning_rate": 8.219821512447159e-06,
      "loss": 0.8671,
      "step": 48050
    },
    {
      "epoch": 2.5081999295471444,
      "grad_norm": 4.5093536376953125,
      "learning_rate": 8.211123288624462e-06,
      "loss": 0.8613,
      "step": 48060
    },
    {
      "epoch": 2.5087218025493496,
      "grad_norm": 5.765995025634766,
      "learning_rate": 8.202425064801767e-06,
      "loss": 0.8615,
      "step": 48070
    },
    {
      "epoch": 2.5092436755515544,
      "grad_norm": 4.49812650680542,
      "learning_rate": 8.193726840979073e-06,
      "loss": 0.8303,
      "step": 48080
    },
    {
      "epoch": 2.5097655485537596,
      "grad_norm": 4.76604700088501,
      "learning_rate": 8.185028617156378e-06,
      "loss": 0.8562,
      "step": 48090
    },
    {
      "epoch": 2.5102874215559643,
      "grad_norm": 4.762475967407227,
      "learning_rate": 8.176330393333681e-06,
      "loss": 0.7899,
      "step": 48100
    },
    {
      "epoch": 2.510809294558169,
      "grad_norm": 5.108333587646484,
      "learning_rate": 8.167632169510987e-06,
      "loss": 0.9383,
      "step": 48110
    },
    {
      "epoch": 2.5113311675603742,
      "grad_norm": 4.398796558380127,
      "learning_rate": 8.15893394568829e-06,
      "loss": 0.8214,
      "step": 48120
    },
    {
      "epoch": 2.511853040562579,
      "grad_norm": 3.818185329437256,
      "learning_rate": 8.150235721865595e-06,
      "loss": 0.822,
      "step": 48130
    },
    {
      "epoch": 2.512374913564784,
      "grad_norm": 4.236447334289551,
      "learning_rate": 8.1415374980429e-06,
      "loss": 0.8789,
      "step": 48140
    },
    {
      "epoch": 2.512896786566989,
      "grad_norm": 4.870281219482422,
      "learning_rate": 8.132839274220206e-06,
      "loss": 0.8818,
      "step": 48150
    },
    {
      "epoch": 2.5134186595691936,
      "grad_norm": 4.353458881378174,
      "learning_rate": 8.12414105039751e-06,
      "loss": 0.8554,
      "step": 48160
    },
    {
      "epoch": 2.513940532571399,
      "grad_norm": 4.635406017303467,
      "learning_rate": 8.115442826574813e-06,
      "loss": 0.8767,
      "step": 48170
    },
    {
      "epoch": 2.5144624055736036,
      "grad_norm": 4.7480926513671875,
      "learning_rate": 8.106744602752118e-06,
      "loss": 0.8428,
      "step": 48180
    },
    {
      "epoch": 2.5149842785758088,
      "grad_norm": 4.766786575317383,
      "learning_rate": 8.098046378929424e-06,
      "loss": 0.7773,
      "step": 48190
    },
    {
      "epoch": 2.5155061515780135,
      "grad_norm": 4.779953956604004,
      "learning_rate": 8.089348155106729e-06,
      "loss": 0.752,
      "step": 48200
    },
    {
      "epoch": 2.5160280245802182,
      "grad_norm": 4.306058883666992,
      "learning_rate": 8.080649931284032e-06,
      "loss": 0.9537,
      "step": 48210
    },
    {
      "epoch": 2.5165498975824234,
      "grad_norm": 4.280622959136963,
      "learning_rate": 8.071951707461336e-06,
      "loss": 0.9041,
      "step": 48220
    },
    {
      "epoch": 2.517071770584628,
      "grad_norm": 4.016634464263916,
      "learning_rate": 8.063253483638641e-06,
      "loss": 0.7875,
      "step": 48230
    },
    {
      "epoch": 2.5175936435868334,
      "grad_norm": 4.228558540344238,
      "learning_rate": 8.054555259815946e-06,
      "loss": 0.771,
      "step": 48240
    },
    {
      "epoch": 2.518115516589038,
      "grad_norm": 5.424009323120117,
      "learning_rate": 8.045857035993252e-06,
      "loss": 0.835,
      "step": 48250
    },
    {
      "epoch": 2.518637389591243,
      "grad_norm": 4.335752487182617,
      "learning_rate": 8.037158812170555e-06,
      "loss": 0.8466,
      "step": 48260
    },
    {
      "epoch": 2.519159262593448,
      "grad_norm": 4.494389057159424,
      "learning_rate": 8.028460588347859e-06,
      "loss": 0.7874,
      "step": 48270
    },
    {
      "epoch": 2.5196811355956528,
      "grad_norm": 5.544040679931641,
      "learning_rate": 8.019762364525164e-06,
      "loss": 0.8473,
      "step": 48280
    },
    {
      "epoch": 2.520203008597858,
      "grad_norm": 4.86817741394043,
      "learning_rate": 8.01106414070247e-06,
      "loss": 0.8242,
      "step": 48290
    },
    {
      "epoch": 2.5207248816000627,
      "grad_norm": 5.530842304229736,
      "learning_rate": 8.002365916879774e-06,
      "loss": 0.8181,
      "step": 48300
    },
    {
      "epoch": 2.5212467546022674,
      "grad_norm": 5.008897304534912,
      "learning_rate": 7.993667693057078e-06,
      "loss": 0.7911,
      "step": 48310
    },
    {
      "epoch": 2.521768627604472,
      "grad_norm": 5.065958499908447,
      "learning_rate": 7.984969469234383e-06,
      "loss": 0.9669,
      "step": 48320
    },
    {
      "epoch": 2.5222905006066774,
      "grad_norm": 4.379714488983154,
      "learning_rate": 7.976271245411687e-06,
      "loss": 0.9774,
      "step": 48330
    },
    {
      "epoch": 2.5228123736088826,
      "grad_norm": 4.360376834869385,
      "learning_rate": 7.967573021588992e-06,
      "loss": 0.8276,
      "step": 48340
    },
    {
      "epoch": 2.5233342466110873,
      "grad_norm": 4.893835544586182,
      "learning_rate": 7.958874797766297e-06,
      "loss": 0.7815,
      "step": 48350
    },
    {
      "epoch": 2.523856119613292,
      "grad_norm": 4.315833568572998,
      "learning_rate": 7.9501765739436e-06,
      "loss": 0.8474,
      "step": 48360
    },
    {
      "epoch": 2.524377992615497,
      "grad_norm": 4.7539963722229,
      "learning_rate": 7.941478350120906e-06,
      "loss": 0.7628,
      "step": 48370
    },
    {
      "epoch": 2.524899865617702,
      "grad_norm": 3.8161025047302246,
      "learning_rate": 7.93278012629821e-06,
      "loss": 0.7879,
      "step": 48380
    },
    {
      "epoch": 2.5254217386199067,
      "grad_norm": 5.11142635345459,
      "learning_rate": 7.924081902475515e-06,
      "loss": 0.8984,
      "step": 48390
    },
    {
      "epoch": 2.525943611622112,
      "grad_norm": 4.752121925354004,
      "learning_rate": 7.91538367865282e-06,
      "loss": 0.9049,
      "step": 48400
    },
    {
      "epoch": 2.5264654846243166,
      "grad_norm": 4.610374450683594,
      "learning_rate": 7.906685454830125e-06,
      "loss": 0.8038,
      "step": 48410
    },
    {
      "epoch": 2.5269873576265214,
      "grad_norm": 4.650662422180176,
      "learning_rate": 7.897987231007429e-06,
      "loss": 0.9008,
      "step": 48420
    },
    {
      "epoch": 2.5275092306287266,
      "grad_norm": 4.715476989746094,
      "learning_rate": 7.889289007184732e-06,
      "loss": 0.8057,
      "step": 48430
    },
    {
      "epoch": 2.5280311036309313,
      "grad_norm": 5.704226016998291,
      "learning_rate": 7.880590783362038e-06,
      "loss": 0.8557,
      "step": 48440
    },
    {
      "epoch": 2.5285529766331365,
      "grad_norm": 4.490243911743164,
      "learning_rate": 7.871892559539343e-06,
      "loss": 0.8025,
      "step": 48450
    },
    {
      "epoch": 2.5290748496353412,
      "grad_norm": 4.596147060394287,
      "learning_rate": 7.863194335716648e-06,
      "loss": 0.7604,
      "step": 48460
    },
    {
      "epoch": 2.529596722637546,
      "grad_norm": 5.369367599487305,
      "learning_rate": 7.854496111893952e-06,
      "loss": 0.8027,
      "step": 48470
    },
    {
      "epoch": 2.530118595639751,
      "grad_norm": 4.265277862548828,
      "learning_rate": 7.845797888071255e-06,
      "loss": 0.7679,
      "step": 48480
    },
    {
      "epoch": 2.530640468641956,
      "grad_norm": 4.941822528839111,
      "learning_rate": 7.83709966424856e-06,
      "loss": 0.7728,
      "step": 48490
    },
    {
      "epoch": 2.531162341644161,
      "grad_norm": 4.969882011413574,
      "learning_rate": 7.828401440425866e-06,
      "loss": 0.8468,
      "step": 48500
    },
    {
      "epoch": 2.531684214646366,
      "grad_norm": 4.422494888305664,
      "learning_rate": 7.819703216603171e-06,
      "loss": 0.8649,
      "step": 48510
    },
    {
      "epoch": 2.5322060876485706,
      "grad_norm": 4.200178623199463,
      "learning_rate": 7.811004992780475e-06,
      "loss": 0.7125,
      "step": 48520
    },
    {
      "epoch": 2.5327279606507758,
      "grad_norm": 4.586757659912109,
      "learning_rate": 7.802306768957778e-06,
      "loss": 0.8746,
      "step": 48530
    },
    {
      "epoch": 2.5332498336529805,
      "grad_norm": 4.301628589630127,
      "learning_rate": 7.793608545135083e-06,
      "loss": 0.8242,
      "step": 48540
    },
    {
      "epoch": 2.5337717066551857,
      "grad_norm": 4.138314723968506,
      "learning_rate": 7.784910321312389e-06,
      "loss": 0.8773,
      "step": 48550
    },
    {
      "epoch": 2.5342935796573904,
      "grad_norm": 4.80852746963501,
      "learning_rate": 7.776212097489694e-06,
      "loss": 0.7957,
      "step": 48560
    },
    {
      "epoch": 2.534815452659595,
      "grad_norm": 4.438985824584961,
      "learning_rate": 7.767513873666997e-06,
      "loss": 0.8549,
      "step": 48570
    },
    {
      "epoch": 2.5353373256618004,
      "grad_norm": 3.306466817855835,
      "learning_rate": 7.758815649844303e-06,
      "loss": 0.8403,
      "step": 48580
    },
    {
      "epoch": 2.535859198664005,
      "grad_norm": 5.015185356140137,
      "learning_rate": 7.750117426021606e-06,
      "loss": 0.8614,
      "step": 48590
    },
    {
      "epoch": 2.5363810716662103,
      "grad_norm": 5.011010646820068,
      "learning_rate": 7.741419202198911e-06,
      "loss": 0.805,
      "step": 48600
    },
    {
      "epoch": 2.536902944668415,
      "grad_norm": 4.304152965545654,
      "learning_rate": 7.732720978376217e-06,
      "loss": 0.8597,
      "step": 48610
    },
    {
      "epoch": 2.53742481767062,
      "grad_norm": 4.866190433502197,
      "learning_rate": 7.72402275455352e-06,
      "loss": 0.835,
      "step": 48620
    },
    {
      "epoch": 2.5379466906728245,
      "grad_norm": 4.116319179534912,
      "learning_rate": 7.715324530730825e-06,
      "loss": 0.858,
      "step": 48630
    },
    {
      "epoch": 2.5384685636750297,
      "grad_norm": 3.9207687377929688,
      "learning_rate": 7.706626306908129e-06,
      "loss": 0.7931,
      "step": 48640
    },
    {
      "epoch": 2.5389904366772345,
      "grad_norm": 4.407230854034424,
      "learning_rate": 7.697928083085434e-06,
      "loss": 0.8011,
      "step": 48650
    },
    {
      "epoch": 2.5395123096794396,
      "grad_norm": 5.306517124176025,
      "learning_rate": 7.68922985926274e-06,
      "loss": 0.7533,
      "step": 48660
    },
    {
      "epoch": 2.5400341826816444,
      "grad_norm": 4.739259719848633,
      "learning_rate": 7.680531635440045e-06,
      "loss": 0.7712,
      "step": 48670
    },
    {
      "epoch": 2.540556055683849,
      "grad_norm": 3.9250359535217285,
      "learning_rate": 7.671833411617348e-06,
      "loss": 0.9105,
      "step": 48680
    },
    {
      "epoch": 2.5410779286860543,
      "grad_norm": 4.813879489898682,
      "learning_rate": 7.663135187794652e-06,
      "loss": 0.8195,
      "step": 48690
    },
    {
      "epoch": 2.541599801688259,
      "grad_norm": 4.904621124267578,
      "learning_rate": 7.654436963971957e-06,
      "loss": 0.8418,
      "step": 48700
    },
    {
      "epoch": 2.5421216746904642,
      "grad_norm": 4.764782428741455,
      "learning_rate": 7.645738740149262e-06,
      "loss": 0.8375,
      "step": 48710
    },
    {
      "epoch": 2.542643547692669,
      "grad_norm": 4.720938205718994,
      "learning_rate": 7.637040516326568e-06,
      "loss": 0.8087,
      "step": 48720
    },
    {
      "epoch": 2.5431654206948737,
      "grad_norm": 4.863508701324463,
      "learning_rate": 7.62834229250387e-06,
      "loss": 0.8987,
      "step": 48730
    },
    {
      "epoch": 2.543687293697079,
      "grad_norm": 5.225543975830078,
      "learning_rate": 7.6196440686811755e-06,
      "loss": 0.86,
      "step": 48740
    },
    {
      "epoch": 2.5442091666992837,
      "grad_norm": 5.176774978637695,
      "learning_rate": 7.61094584485848e-06,
      "loss": 0.8405,
      "step": 48750
    },
    {
      "epoch": 2.544731039701489,
      "grad_norm": 5.019708633422852,
      "learning_rate": 7.602247621035785e-06,
      "loss": 0.8137,
      "step": 48760
    },
    {
      "epoch": 2.5452529127036936,
      "grad_norm": 4.771297454833984,
      "learning_rate": 7.59354939721309e-06,
      "loss": 0.8735,
      "step": 48770
    },
    {
      "epoch": 2.5457747857058983,
      "grad_norm": 4.9417405128479,
      "learning_rate": 7.584851173390393e-06,
      "loss": 0.8001,
      "step": 48780
    },
    {
      "epoch": 2.5462966587081035,
      "grad_norm": 4.449951171875,
      "learning_rate": 7.576152949567698e-06,
      "loss": 0.8048,
      "step": 48790
    },
    {
      "epoch": 2.5468185317103083,
      "grad_norm": 3.9681899547576904,
      "learning_rate": 7.5674547257450035e-06,
      "loss": 0.8247,
      "step": 48800
    },
    {
      "epoch": 2.5473404047125134,
      "grad_norm": 4.538145542144775,
      "learning_rate": 7.558756501922308e-06,
      "loss": 0.8881,
      "step": 48810
    },
    {
      "epoch": 2.547862277714718,
      "grad_norm": 4.231890678405762,
      "learning_rate": 7.550058278099613e-06,
      "loss": 0.7656,
      "step": 48820
    },
    {
      "epoch": 2.548384150716923,
      "grad_norm": 4.806024551391602,
      "learning_rate": 7.541360054276917e-06,
      "loss": 0.8528,
      "step": 48830
    },
    {
      "epoch": 2.548906023719128,
      "grad_norm": 5.187505722045898,
      "learning_rate": 7.532661830454221e-06,
      "loss": 0.9006,
      "step": 48840
    },
    {
      "epoch": 2.549427896721333,
      "grad_norm": 4.129591941833496,
      "learning_rate": 7.523963606631526e-06,
      "loss": 0.814,
      "step": 48850
    },
    {
      "epoch": 2.549949769723538,
      "grad_norm": 4.643509864807129,
      "learning_rate": 7.515265382808831e-06,
      "loss": 0.8475,
      "step": 48860
    },
    {
      "epoch": 2.550471642725743,
      "grad_norm": 5.388970851898193,
      "learning_rate": 7.506567158986136e-06,
      "loss": 0.8489,
      "step": 48870
    },
    {
      "epoch": 2.5509935157279475,
      "grad_norm": 4.48061990737915,
      "learning_rate": 7.4978689351634396e-06,
      "loss": 0.8841,
      "step": 48880
    },
    {
      "epoch": 2.5515153887301523,
      "grad_norm": 3.8696253299713135,
      "learning_rate": 7.489170711340744e-06,
      "loss": 0.8611,
      "step": 48890
    },
    {
      "epoch": 2.5520372617323575,
      "grad_norm": 4.658830642700195,
      "learning_rate": 7.480472487518049e-06,
      "loss": 0.7966,
      "step": 48900
    },
    {
      "epoch": 2.552559134734562,
      "grad_norm": 4.6994309425354,
      "learning_rate": 7.471774263695354e-06,
      "loss": 0.7747,
      "step": 48910
    },
    {
      "epoch": 2.5530810077367674,
      "grad_norm": 4.544269561767578,
      "learning_rate": 7.463076039872659e-06,
      "loss": 0.8756,
      "step": 48920
    },
    {
      "epoch": 2.553602880738972,
      "grad_norm": 4.718143463134766,
      "learning_rate": 7.454377816049963e-06,
      "loss": 0.9479,
      "step": 48930
    },
    {
      "epoch": 2.554124753741177,
      "grad_norm": 4.772875785827637,
      "learning_rate": 7.445679592227267e-06,
      "loss": 0.8768,
      "step": 48940
    },
    {
      "epoch": 2.554646626743382,
      "grad_norm": 4.671030521392822,
      "learning_rate": 7.436981368404572e-06,
      "loss": 0.7878,
      "step": 48950
    },
    {
      "epoch": 2.555168499745587,
      "grad_norm": 4.604133605957031,
      "learning_rate": 7.428283144581876e-06,
      "loss": 0.828,
      "step": 48960
    },
    {
      "epoch": 2.555690372747792,
      "grad_norm": 4.754587650299072,
      "learning_rate": 7.419584920759182e-06,
      "loss": 0.8747,
      "step": 48970
    },
    {
      "epoch": 2.5562122457499967,
      "grad_norm": 4.7548699378967285,
      "learning_rate": 7.410886696936487e-06,
      "loss": 0.8855,
      "step": 48980
    },
    {
      "epoch": 2.5567341187522015,
      "grad_norm": 5.739288806915283,
      "learning_rate": 7.40218847311379e-06,
      "loss": 0.8866,
      "step": 48990
    },
    {
      "epoch": 2.5572559917544067,
      "grad_norm": 5.754663944244385,
      "learning_rate": 7.393490249291095e-06,
      "loss": 0.8264,
      "step": 49000
    },
    {
      "epoch": 2.5577778647566114,
      "grad_norm": 4.457947254180908,
      "learning_rate": 7.3847920254684e-06,
      "loss": 0.8585,
      "step": 49010
    },
    {
      "epoch": 2.5582997377588166,
      "grad_norm": 5.6070170402526855,
      "learning_rate": 7.3760938016457045e-06,
      "loss": 0.9557,
      "step": 49020
    },
    {
      "epoch": 2.5588216107610213,
      "grad_norm": 5.413885593414307,
      "learning_rate": 7.36739557782301e-06,
      "loss": 0.9294,
      "step": 49030
    },
    {
      "epoch": 2.559343483763226,
      "grad_norm": 5.333846569061279,
      "learning_rate": 7.358697354000313e-06,
      "loss": 0.9332,
      "step": 49040
    },
    {
      "epoch": 2.5598653567654313,
      "grad_norm": 4.074025630950928,
      "learning_rate": 7.349999130177618e-06,
      "loss": 0.8341,
      "step": 49050
    },
    {
      "epoch": 2.560387229767636,
      "grad_norm": 5.27431583404541,
      "learning_rate": 7.341300906354923e-06,
      "loss": 0.854,
      "step": 49060
    },
    {
      "epoch": 2.560909102769841,
      "grad_norm": 4.583752632141113,
      "learning_rate": 7.332602682532227e-06,
      "loss": 0.8803,
      "step": 49070
    },
    {
      "epoch": 2.561430975772046,
      "grad_norm": 3.8466084003448486,
      "learning_rate": 7.3239044587095326e-06,
      "loss": 0.7427,
      "step": 49080
    },
    {
      "epoch": 2.5619528487742507,
      "grad_norm": 4.781815528869629,
      "learning_rate": 7.315206234886836e-06,
      "loss": 0.922,
      "step": 49090
    },
    {
      "epoch": 2.562474721776456,
      "grad_norm": 4.002849578857422,
      "learning_rate": 7.3065080110641405e-06,
      "loss": 0.7992,
      "step": 49100
    },
    {
      "epoch": 2.5629965947786606,
      "grad_norm": 5.5148606300354,
      "learning_rate": 7.297809787241446e-06,
      "loss": 0.8755,
      "step": 49110
    },
    {
      "epoch": 2.563518467780866,
      "grad_norm": 5.107730865478516,
      "learning_rate": 7.28911156341875e-06,
      "loss": 0.7828,
      "step": 49120
    },
    {
      "epoch": 2.5640403407830705,
      "grad_norm": 4.475951194763184,
      "learning_rate": 7.280413339596055e-06,
      "loss": 0.8527,
      "step": 49130
    },
    {
      "epoch": 2.5645622137852753,
      "grad_norm": 3.796945571899414,
      "learning_rate": 7.27171511577336e-06,
      "loss": 0.7885,
      "step": 49140
    },
    {
      "epoch": 2.56508408678748,
      "grad_norm": 4.998510360717773,
      "learning_rate": 7.263016891950663e-06,
      "loss": 0.8282,
      "step": 49150
    },
    {
      "epoch": 2.565605959789685,
      "grad_norm": 3.909180164337158,
      "learning_rate": 7.2543186681279686e-06,
      "loss": 0.7645,
      "step": 49160
    },
    {
      "epoch": 2.56612783279189,
      "grad_norm": 5.357186794281006,
      "learning_rate": 7.245620444305273e-06,
      "loss": 0.8214,
      "step": 49170
    },
    {
      "epoch": 2.566649705794095,
      "grad_norm": 3.9382591247558594,
      "learning_rate": 7.236922220482578e-06,
      "loss": 0.8267,
      "step": 49180
    },
    {
      "epoch": 2.5671715787963,
      "grad_norm": 4.388456344604492,
      "learning_rate": 7.2282239966598835e-06,
      "loss": 0.854,
      "step": 49190
    },
    {
      "epoch": 2.5676934517985046,
      "grad_norm": 4.407467842102051,
      "learning_rate": 7.219525772837186e-06,
      "loss": 0.8668,
      "step": 49200
    },
    {
      "epoch": 2.56821532480071,
      "grad_norm": 4.623834133148193,
      "learning_rate": 7.210827549014491e-06,
      "loss": 0.8267,
      "step": 49210
    },
    {
      "epoch": 2.5687371978029145,
      "grad_norm": 4.864684104919434,
      "learning_rate": 7.202129325191797e-06,
      "loss": 0.8337,
      "step": 49220
    },
    {
      "epoch": 2.5692590708051197,
      "grad_norm": 4.745279312133789,
      "learning_rate": 7.193431101369101e-06,
      "loss": 0.8301,
      "step": 49230
    },
    {
      "epoch": 2.5697809438073245,
      "grad_norm": 5.25486946105957,
      "learning_rate": 7.184732877546406e-06,
      "loss": 0.956,
      "step": 49240
    },
    {
      "epoch": 2.570302816809529,
      "grad_norm": 5.3012003898620605,
      "learning_rate": 7.17603465372371e-06,
      "loss": 0.9003,
      "step": 49250
    },
    {
      "epoch": 2.5708246898117344,
      "grad_norm": 4.495467662811279,
      "learning_rate": 7.167336429901014e-06,
      "loss": 0.8898,
      "step": 49260
    },
    {
      "epoch": 2.571346562813939,
      "grad_norm": 3.7472896575927734,
      "learning_rate": 7.1586382060783195e-06,
      "loss": 0.8514,
      "step": 49270
    },
    {
      "epoch": 2.5718684358161443,
      "grad_norm": 4.365054130554199,
      "learning_rate": 7.149939982255624e-06,
      "loss": 0.8587,
      "step": 49280
    },
    {
      "epoch": 2.572390308818349,
      "grad_norm": 3.9826202392578125,
      "learning_rate": 7.141241758432929e-06,
      "loss": 0.8532,
      "step": 49290
    },
    {
      "epoch": 2.572912181820554,
      "grad_norm": 4.9755353927612305,
      "learning_rate": 7.132543534610233e-06,
      "loss": 0.8352,
      "step": 49300
    },
    {
      "epoch": 2.573434054822759,
      "grad_norm": 4.435184478759766,
      "learning_rate": 7.123845310787537e-06,
      "loss": 0.7823,
      "step": 49310
    },
    {
      "epoch": 2.5739559278249637,
      "grad_norm": 4.3132643699646,
      "learning_rate": 7.115147086964842e-06,
      "loss": 0.8107,
      "step": 49320
    },
    {
      "epoch": 2.574477800827169,
      "grad_norm": 4.222623825073242,
      "learning_rate": 7.106448863142147e-06,
      "loss": 0.7314,
      "step": 49330
    },
    {
      "epoch": 2.5749996738293737,
      "grad_norm": 4.379654884338379,
      "learning_rate": 7.097750639319452e-06,
      "loss": 0.7757,
      "step": 49340
    },
    {
      "epoch": 2.5755215468315784,
      "grad_norm": 4.409379005432129,
      "learning_rate": 7.0890524154967555e-06,
      "loss": 0.8356,
      "step": 49350
    },
    {
      "epoch": 2.5760434198337836,
      "grad_norm": 4.337382793426514,
      "learning_rate": 7.08035419167406e-06,
      "loss": 0.8445,
      "step": 49360
    },
    {
      "epoch": 2.5765652928359883,
      "grad_norm": 4.049713134765625,
      "learning_rate": 7.071655967851365e-06,
      "loss": 0.785,
      "step": 49370
    },
    {
      "epoch": 2.5770871658381935,
      "grad_norm": 4.041743755340576,
      "learning_rate": 7.0629577440286695e-06,
      "loss": 0.7156,
      "step": 49380
    },
    {
      "epoch": 2.5776090388403983,
      "grad_norm": 4.740169048309326,
      "learning_rate": 7.054259520205975e-06,
      "loss": 0.8512,
      "step": 49390
    },
    {
      "epoch": 2.578130911842603,
      "grad_norm": 4.595633029937744,
      "learning_rate": 7.045561296383279e-06,
      "loss": 0.8156,
      "step": 49400
    },
    {
      "epoch": 2.5786527848448078,
      "grad_norm": 4.585489749908447,
      "learning_rate": 7.036863072560583e-06,
      "loss": 0.7029,
      "step": 49410
    },
    {
      "epoch": 2.579174657847013,
      "grad_norm": 4.339959621429443,
      "learning_rate": 7.028164848737888e-06,
      "loss": 0.8796,
      "step": 49420
    },
    {
      "epoch": 2.5796965308492177,
      "grad_norm": 4.102380275726318,
      "learning_rate": 7.019466624915192e-06,
      "loss": 0.9215,
      "step": 49430
    },
    {
      "epoch": 2.580218403851423,
      "grad_norm": 4.488801002502441,
      "learning_rate": 7.010768401092498e-06,
      "loss": 0.7868,
      "step": 49440
    },
    {
      "epoch": 2.5807402768536276,
      "grad_norm": 4.653652191162109,
      "learning_rate": 7.002070177269803e-06,
      "loss": 0.878,
      "step": 49450
    },
    {
      "epoch": 2.5812621498558324,
      "grad_norm": 3.662114381790161,
      "learning_rate": 6.9933719534471055e-06,
      "loss": 0.8331,
      "step": 49460
    },
    {
      "epoch": 2.5817840228580375,
      "grad_norm": 4.639401435852051,
      "learning_rate": 6.984673729624411e-06,
      "loss": 0.8189,
      "step": 49470
    },
    {
      "epoch": 2.5823058958602423,
      "grad_norm": 4.627367973327637,
      "learning_rate": 6.975975505801716e-06,
      "loss": 0.8457,
      "step": 49480
    },
    {
      "epoch": 2.5828277688624475,
      "grad_norm": 4.271989345550537,
      "learning_rate": 6.96727728197902e-06,
      "loss": 0.8056,
      "step": 49490
    },
    {
      "epoch": 2.583349641864652,
      "grad_norm": 4.980105876922607,
      "learning_rate": 6.958579058156326e-06,
      "loss": 0.8953,
      "step": 49500
    },
    {
      "epoch": 2.583871514866857,
      "grad_norm": 4.473182201385498,
      "learning_rate": 6.949880834333629e-06,
      "loss": 0.8407,
      "step": 49510
    },
    {
      "epoch": 2.584393387869062,
      "grad_norm": 4.635773658752441,
      "learning_rate": 6.941182610510934e-06,
      "loss": 0.8576,
      "step": 49520
    },
    {
      "epoch": 2.584915260871267,
      "grad_norm": 4.822361469268799,
      "learning_rate": 6.932484386688239e-06,
      "loss": 0.8818,
      "step": 49530
    },
    {
      "epoch": 2.585437133873472,
      "grad_norm": 4.5942840576171875,
      "learning_rate": 6.923786162865543e-06,
      "loss": 0.7847,
      "step": 49540
    },
    {
      "epoch": 2.585959006875677,
      "grad_norm": 4.339303016662598,
      "learning_rate": 6.9150879390428485e-06,
      "loss": 0.8494,
      "step": 49550
    },
    {
      "epoch": 2.5864808798778816,
      "grad_norm": 5.075150489807129,
      "learning_rate": 6.906389715220152e-06,
      "loss": 0.8606,
      "step": 49560
    },
    {
      "epoch": 2.5870027528800867,
      "grad_norm": 4.107693672180176,
      "learning_rate": 6.8976914913974564e-06,
      "loss": 0.8215,
      "step": 49570
    },
    {
      "epoch": 2.5875246258822915,
      "grad_norm": 5.121021747589111,
      "learning_rate": 6.888993267574762e-06,
      "loss": 0.8438,
      "step": 49580
    },
    {
      "epoch": 2.5880464988844967,
      "grad_norm": 5.069549560546875,
      "learning_rate": 6.880295043752066e-06,
      "loss": 0.834,
      "step": 49590
    },
    {
      "epoch": 2.5885683718867014,
      "grad_norm": 4.717019081115723,
      "learning_rate": 6.871596819929371e-06,
      "loss": 0.8198,
      "step": 49600
    },
    {
      "epoch": 2.589090244888906,
      "grad_norm": 3.896070718765259,
      "learning_rate": 6.862898596106675e-06,
      "loss": 0.8619,
      "step": 49610
    },
    {
      "epoch": 2.5896121178911113,
      "grad_norm": 4.310014724731445,
      "learning_rate": 6.854200372283979e-06,
      "loss": 0.8427,
      "step": 49620
    },
    {
      "epoch": 2.590133990893316,
      "grad_norm": 5.039496898651123,
      "learning_rate": 6.8455021484612845e-06,
      "loss": 0.7232,
      "step": 49630
    },
    {
      "epoch": 2.5906558638955213,
      "grad_norm": 5.036965370178223,
      "learning_rate": 6.836803924638589e-06,
      "loss": 0.8607,
      "step": 49640
    },
    {
      "epoch": 2.591177736897726,
      "grad_norm": 5.8737335205078125,
      "learning_rate": 6.828105700815894e-06,
      "loss": 0.8552,
      "step": 49650
    },
    {
      "epoch": 2.5916996098999308,
      "grad_norm": 5.267937660217285,
      "learning_rate": 6.819407476993199e-06,
      "loss": 0.9129,
      "step": 49660
    },
    {
      "epoch": 2.5922214829021355,
      "grad_norm": 4.373376369476318,
      "learning_rate": 6.810709253170502e-06,
      "loss": 0.8402,
      "step": 49670
    },
    {
      "epoch": 2.5927433559043407,
      "grad_norm": 5.194997310638428,
      "learning_rate": 6.802011029347807e-06,
      "loss": 0.859,
      "step": 49680
    },
    {
      "epoch": 2.5932652289065454,
      "grad_norm": 4.041657447814941,
      "learning_rate": 6.7933128055251126e-06,
      "loss": 0.8273,
      "step": 49690
    },
    {
      "epoch": 2.5937871019087506,
      "grad_norm": 4.389891147613525,
      "learning_rate": 6.784614581702417e-06,
      "loss": 0.8723,
      "step": 49700
    },
    {
      "epoch": 2.5943089749109554,
      "grad_norm": 4.323813438415527,
      "learning_rate": 6.775916357879722e-06,
      "loss": 0.8458,
      "step": 49710
    },
    {
      "epoch": 2.59483084791316,
      "grad_norm": 5.239922046661377,
      "learning_rate": 6.767218134057026e-06,
      "loss": 0.8315,
      "step": 49720
    },
    {
      "epoch": 2.5953527209153653,
      "grad_norm": 4.520493984222412,
      "learning_rate": 6.75851991023433e-06,
      "loss": 0.881,
      "step": 49730
    },
    {
      "epoch": 2.59587459391757,
      "grad_norm": 4.355101585388184,
      "learning_rate": 6.749821686411635e-06,
      "loss": 0.7741,
      "step": 49740
    },
    {
      "epoch": 2.596396466919775,
      "grad_norm": 5.504223823547363,
      "learning_rate": 6.74112346258894e-06,
      "loss": 0.8809,
      "step": 49750
    },
    {
      "epoch": 2.59691833992198,
      "grad_norm": 4.778857707977295,
      "learning_rate": 6.732425238766245e-06,
      "loss": 0.7972,
      "step": 49760
    },
    {
      "epoch": 2.5974402129241847,
      "grad_norm": 5.102563381195068,
      "learning_rate": 6.723727014943549e-06,
      "loss": 0.9008,
      "step": 49770
    },
    {
      "epoch": 2.59796208592639,
      "grad_norm": 5.288134574890137,
      "learning_rate": 6.715028791120853e-06,
      "loss": 0.8838,
      "step": 49780
    },
    {
      "epoch": 2.5984839589285946,
      "grad_norm": 4.7076640129089355,
      "learning_rate": 6.706330567298158e-06,
      "loss": 0.8444,
      "step": 49790
    },
    {
      "epoch": 2.5990058319308,
      "grad_norm": 5.305046081542969,
      "learning_rate": 6.697632343475463e-06,
      "loss": 0.8583,
      "step": 49800
    },
    {
      "epoch": 2.5995277049330046,
      "grad_norm": 4.871392726898193,
      "learning_rate": 6.688934119652768e-06,
      "loss": 0.8813,
      "step": 49810
    },
    {
      "epoch": 2.6000495779352093,
      "grad_norm": 4.320080757141113,
      "learning_rate": 6.680235895830071e-06,
      "loss": 0.919,
      "step": 49820
    },
    {
      "epoch": 2.6005714509374145,
      "grad_norm": 4.502804756164551,
      "learning_rate": 6.671537672007376e-06,
      "loss": 0.8964,
      "step": 49830
    },
    {
      "epoch": 2.6010933239396192,
      "grad_norm": 4.5040669441223145,
      "learning_rate": 6.662839448184681e-06,
      "loss": 0.795,
      "step": 49840
    },
    {
      "epoch": 2.6016151969418244,
      "grad_norm": 4.901814937591553,
      "learning_rate": 6.6541412243619855e-06,
      "loss": 0.8703,
      "step": 49850
    },
    {
      "epoch": 2.602137069944029,
      "grad_norm": 4.3383660316467285,
      "learning_rate": 6.645443000539291e-06,
      "loss": 0.8982,
      "step": 49860
    },
    {
      "epoch": 2.602658942946234,
      "grad_norm": 3.7374932765960693,
      "learning_rate": 6.636744776716594e-06,
      "loss": 0.8506,
      "step": 49870
    },
    {
      "epoch": 2.603180815948439,
      "grad_norm": 4.789984226226807,
      "learning_rate": 6.628046552893899e-06,
      "loss": 0.8402,
      "step": 49880
    },
    {
      "epoch": 2.603702688950644,
      "grad_norm": 4.657320499420166,
      "learning_rate": 6.619348329071204e-06,
      "loss": 0.9207,
      "step": 49890
    },
    {
      "epoch": 2.604224561952849,
      "grad_norm": 4.317512512207031,
      "learning_rate": 6.610650105248508e-06,
      "loss": 0.8895,
      "step": 49900
    },
    {
      "epoch": 2.6047464349550538,
      "grad_norm": 4.457287311553955,
      "learning_rate": 6.6019518814258135e-06,
      "loss": 0.8305,
      "step": 49910
    },
    {
      "epoch": 2.6052683079572585,
      "grad_norm": 5.076478481292725,
      "learning_rate": 6.593253657603119e-06,
      "loss": 0.9221,
      "step": 49920
    },
    {
      "epoch": 2.6057901809594632,
      "grad_norm": 4.159174919128418,
      "learning_rate": 6.5845554337804215e-06,
      "loss": 0.8722,
      "step": 49930
    },
    {
      "epoch": 2.6063120539616684,
      "grad_norm": 4.233461380004883,
      "learning_rate": 6.575857209957727e-06,
      "loss": 0.8077,
      "step": 49940
    },
    {
      "epoch": 2.6068339269638736,
      "grad_norm": 4.515453338623047,
      "learning_rate": 6.567158986135032e-06,
      "loss": 0.8593,
      "step": 49950
    },
    {
      "epoch": 2.6073557999660784,
      "grad_norm": 3.936107873916626,
      "learning_rate": 6.558460762312336e-06,
      "loss": 0.7814,
      "step": 49960
    },
    {
      "epoch": 2.607877672968283,
      "grad_norm": 4.541705131530762,
      "learning_rate": 6.549762538489642e-06,
      "loss": 0.8604,
      "step": 49970
    },
    {
      "epoch": 2.608399545970488,
      "grad_norm": 4.12291145324707,
      "learning_rate": 6.541064314666945e-06,
      "loss": 0.84,
      "step": 49980
    },
    {
      "epoch": 2.608921418972693,
      "grad_norm": 5.080279350280762,
      "learning_rate": 6.5323660908442495e-06,
      "loss": 0.906,
      "step": 49990
    },
    {
      "epoch": 2.6094432919748978,
      "grad_norm": 5.2080769538879395,
      "learning_rate": 6.523667867021555e-06,
      "loss": 0.8489,
      "step": 50000
    },
    {
      "epoch": 2.609965164977103,
      "grad_norm": 4.383556842803955,
      "learning_rate": 6.514969643198859e-06,
      "loss": 0.8388,
      "step": 50010
    },
    {
      "epoch": 2.6104870379793077,
      "grad_norm": 4.05965518951416,
      "learning_rate": 6.506271419376164e-06,
      "loss": 0.7307,
      "step": 50020
    },
    {
      "epoch": 2.6110089109815124,
      "grad_norm": 4.348365783691406,
      "learning_rate": 6.497573195553468e-06,
      "loss": 0.837,
      "step": 50030
    },
    {
      "epoch": 2.6115307839837176,
      "grad_norm": 4.710789680480957,
      "learning_rate": 6.488874971730772e-06,
      "loss": 0.7805,
      "step": 50040
    },
    {
      "epoch": 2.6120526569859224,
      "grad_norm": 4.878026485443115,
      "learning_rate": 6.480176747908078e-06,
      "loss": 0.7325,
      "step": 50050
    },
    {
      "epoch": 2.6125745299881276,
      "grad_norm": 4.550336837768555,
      "learning_rate": 6.471478524085382e-06,
      "loss": 0.851,
      "step": 50060
    },
    {
      "epoch": 2.6130964029903323,
      "grad_norm": 5.0207953453063965,
      "learning_rate": 6.462780300262687e-06,
      "loss": 0.949,
      "step": 50070
    },
    {
      "epoch": 2.613618275992537,
      "grad_norm": 4.882715702056885,
      "learning_rate": 6.454082076439991e-06,
      "loss": 0.8421,
      "step": 50080
    },
    {
      "epoch": 2.6141401489947422,
      "grad_norm": 5.135997772216797,
      "learning_rate": 6.445383852617295e-06,
      "loss": 0.8265,
      "step": 50090
    },
    {
      "epoch": 2.614662021996947,
      "grad_norm": 4.864677429199219,
      "learning_rate": 6.4366856287946004e-06,
      "loss": 0.8278,
      "step": 50100
    },
    {
      "epoch": 2.615183894999152,
      "grad_norm": 4.544918060302734,
      "learning_rate": 6.427987404971905e-06,
      "loss": 0.8514,
      "step": 50110
    },
    {
      "epoch": 2.615705768001357,
      "grad_norm": 4.8951897621154785,
      "learning_rate": 6.42015900353148e-06,
      "loss": 0.8635,
      "step": 50120
    },
    {
      "epoch": 2.6162276410035616,
      "grad_norm": 6.209921360015869,
      "learning_rate": 6.411460779708783e-06,
      "loss": 0.8434,
      "step": 50130
    },
    {
      "epoch": 2.616749514005767,
      "grad_norm": 4.850776195526123,
      "learning_rate": 6.4027625558860885e-06,
      "loss": 0.9036,
      "step": 50140
    },
    {
      "epoch": 2.6172713870079716,
      "grad_norm": 4.777790546417236,
      "learning_rate": 6.394064332063393e-06,
      "loss": 0.8583,
      "step": 50150
    },
    {
      "epoch": 2.6177932600101768,
      "grad_norm": 4.5929107666015625,
      "learning_rate": 6.385366108240698e-06,
      "loss": 0.8142,
      "step": 50160
    },
    {
      "epoch": 2.6183151330123815,
      "grad_norm": 4.943526268005371,
      "learning_rate": 6.3766678844180025e-06,
      "loss": 0.9216,
      "step": 50170
    },
    {
      "epoch": 2.6188370060145862,
      "grad_norm": 4.836055278778076,
      "learning_rate": 6.367969660595306e-06,
      "loss": 0.9138,
      "step": 50180
    },
    {
      "epoch": 2.619358879016791,
      "grad_norm": 4.785026550292969,
      "learning_rate": 6.359271436772611e-06,
      "loss": 0.7728,
      "step": 50190
    },
    {
      "epoch": 2.619880752018996,
      "grad_norm": 4.349278450012207,
      "learning_rate": 6.350573212949916e-06,
      "loss": 0.8219,
      "step": 50200
    },
    {
      "epoch": 2.6204026250212014,
      "grad_norm": 4.583593845367432,
      "learning_rate": 6.341874989127221e-06,
      "loss": 0.7594,
      "step": 50210
    },
    {
      "epoch": 2.620924498023406,
      "grad_norm": 5.050291538238525,
      "learning_rate": 6.333176765304525e-06,
      "loss": 0.8547,
      "step": 50220
    },
    {
      "epoch": 2.621446371025611,
      "grad_norm": 5.25440788269043,
      "learning_rate": 6.324478541481829e-06,
      "loss": 0.834,
      "step": 50230
    },
    {
      "epoch": 2.6219682440278156,
      "grad_norm": 4.850320816040039,
      "learning_rate": 6.315780317659134e-06,
      "loss": 0.8529,
      "step": 50240
    },
    {
      "epoch": 2.6224901170300208,
      "grad_norm": 4.545275688171387,
      "learning_rate": 6.3070820938364386e-06,
      "loss": 0.8236,
      "step": 50250
    },
    {
      "epoch": 2.6230119900322255,
      "grad_norm": 4.464476108551025,
      "learning_rate": 6.298383870013744e-06,
      "loss": 0.8827,
      "step": 50260
    },
    {
      "epoch": 2.6235338630344307,
      "grad_norm": 3.985719680786133,
      "learning_rate": 6.289685646191048e-06,
      "loss": 0.8809,
      "step": 50270
    },
    {
      "epoch": 2.6240557360366354,
      "grad_norm": 4.256586074829102,
      "learning_rate": 6.280987422368352e-06,
      "loss": 0.8142,
      "step": 50280
    },
    {
      "epoch": 2.62457760903884,
      "grad_norm": 5.310003757476807,
      "learning_rate": 6.272289198545657e-06,
      "loss": 0.8315,
      "step": 50290
    },
    {
      "epoch": 2.6250994820410454,
      "grad_norm": 3.892699718475342,
      "learning_rate": 6.263590974722961e-06,
      "loss": 0.7162,
      "step": 50300
    },
    {
      "epoch": 2.62562135504325,
      "grad_norm": 5.052011013031006,
      "learning_rate": 6.254892750900267e-06,
      "loss": 0.8231,
      "step": 50310
    },
    {
      "epoch": 2.6261432280454553,
      "grad_norm": 4.602260589599609,
      "learning_rate": 6.246194527077571e-06,
      "loss": 0.871,
      "step": 50320
    },
    {
      "epoch": 2.62666510104766,
      "grad_norm": 5.5414299964904785,
      "learning_rate": 6.237496303254875e-06,
      "loss": 0.767,
      "step": 50330
    },
    {
      "epoch": 2.627186974049865,
      "grad_norm": 5.108911514282227,
      "learning_rate": 6.228798079432181e-06,
      "loss": 0.8075,
      "step": 50340
    },
    {
      "epoch": 2.62770884705207,
      "grad_norm": 4.651497840881348,
      "learning_rate": 6.220099855609485e-06,
      "loss": 0.8572,
      "step": 50350
    },
    {
      "epoch": 2.6282307200542747,
      "grad_norm": 4.920731067657471,
      "learning_rate": 6.2114016317867895e-06,
      "loss": 0.7829,
      "step": 50360
    },
    {
      "epoch": 2.62875259305648,
      "grad_norm": 5.16374397277832,
      "learning_rate": 6.202703407964094e-06,
      "loss": 0.85,
      "step": 50370
    },
    {
      "epoch": 2.6292744660586846,
      "grad_norm": 4.703656196594238,
      "learning_rate": 6.194005184141398e-06,
      "loss": 0.8287,
      "step": 50380
    },
    {
      "epoch": 2.6297963390608894,
      "grad_norm": 4.909635066986084,
      "learning_rate": 6.1853069603187035e-06,
      "loss": 0.89,
      "step": 50390
    },
    {
      "epoch": 2.6303182120630946,
      "grad_norm": 3.897876262664795,
      "learning_rate": 6.176608736496008e-06,
      "loss": 0.7826,
      "step": 50400
    },
    {
      "epoch": 2.6308400850652993,
      "grad_norm": 5.468498706817627,
      "learning_rate": 6.167910512673312e-06,
      "loss": 0.8674,
      "step": 50410
    },
    {
      "epoch": 2.6313619580675045,
      "grad_norm": 4.566795349121094,
      "learning_rate": 6.159212288850617e-06,
      "loss": 0.8568,
      "step": 50420
    },
    {
      "epoch": 2.6318838310697092,
      "grad_norm": 4.561558246612549,
      "learning_rate": 6.150514065027922e-06,
      "loss": 0.8397,
      "step": 50430
    },
    {
      "epoch": 2.632405704071914,
      "grad_norm": 4.170536994934082,
      "learning_rate": 6.141815841205226e-06,
      "loss": 0.8226,
      "step": 50440
    },
    {
      "epoch": 2.632927577074119,
      "grad_norm": 5.134334087371826,
      "learning_rate": 6.133117617382531e-06,
      "loss": 0.9573,
      "step": 50450
    },
    {
      "epoch": 2.633449450076324,
      "grad_norm": 4.176207065582275,
      "learning_rate": 6.124419393559835e-06,
      "loss": 0.8016,
      "step": 50460
    },
    {
      "epoch": 2.633971323078529,
      "grad_norm": 4.108767509460449,
      "learning_rate": 6.11572116973714e-06,
      "loss": 0.8538,
      "step": 50470
    },
    {
      "epoch": 2.634493196080734,
      "grad_norm": 4.521862506866455,
      "learning_rate": 6.107022945914445e-06,
      "loss": 0.7566,
      "step": 50480
    },
    {
      "epoch": 2.6350150690829386,
      "grad_norm": 4.191551685333252,
      "learning_rate": 6.098324722091749e-06,
      "loss": 0.8342,
      "step": 50490
    },
    {
      "epoch": 2.6355369420851433,
      "grad_norm": 4.636600494384766,
      "learning_rate": 6.0896264982690535e-06,
      "loss": 0.9121,
      "step": 50500
    },
    {
      "epoch": 2.6360588150873485,
      "grad_norm": 4.546901702880859,
      "learning_rate": 6.080928274446358e-06,
      "loss": 0.8082,
      "step": 50510
    },
    {
      "epoch": 2.6365806880895533,
      "grad_norm": 4.893102169036865,
      "learning_rate": 6.072230050623663e-06,
      "loss": 0.9064,
      "step": 50520
    },
    {
      "epoch": 2.6371025610917584,
      "grad_norm": 4.8964128494262695,
      "learning_rate": 6.063531826800968e-06,
      "loss": 0.8677,
      "step": 50530
    },
    {
      "epoch": 2.637624434093963,
      "grad_norm": 5.4127888679504395,
      "learning_rate": 6.054833602978272e-06,
      "loss": 0.9116,
      "step": 50540
    },
    {
      "epoch": 2.638146307096168,
      "grad_norm": 4.794893264770508,
      "learning_rate": 6.046135379155577e-06,
      "loss": 0.8533,
      "step": 50550
    },
    {
      "epoch": 2.638668180098373,
      "grad_norm": 4.559169769287109,
      "learning_rate": 6.037437155332882e-06,
      "loss": 0.8775,
      "step": 50560
    },
    {
      "epoch": 2.639190053100578,
      "grad_norm": 5.026275634765625,
      "learning_rate": 6.028738931510186e-06,
      "loss": 0.8324,
      "step": 50570
    },
    {
      "epoch": 2.639711926102783,
      "grad_norm": 4.747798919677734,
      "learning_rate": 6.02004070768749e-06,
      "loss": 0.7852,
      "step": 50580
    },
    {
      "epoch": 2.640233799104988,
      "grad_norm": 5.9120049476623535,
      "learning_rate": 6.011342483864795e-06,
      "loss": 0.9011,
      "step": 50590
    },
    {
      "epoch": 2.6407556721071925,
      "grad_norm": 4.85453987121582,
      "learning_rate": 6.0026442600421e-06,
      "loss": 0.8116,
      "step": 50600
    },
    {
      "epoch": 2.6412775451093977,
      "grad_norm": 5.064073085784912,
      "learning_rate": 5.9939460362194044e-06,
      "loss": 0.7941,
      "step": 50610
    },
    {
      "epoch": 2.6417994181116025,
      "grad_norm": 5.8861846923828125,
      "learning_rate": 5.985247812396709e-06,
      "loss": 0.8019,
      "step": 50620
    },
    {
      "epoch": 2.6423212911138076,
      "grad_norm": 4.626157760620117,
      "learning_rate": 5.976549588574013e-06,
      "loss": 0.8276,
      "step": 50630
    },
    {
      "epoch": 2.6428431641160124,
      "grad_norm": 5.786007404327393,
      "learning_rate": 5.967851364751318e-06,
      "loss": 0.9041,
      "step": 50640
    },
    {
      "epoch": 2.643365037118217,
      "grad_norm": 4.724814414978027,
      "learning_rate": 5.959153140928623e-06,
      "loss": 0.8584,
      "step": 50650
    },
    {
      "epoch": 2.6438869101204223,
      "grad_norm": 5.421228885650635,
      "learning_rate": 5.950454917105927e-06,
      "loss": 0.8003,
      "step": 50660
    },
    {
      "epoch": 2.644408783122627,
      "grad_norm": 4.161221504211426,
      "learning_rate": 5.941756693283232e-06,
      "loss": 0.7998,
      "step": 50670
    },
    {
      "epoch": 2.6449306561248322,
      "grad_norm": 4.6871018409729,
      "learning_rate": 5.933058469460537e-06,
      "loss": 0.8311,
      "step": 50680
    },
    {
      "epoch": 2.645452529127037,
      "grad_norm": 4.692993640899658,
      "learning_rate": 5.924360245637841e-06,
      "loss": 0.857,
      "step": 50690
    },
    {
      "epoch": 2.6459744021292417,
      "grad_norm": 4.994898319244385,
      "learning_rate": 5.915662021815146e-06,
      "loss": 0.8605,
      "step": 50700
    },
    {
      "epoch": 2.646496275131447,
      "grad_norm": 4.701370716094971,
      "learning_rate": 5.90696379799245e-06,
      "loss": 0.8141,
      "step": 50710
    },
    {
      "epoch": 2.6470181481336517,
      "grad_norm": 5.009135723114014,
      "learning_rate": 5.8982655741697545e-06,
      "loss": 0.7704,
      "step": 50720
    },
    {
      "epoch": 2.647540021135857,
      "grad_norm": 5.775739669799805,
      "learning_rate": 5.88956735034706e-06,
      "loss": 0.8968,
      "step": 50730
    },
    {
      "epoch": 2.6480618941380616,
      "grad_norm": 4.840253829956055,
      "learning_rate": 5.880869126524364e-06,
      "loss": 0.8101,
      "step": 50740
    },
    {
      "epoch": 2.6485837671402663,
      "grad_norm": 4.712198734283447,
      "learning_rate": 5.8721709027016685e-06,
      "loss": 0.8612,
      "step": 50750
    },
    {
      "epoch": 2.649105640142471,
      "grad_norm": 4.7216620445251465,
      "learning_rate": 5.863472678878973e-06,
      "loss": 0.7501,
      "step": 50760
    },
    {
      "epoch": 2.6496275131446763,
      "grad_norm": 5.149540424346924,
      "learning_rate": 5.854774455056278e-06,
      "loss": 0.873,
      "step": 50770
    },
    {
      "epoch": 2.650149386146881,
      "grad_norm": 5.307474613189697,
      "learning_rate": 5.8460762312335826e-06,
      "loss": 0.8596,
      "step": 50780
    },
    {
      "epoch": 2.650671259149086,
      "grad_norm": 4.241281032562256,
      "learning_rate": 5.837378007410887e-06,
      "loss": 0.8814,
      "step": 50790
    },
    {
      "epoch": 2.651193132151291,
      "grad_norm": 4.388737678527832,
      "learning_rate": 5.828679783588191e-06,
      "loss": 0.8564,
      "step": 50800
    },
    {
      "epoch": 2.6517150051534957,
      "grad_norm": 4.257277488708496,
      "learning_rate": 5.819981559765497e-06,
      "loss": 0.8389,
      "step": 50810
    },
    {
      "epoch": 2.652236878155701,
      "grad_norm": 5.073216915130615,
      "learning_rate": 5.811283335942801e-06,
      "loss": 0.886,
      "step": 50820
    },
    {
      "epoch": 2.6527587511579056,
      "grad_norm": 4.663071155548096,
      "learning_rate": 5.802585112120105e-06,
      "loss": 0.9861,
      "step": 50830
    },
    {
      "epoch": 2.653280624160111,
      "grad_norm": 4.564983367919922,
      "learning_rate": 5.79388688829741e-06,
      "loss": 0.7714,
      "step": 50840
    },
    {
      "epoch": 2.6538024971623155,
      "grad_norm": 5.266777038574219,
      "learning_rate": 5.785188664474714e-06,
      "loss": 0.8566,
      "step": 50850
    },
    {
      "epoch": 2.6543243701645203,
      "grad_norm": 4.426299571990967,
      "learning_rate": 5.776490440652019e-06,
      "loss": 0.8101,
      "step": 50860
    },
    {
      "epoch": 2.6548462431667255,
      "grad_norm": 4.3791279792785645,
      "learning_rate": 5.767792216829324e-06,
      "loss": 0.8269,
      "step": 50870
    },
    {
      "epoch": 2.65536811616893,
      "grad_norm": 4.733217716217041,
      "learning_rate": 5.759093993006628e-06,
      "loss": 0.8578,
      "step": 50880
    },
    {
      "epoch": 2.6558899891711354,
      "grad_norm": 4.802331447601318,
      "learning_rate": 5.750395769183933e-06,
      "loss": 0.8739,
      "step": 50890
    },
    {
      "epoch": 2.65641186217334,
      "grad_norm": 4.479786396026611,
      "learning_rate": 5.741697545361238e-06,
      "loss": 0.8687,
      "step": 50900
    },
    {
      "epoch": 2.656933735175545,
      "grad_norm": 4.629636764526367,
      "learning_rate": 5.732999321538542e-06,
      "loss": 0.858,
      "step": 50910
    },
    {
      "epoch": 2.65745560817775,
      "grad_norm": 4.7573065757751465,
      "learning_rate": 5.724301097715847e-06,
      "loss": 0.8813,
      "step": 50920
    },
    {
      "epoch": 2.657977481179955,
      "grad_norm": 4.862158298492432,
      "learning_rate": 5.715602873893151e-06,
      "loss": 0.9232,
      "step": 50930
    },
    {
      "epoch": 2.65849935418216,
      "grad_norm": 4.935567378997803,
      "learning_rate": 5.706904650070456e-06,
      "loss": 0.8449,
      "step": 50940
    },
    {
      "epoch": 2.6590212271843647,
      "grad_norm": 5.288764476776123,
      "learning_rate": 5.698206426247761e-06,
      "loss": 0.8537,
      "step": 50950
    },
    {
      "epoch": 2.6595431001865695,
      "grad_norm": 4.487301349639893,
      "learning_rate": 5.689508202425065e-06,
      "loss": 0.8378,
      "step": 50960
    },
    {
      "epoch": 2.6600649731887747,
      "grad_norm": 5.181509494781494,
      "learning_rate": 5.6808099786023695e-06,
      "loss": 0.8921,
      "step": 50970
    },
    {
      "epoch": 2.6605868461909794,
      "grad_norm": 5.14766263961792,
      "learning_rate": 5.672111754779674e-06,
      "loss": 0.8772,
      "step": 50980
    },
    {
      "epoch": 2.6611087191931846,
      "grad_norm": 4.133881568908691,
      "learning_rate": 5.663413530956979e-06,
      "loss": 0.8274,
      "step": 50990
    },
    {
      "epoch": 2.6616305921953893,
      "grad_norm": 4.757988452911377,
      "learning_rate": 5.6547153071342835e-06,
      "loss": 0.797,
      "step": 51000
    },
    {
      "epoch": 2.662152465197594,
      "grad_norm": 4.429468631744385,
      "learning_rate": 5.646017083311588e-06,
      "loss": 0.822,
      "step": 51010
    },
    {
      "epoch": 2.662674338199799,
      "grad_norm": 5.15580940246582,
      "learning_rate": 5.637318859488892e-06,
      "loss": 0.8597,
      "step": 51020
    },
    {
      "epoch": 2.663196211202004,
      "grad_norm": 4.504083156585693,
      "learning_rate": 5.6286206356661975e-06,
      "loss": 0.777,
      "step": 51030
    },
    {
      "epoch": 2.6637180842042087,
      "grad_norm": 4.289736747741699,
      "learning_rate": 5.619922411843502e-06,
      "loss": 0.7622,
      "step": 51040
    },
    {
      "epoch": 2.664239957206414,
      "grad_norm": 4.322784900665283,
      "learning_rate": 5.611224188020806e-06,
      "loss": 0.8482,
      "step": 51050
    },
    {
      "epoch": 2.6647618302086187,
      "grad_norm": 4.058933734893799,
      "learning_rate": 5.602525964198111e-06,
      "loss": 0.8297,
      "step": 51060
    },
    {
      "epoch": 2.6652837032108234,
      "grad_norm": 5.1375555992126465,
      "learning_rate": 5.593827740375416e-06,
      "loss": 0.8392,
      "step": 51070
    },
    {
      "epoch": 2.6658055762130286,
      "grad_norm": 4.810498237609863,
      "learning_rate": 5.58512951655272e-06,
      "loss": 0.8748,
      "step": 51080
    },
    {
      "epoch": 2.6663274492152333,
      "grad_norm": 4.236181259155273,
      "learning_rate": 5.576431292730025e-06,
      "loss": 0.7901,
      "step": 51090
    },
    {
      "epoch": 2.6668493222174385,
      "grad_norm": 5.012507915496826,
      "learning_rate": 5.567733068907329e-06,
      "loss": 0.7849,
      "step": 51100
    },
    {
      "epoch": 2.6673711952196433,
      "grad_norm": 5.151932239532471,
      "learning_rate": 5.559034845084634e-06,
      "loss": 0.834,
      "step": 51110
    },
    {
      "epoch": 2.667893068221848,
      "grad_norm": 4.753223896026611,
      "learning_rate": 5.550336621261939e-06,
      "loss": 0.8093,
      "step": 51120
    },
    {
      "epoch": 2.668414941224053,
      "grad_norm": 4.892752647399902,
      "learning_rate": 5.541638397439243e-06,
      "loss": 0.8736,
      "step": 51130
    },
    {
      "epoch": 2.668936814226258,
      "grad_norm": 4.358457088470459,
      "learning_rate": 5.532940173616548e-06,
      "loss": 0.8858,
      "step": 51140
    },
    {
      "epoch": 2.669458687228463,
      "grad_norm": 4.709717273712158,
      "learning_rate": 5.524241949793852e-06,
      "loss": 0.8658,
      "step": 51150
    },
    {
      "epoch": 2.669980560230668,
      "grad_norm": 4.7882490158081055,
      "learning_rate": 5.515543725971157e-06,
      "loss": 0.8277,
      "step": 51160
    },
    {
      "epoch": 2.6705024332328726,
      "grad_norm": 5.205420970916748,
      "learning_rate": 5.506845502148462e-06,
      "loss": 0.8993,
      "step": 51170
    },
    {
      "epoch": 2.671024306235078,
      "grad_norm": 5.073353290557861,
      "learning_rate": 5.498147278325766e-06,
      "loss": 0.8691,
      "step": 51180
    },
    {
      "epoch": 2.6715461792372825,
      "grad_norm": 4.916073322296143,
      "learning_rate": 5.48944905450307e-06,
      "loss": 0.7803,
      "step": 51190
    },
    {
      "epoch": 2.6720680522394877,
      "grad_norm": 4.99066162109375,
      "learning_rate": 5.480750830680376e-06,
      "loss": 0.8587,
      "step": 51200
    },
    {
      "epoch": 2.6725899252416925,
      "grad_norm": 4.773209095001221,
      "learning_rate": 5.47205260685768e-06,
      "loss": 0.8954,
      "step": 51210
    },
    {
      "epoch": 2.673111798243897,
      "grad_norm": 4.377996921539307,
      "learning_rate": 5.4633543830349845e-06,
      "loss": 0.7721,
      "step": 51220
    },
    {
      "epoch": 2.6736336712461024,
      "grad_norm": 5.285329818725586,
      "learning_rate": 5.454656159212289e-06,
      "loss": 0.7847,
      "step": 51230
    },
    {
      "epoch": 2.674155544248307,
      "grad_norm": 5.008666515350342,
      "learning_rate": 5.445957935389594e-06,
      "loss": 0.9634,
      "step": 51240
    },
    {
      "epoch": 2.6746774172505123,
      "grad_norm": 4.380336284637451,
      "learning_rate": 5.4372597115668985e-06,
      "loss": 0.8029,
      "step": 51250
    },
    {
      "epoch": 2.675199290252717,
      "grad_norm": 4.606847286224365,
      "learning_rate": 5.428561487744203e-06,
      "loss": 0.7743,
      "step": 51260
    },
    {
      "epoch": 2.675721163254922,
      "grad_norm": 4.817721843719482,
      "learning_rate": 5.419863263921507e-06,
      "loss": 0.9888,
      "step": 51270
    },
    {
      "epoch": 2.6762430362571266,
      "grad_norm": 5.0249152183532715,
      "learning_rate": 5.411165040098812e-06,
      "loss": 0.7935,
      "step": 51280
    },
    {
      "epoch": 2.6767649092593317,
      "grad_norm": 4.431534767150879,
      "learning_rate": 5.402466816276117e-06,
      "loss": 0.8624,
      "step": 51290
    },
    {
      "epoch": 2.6772867822615365,
      "grad_norm": 5.54473352432251,
      "learning_rate": 5.393768592453421e-06,
      "loss": 0.9393,
      "step": 51300
    },
    {
      "epoch": 2.6778086552637417,
      "grad_norm": 4.752933025360107,
      "learning_rate": 5.385070368630726e-06,
      "loss": 0.8009,
      "step": 51310
    },
    {
      "epoch": 2.6783305282659464,
      "grad_norm": 4.811878204345703,
      "learning_rate": 5.37637214480803e-06,
      "loss": 0.9558,
      "step": 51320
    },
    {
      "epoch": 2.678852401268151,
      "grad_norm": 5.043144226074219,
      "learning_rate": 5.367673920985335e-06,
      "loss": 0.9345,
      "step": 51330
    },
    {
      "epoch": 2.6793742742703563,
      "grad_norm": 4.561601161956787,
      "learning_rate": 5.35897569716264e-06,
      "loss": 0.812,
      "step": 51340
    },
    {
      "epoch": 2.679896147272561,
      "grad_norm": 4.397334098815918,
      "learning_rate": 5.350277473339944e-06,
      "loss": 0.7516,
      "step": 51350
    },
    {
      "epoch": 2.6804180202747663,
      "grad_norm": 4.54673433303833,
      "learning_rate": 5.3415792495172485e-06,
      "loss": 0.8885,
      "step": 51360
    },
    {
      "epoch": 2.680939893276971,
      "grad_norm": 4.87671422958374,
      "learning_rate": 5.332881025694554e-06,
      "loss": 0.8637,
      "step": 51370
    },
    {
      "epoch": 2.6814617662791758,
      "grad_norm": 4.568238258361816,
      "learning_rate": 5.324182801871858e-06,
      "loss": 0.7966,
      "step": 51380
    },
    {
      "epoch": 2.681983639281381,
      "grad_norm": 4.410244941711426,
      "learning_rate": 5.3154845780491626e-06,
      "loss": 0.7822,
      "step": 51390
    },
    {
      "epoch": 2.6825055122835857,
      "grad_norm": 5.025923252105713,
      "learning_rate": 5.306786354226467e-06,
      "loss": 0.8014,
      "step": 51400
    },
    {
      "epoch": 2.683027385285791,
      "grad_norm": 4.385766506195068,
      "learning_rate": 5.298088130403771e-06,
      "loss": 0.8757,
      "step": 51410
    },
    {
      "epoch": 2.6835492582879956,
      "grad_norm": 4.332940578460693,
      "learning_rate": 5.289389906581077e-06,
      "loss": 0.9003,
      "step": 51420
    },
    {
      "epoch": 2.6840711312902004,
      "grad_norm": 4.908944606781006,
      "learning_rate": 5.280691682758381e-06,
      "loss": 0.8592,
      "step": 51430
    },
    {
      "epoch": 2.6845930042924055,
      "grad_norm": 4.7530364990234375,
      "learning_rate": 5.271993458935685e-06,
      "loss": 0.9047,
      "step": 51440
    },
    {
      "epoch": 2.6851148772946103,
      "grad_norm": 4.781326770782471,
      "learning_rate": 5.263295235112991e-06,
      "loss": 0.8048,
      "step": 51450
    },
    {
      "epoch": 2.6856367502968155,
      "grad_norm": 4.898756504058838,
      "learning_rate": 5.254597011290295e-06,
      "loss": 0.9071,
      "step": 51460
    },
    {
      "epoch": 2.68615862329902,
      "grad_norm": 4.446728706359863,
      "learning_rate": 5.2458987874675994e-06,
      "loss": 0.7871,
      "step": 51470
    },
    {
      "epoch": 2.686680496301225,
      "grad_norm": 4.752274036407471,
      "learning_rate": 5.237200563644904e-06,
      "loss": 0.7899,
      "step": 51480
    },
    {
      "epoch": 2.68720236930343,
      "grad_norm": 4.859979152679443,
      "learning_rate": 5.228502339822208e-06,
      "loss": 0.8164,
      "step": 51490
    },
    {
      "epoch": 2.687724242305635,
      "grad_norm": 4.904426097869873,
      "learning_rate": 5.2198041159995135e-06,
      "loss": 0.8709,
      "step": 51500
    },
    {
      "epoch": 2.68824611530784,
      "grad_norm": 4.778563499450684,
      "learning_rate": 5.211105892176818e-06,
      "loss": 0.8955,
      "step": 51510
    },
    {
      "epoch": 2.688767988310045,
      "grad_norm": 4.9402384757995605,
      "learning_rate": 5.202407668354122e-06,
      "loss": 0.7248,
      "step": 51520
    },
    {
      "epoch": 2.6892898613122496,
      "grad_norm": 4.06679630279541,
      "learning_rate": 5.193709444531427e-06,
      "loss": 0.8179,
      "step": 51530
    },
    {
      "epoch": 2.6898117343144543,
      "grad_norm": 4.433610439300537,
      "learning_rate": 5.185011220708731e-06,
      "loss": 0.8412,
      "step": 51540
    },
    {
      "epoch": 2.6903336073166595,
      "grad_norm": 4.477039337158203,
      "learning_rate": 5.176312996886036e-06,
      "loss": 0.8433,
      "step": 51550
    },
    {
      "epoch": 2.6908554803188647,
      "grad_norm": 4.637571811676025,
      "learning_rate": 5.167614773063341e-06,
      "loss": 0.8201,
      "step": 51560
    },
    {
      "epoch": 2.6913773533210694,
      "grad_norm": 4.4046549797058105,
      "learning_rate": 5.158916549240645e-06,
      "loss": 0.8675,
      "step": 51570
    },
    {
      "epoch": 2.691899226323274,
      "grad_norm": 5.056957244873047,
      "learning_rate": 5.15021832541795e-06,
      "loss": 0.8429,
      "step": 51580
    },
    {
      "epoch": 2.692421099325479,
      "grad_norm": 4.729613304138184,
      "learning_rate": 5.141520101595255e-06,
      "loss": 0.8333,
      "step": 51590
    },
    {
      "epoch": 2.692942972327684,
      "grad_norm": 4.9230875968933105,
      "learning_rate": 5.132821877772559e-06,
      "loss": 0.8638,
      "step": 51600
    },
    {
      "epoch": 2.693464845329889,
      "grad_norm": 4.502432346343994,
      "learning_rate": 5.1241236539498635e-06,
      "loss": 0.7352,
      "step": 51610
    },
    {
      "epoch": 2.693986718332094,
      "grad_norm": 4.464000225067139,
      "learning_rate": 5.115425430127168e-06,
      "loss": 0.8602,
      "step": 51620
    },
    {
      "epoch": 2.6945085913342988,
      "grad_norm": 4.118071556091309,
      "learning_rate": 5.106727206304473e-06,
      "loss": 0.8424,
      "step": 51630
    },
    {
      "epoch": 2.6950304643365035,
      "grad_norm": 4.170115947723389,
      "learning_rate": 5.0980289824817776e-06,
      "loss": 0.9106,
      "step": 51640
    },
    {
      "epoch": 2.6955523373387087,
      "grad_norm": 4.677574634552002,
      "learning_rate": 5.089330758659082e-06,
      "loss": 0.8072,
      "step": 51650
    },
    {
      "epoch": 2.6960742103409134,
      "grad_norm": 4.061513900756836,
      "learning_rate": 5.080632534836386e-06,
      "loss": 0.8803,
      "step": 51660
    },
    {
      "epoch": 2.6965960833431186,
      "grad_norm": 5.178466320037842,
      "learning_rate": 5.071934311013691e-06,
      "loss": 1.0189,
      "step": 51670
    },
    {
      "epoch": 2.6971179563453234,
      "grad_norm": 4.480379104614258,
      "learning_rate": 5.063236087190996e-06,
      "loss": 0.8411,
      "step": 51680
    },
    {
      "epoch": 2.697639829347528,
      "grad_norm": 6.022023677825928,
      "learning_rate": 5.0545378633683e-06,
      "loss": 0.9692,
      "step": 51690
    },
    {
      "epoch": 2.6981617023497333,
      "grad_norm": 4.673121929168701,
      "learning_rate": 5.045839639545605e-06,
      "loss": 0.8823,
      "step": 51700
    },
    {
      "epoch": 2.698683575351938,
      "grad_norm": 4.600151538848877,
      "learning_rate": 5.03714141572291e-06,
      "loss": 0.8358,
      "step": 51710
    },
    {
      "epoch": 2.699205448354143,
      "grad_norm": 4.8608903884887695,
      "learning_rate": 5.028443191900214e-06,
      "loss": 0.8591,
      "step": 51720
    },
    {
      "epoch": 2.699727321356348,
      "grad_norm": 4.9549431800842285,
      "learning_rate": 5.019744968077519e-06,
      "loss": 0.8072,
      "step": 51730
    },
    {
      "epoch": 2.7002491943585527,
      "grad_norm": 4.7564377784729,
      "learning_rate": 5.011046744254823e-06,
      "loss": 0.8154,
      "step": 51740
    },
    {
      "epoch": 2.700771067360758,
      "grad_norm": 4.280674457550049,
      "learning_rate": 5.002348520432128e-06,
      "loss": 0.9029,
      "step": 51750
    },
    {
      "epoch": 2.7012929403629626,
      "grad_norm": 4.860844135284424,
      "learning_rate": 4.993650296609433e-06,
      "loss": 0.9355,
      "step": 51760
    },
    {
      "epoch": 2.701814813365168,
      "grad_norm": 4.849002361297607,
      "learning_rate": 4.984952072786737e-06,
      "loss": 0.7928,
      "step": 51770
    },
    {
      "epoch": 2.7023366863673726,
      "grad_norm": 5.0418195724487305,
      "learning_rate": 4.976253848964042e-06,
      "loss": 0.8585,
      "step": 51780
    },
    {
      "epoch": 2.7028585593695773,
      "grad_norm": 4.974592208862305,
      "learning_rate": 4.967555625141346e-06,
      "loss": 0.8761,
      "step": 51790
    },
    {
      "epoch": 2.703380432371782,
      "grad_norm": 4.583763599395752,
      "learning_rate": 4.958857401318651e-06,
      "loss": 0.7299,
      "step": 51800
    },
    {
      "epoch": 2.7039023053739872,
      "grad_norm": 4.309103488922119,
      "learning_rate": 4.950159177495956e-06,
      "loss": 0.8278,
      "step": 51810
    },
    {
      "epoch": 2.7044241783761924,
      "grad_norm": 4.475305080413818,
      "learning_rate": 4.94146095367326e-06,
      "loss": 0.9113,
      "step": 51820
    },
    {
      "epoch": 2.704946051378397,
      "grad_norm": 4.097082138061523,
      "learning_rate": 4.9327627298505645e-06,
      "loss": 0.9192,
      "step": 51830
    },
    {
      "epoch": 2.705467924380602,
      "grad_norm": 4.345293045043945,
      "learning_rate": 4.92406450602787e-06,
      "loss": 0.8532,
      "step": 51840
    },
    {
      "epoch": 2.7059897973828066,
      "grad_norm": 4.814178466796875,
      "learning_rate": 4.915366282205174e-06,
      "loss": 0.7785,
      "step": 51850
    },
    {
      "epoch": 2.706511670385012,
      "grad_norm": 5.116325855255127,
      "learning_rate": 4.9066680583824785e-06,
      "loss": 0.7721,
      "step": 51860
    },
    {
      "epoch": 2.7070335433872166,
      "grad_norm": 4.794926643371582,
      "learning_rate": 4.897969834559783e-06,
      "loss": 0.8476,
      "step": 51870
    },
    {
      "epoch": 2.7075554163894218,
      "grad_norm": 3.9442553520202637,
      "learning_rate": 4.889271610737087e-06,
      "loss": 0.7387,
      "step": 51880
    },
    {
      "epoch": 2.7080772893916265,
      "grad_norm": 4.207861423492432,
      "learning_rate": 4.8805733869143925e-06,
      "loss": 0.8055,
      "step": 51890
    },
    {
      "epoch": 2.7085991623938313,
      "grad_norm": 4.601115703582764,
      "learning_rate": 4.871875163091697e-06,
      "loss": 0.8309,
      "step": 51900
    },
    {
      "epoch": 2.7091210353960364,
      "grad_norm": 4.4802565574646,
      "learning_rate": 4.863176939269001e-06,
      "loss": 0.8534,
      "step": 51910
    },
    {
      "epoch": 2.709642908398241,
      "grad_norm": 4.914011478424072,
      "learning_rate": 4.8544787154463066e-06,
      "loss": 0.8545,
      "step": 51920
    },
    {
      "epoch": 2.7101647814004464,
      "grad_norm": 4.642910957336426,
      "learning_rate": 4.845780491623611e-06,
      "loss": 0.8545,
      "step": 51930
    },
    {
      "epoch": 2.710686654402651,
      "grad_norm": 4.592398643493652,
      "learning_rate": 4.837082267800915e-06,
      "loss": 0.8686,
      "step": 51940
    },
    {
      "epoch": 2.711208527404856,
      "grad_norm": 5.036849498748779,
      "learning_rate": 4.82838404397822e-06,
      "loss": 0.9209,
      "step": 51950
    },
    {
      "epoch": 2.711730400407061,
      "grad_norm": 4.487112522125244,
      "learning_rate": 4.819685820155524e-06,
      "loss": 0.8158,
      "step": 51960
    },
    {
      "epoch": 2.712252273409266,
      "grad_norm": 5.188543796539307,
      "learning_rate": 4.810987596332829e-06,
      "loss": 0.7402,
      "step": 51970
    },
    {
      "epoch": 2.712774146411471,
      "grad_norm": 4.706326007843018,
      "learning_rate": 4.802289372510134e-06,
      "loss": 0.9108,
      "step": 51980
    },
    {
      "epoch": 2.7132960194136757,
      "grad_norm": 4.303633689880371,
      "learning_rate": 4.793591148687438e-06,
      "loss": 0.8546,
      "step": 51990
    },
    {
      "epoch": 2.7138178924158805,
      "grad_norm": 4.848967552185059,
      "learning_rate": 4.784892924864743e-06,
      "loss": 0.829,
      "step": 52000
    },
    {
      "epoch": 2.7143397654180856,
      "grad_norm": 4.4301981925964355,
      "learning_rate": 4.776194701042047e-06,
      "loss": 0.879,
      "step": 52010
    },
    {
      "epoch": 2.7148616384202904,
      "grad_norm": 5.192040920257568,
      "learning_rate": 4.767496477219352e-06,
      "loss": 0.8644,
      "step": 52020
    },
    {
      "epoch": 2.7153835114224956,
      "grad_norm": 5.112054347991943,
      "learning_rate": 4.758798253396657e-06,
      "loss": 0.8703,
      "step": 52030
    },
    {
      "epoch": 2.7159053844247003,
      "grad_norm": 4.611330032348633,
      "learning_rate": 4.750100029573961e-06,
      "loss": 0.8213,
      "step": 52040
    },
    {
      "epoch": 2.716427257426905,
      "grad_norm": 5.028833866119385,
      "learning_rate": 4.741401805751266e-06,
      "loss": 0.7846,
      "step": 52050
    },
    {
      "epoch": 2.7169491304291102,
      "grad_norm": 4.775450229644775,
      "learning_rate": 4.732703581928571e-06,
      "loss": 0.8679,
      "step": 52060
    },
    {
      "epoch": 2.717471003431315,
      "grad_norm": 4.441188335418701,
      "learning_rate": 4.724005358105875e-06,
      "loss": 0.7528,
      "step": 52070
    },
    {
      "epoch": 2.71799287643352,
      "grad_norm": 4.243739604949951,
      "learning_rate": 4.7153071342831794e-06,
      "loss": 0.8583,
      "step": 52080
    },
    {
      "epoch": 2.718514749435725,
      "grad_norm": 5.326980113983154,
      "learning_rate": 4.706608910460484e-06,
      "loss": 0.8247,
      "step": 52090
    },
    {
      "epoch": 2.7190366224379297,
      "grad_norm": 4.971470355987549,
      "learning_rate": 4.697910686637789e-06,
      "loss": 0.8165,
      "step": 52100
    },
    {
      "epoch": 2.7195584954401344,
      "grad_norm": 5.170538902282715,
      "learning_rate": 4.6892124628150935e-06,
      "loss": 0.9263,
      "step": 52110
    },
    {
      "epoch": 2.7200803684423396,
      "grad_norm": 4.49040412902832,
      "learning_rate": 4.680514238992398e-06,
      "loss": 0.9795,
      "step": 52120
    },
    {
      "epoch": 2.7206022414445443,
      "grad_norm": 5.0091705322265625,
      "learning_rate": 4.671816015169702e-06,
      "loss": 0.8355,
      "step": 52130
    },
    {
      "epoch": 2.7211241144467495,
      "grad_norm": 4.410256385803223,
      "learning_rate": 4.663117791347007e-06,
      "loss": 0.7865,
      "step": 52140
    },
    {
      "epoch": 2.7216459874489543,
      "grad_norm": 5.318819999694824,
      "learning_rate": 4.654419567524312e-06,
      "loss": 0.8637,
      "step": 52150
    },
    {
      "epoch": 2.722167860451159,
      "grad_norm": 4.686933517456055,
      "learning_rate": 4.645721343701616e-06,
      "loss": 0.9086,
      "step": 52160
    },
    {
      "epoch": 2.722689733453364,
      "grad_norm": 4.612084865570068,
      "learning_rate": 4.637023119878921e-06,
      "loss": 0.8395,
      "step": 52170
    },
    {
      "epoch": 2.723211606455569,
      "grad_norm": 4.640963077545166,
      "learning_rate": 4.628324896056226e-06,
      "loss": 0.8974,
      "step": 52180
    },
    {
      "epoch": 2.723733479457774,
      "grad_norm": 4.376566410064697,
      "learning_rate": 4.61962667223353e-06,
      "loss": 0.8057,
      "step": 52190
    },
    {
      "epoch": 2.724255352459979,
      "grad_norm": 4.222417831420898,
      "learning_rate": 4.610928448410835e-06,
      "loss": 0.8189,
      "step": 52200
    },
    {
      "epoch": 2.7247772254621836,
      "grad_norm": 4.673605442047119,
      "learning_rate": 4.602230224588139e-06,
      "loss": 0.8604,
      "step": 52210
    },
    {
      "epoch": 2.725299098464389,
      "grad_norm": 3.4893202781677246,
      "learning_rate": 4.5935320007654435e-06,
      "loss": 0.7581,
      "step": 52220
    },
    {
      "epoch": 2.7258209714665935,
      "grad_norm": 4.403931617736816,
      "learning_rate": 4.584833776942749e-06,
      "loss": 0.8194,
      "step": 52230
    },
    {
      "epoch": 2.7263428444687987,
      "grad_norm": 4.507935523986816,
      "learning_rate": 4.576135553120053e-06,
      "loss": 0.8588,
      "step": 52240
    },
    {
      "epoch": 2.7268647174710035,
      "grad_norm": 4.604513645172119,
      "learning_rate": 4.5674373292973576e-06,
      "loss": 0.8946,
      "step": 52250
    },
    {
      "epoch": 2.727386590473208,
      "grad_norm": 4.080974102020264,
      "learning_rate": 4.558739105474663e-06,
      "loss": 0.8254,
      "step": 52260
    },
    {
      "epoch": 2.7279084634754134,
      "grad_norm": 4.362738609313965,
      "learning_rate": 4.550040881651966e-06,
      "loss": 0.808,
      "step": 52270
    },
    {
      "epoch": 2.728430336477618,
      "grad_norm": 3.7359368801116943,
      "learning_rate": 4.541342657829272e-06,
      "loss": 0.7261,
      "step": 52280
    },
    {
      "epoch": 2.7289522094798233,
      "grad_norm": 4.138978481292725,
      "learning_rate": 4.532644434006576e-06,
      "loss": 0.8799,
      "step": 52290
    },
    {
      "epoch": 2.729474082482028,
      "grad_norm": 3.8342738151550293,
      "learning_rate": 4.52394621018388e-06,
      "loss": 0.7707,
      "step": 52300
    },
    {
      "epoch": 2.729995955484233,
      "grad_norm": 4.1633501052856445,
      "learning_rate": 4.515247986361186e-06,
      "loss": 0.7986,
      "step": 52310
    },
    {
      "epoch": 2.730517828486438,
      "grad_norm": 5.053062915802002,
      "learning_rate": 4.50654976253849e-06,
      "loss": 0.8992,
      "step": 52320
    },
    {
      "epoch": 2.7310397014886427,
      "grad_norm": 5.365872383117676,
      "learning_rate": 4.4978515387157944e-06,
      "loss": 0.8729,
      "step": 52330
    },
    {
      "epoch": 2.731561574490848,
      "grad_norm": 5.008662700653076,
      "learning_rate": 4.489153314893099e-06,
      "loss": 0.8927,
      "step": 52340
    },
    {
      "epoch": 2.7320834474930527,
      "grad_norm": 4.374517917633057,
      "learning_rate": 4.480455091070403e-06,
      "loss": 0.8302,
      "step": 52350
    },
    {
      "epoch": 2.7326053204952574,
      "grad_norm": 4.150871753692627,
      "learning_rate": 4.4717568672477085e-06,
      "loss": 0.8284,
      "step": 52360
    },
    {
      "epoch": 2.733127193497462,
      "grad_norm": 4.471806526184082,
      "learning_rate": 4.463058643425013e-06,
      "loss": 0.9105,
      "step": 52370
    },
    {
      "epoch": 2.7336490664996673,
      "grad_norm": 4.399199485778809,
      "learning_rate": 4.454360419602317e-06,
      "loss": 0.7995,
      "step": 52380
    },
    {
      "epoch": 2.734170939501872,
      "grad_norm": 4.530108451843262,
      "learning_rate": 4.4456621957796225e-06,
      "loss": 0.7892,
      "step": 52390
    },
    {
      "epoch": 2.7346928125040773,
      "grad_norm": 4.692741394042969,
      "learning_rate": 4.436963971956926e-06,
      "loss": 0.8167,
      "step": 52400
    },
    {
      "epoch": 2.735214685506282,
      "grad_norm": 4.655777454376221,
      "learning_rate": 4.428265748134231e-06,
      "loss": 0.8308,
      "step": 52410
    },
    {
      "epoch": 2.7357365585084867,
      "grad_norm": 4.666452407836914,
      "learning_rate": 4.419567524311536e-06,
      "loss": 0.8666,
      "step": 52420
    },
    {
      "epoch": 2.736258431510692,
      "grad_norm": 4.676060676574707,
      "learning_rate": 4.41086930048884e-06,
      "loss": 0.8068,
      "step": 52430
    },
    {
      "epoch": 2.7367803045128967,
      "grad_norm": 4.304763317108154,
      "learning_rate": 4.402171076666145e-06,
      "loss": 0.8782,
      "step": 52440
    },
    {
      "epoch": 2.737302177515102,
      "grad_norm": 4.120530605316162,
      "learning_rate": 4.39347285284345e-06,
      "loss": 0.8392,
      "step": 52450
    },
    {
      "epoch": 2.7378240505173066,
      "grad_norm": 3.7314538955688477,
      "learning_rate": 4.384774629020754e-06,
      "loss": 0.7103,
      "step": 52460
    },
    {
      "epoch": 2.7383459235195113,
      "grad_norm": 4.233850479125977,
      "learning_rate": 4.3760764051980585e-06,
      "loss": 0.8144,
      "step": 52470
    },
    {
      "epoch": 2.7388677965217165,
      "grad_norm": 4.759392261505127,
      "learning_rate": 4.367378181375363e-06,
      "loss": 0.8356,
      "step": 52480
    },
    {
      "epoch": 2.7393896695239213,
      "grad_norm": 5.062376022338867,
      "learning_rate": 4.358679957552668e-06,
      "loss": 0.9452,
      "step": 52490
    },
    {
      "epoch": 2.7399115425261265,
      "grad_norm": 5.267531871795654,
      "learning_rate": 4.3499817337299725e-06,
      "loss": 0.9058,
      "step": 52500
    },
    {
      "epoch": 2.740433415528331,
      "grad_norm": 5.08240270614624,
      "learning_rate": 4.341283509907277e-06,
      "loss": 0.9161,
      "step": 52510
    },
    {
      "epoch": 2.740955288530536,
      "grad_norm": 4.492188930511475,
      "learning_rate": 4.332585286084582e-06,
      "loss": 0.8538,
      "step": 52520
    },
    {
      "epoch": 2.741477161532741,
      "grad_norm": 5.083630561828613,
      "learning_rate": 4.323887062261886e-06,
      "loss": 0.845,
      "step": 52530
    },
    {
      "epoch": 2.741999034534946,
      "grad_norm": 3.6806416511535645,
      "learning_rate": 4.315188838439191e-06,
      "loss": 0.8011,
      "step": 52540
    },
    {
      "epoch": 2.742520907537151,
      "grad_norm": 4.032493591308594,
      "learning_rate": 4.306490614616495e-06,
      "loss": 0.8537,
      "step": 52550
    },
    {
      "epoch": 2.743042780539356,
      "grad_norm": 4.144374370574951,
      "learning_rate": 4.298662213176069e-06,
      "loss": 0.7944,
      "step": 52560
    },
    {
      "epoch": 2.7435646535415605,
      "grad_norm": 5.709163665771484,
      "learning_rate": 4.289963989353375e-06,
      "loss": 0.8976,
      "step": 52570
    },
    {
      "epoch": 2.7440865265437657,
      "grad_norm": 5.421824932098389,
      "learning_rate": 4.281265765530679e-06,
      "loss": 0.8994,
      "step": 52580
    },
    {
      "epoch": 2.7446083995459705,
      "grad_norm": 5.056986331939697,
      "learning_rate": 4.2725675417079835e-06,
      "loss": 0.8512,
      "step": 52590
    },
    {
      "epoch": 2.7451302725481757,
      "grad_norm": 4.777195453643799,
      "learning_rate": 4.263869317885288e-06,
      "loss": 0.7315,
      "step": 52600
    },
    {
      "epoch": 2.7456521455503804,
      "grad_norm": 4.8564276695251465,
      "learning_rate": 4.255171094062592e-06,
      "loss": 0.9186,
      "step": 52610
    },
    {
      "epoch": 2.746174018552585,
      "grad_norm": 5.0725789070129395,
      "learning_rate": 4.2464728702398975e-06,
      "loss": 0.7998,
      "step": 52620
    },
    {
      "epoch": 2.74669589155479,
      "grad_norm": 4.577502727508545,
      "learning_rate": 4.237774646417202e-06,
      "loss": 0.7643,
      "step": 52630
    },
    {
      "epoch": 2.747217764556995,
      "grad_norm": 4.4365010261535645,
      "learning_rate": 4.229076422594506e-06,
      "loss": 0.7809,
      "step": 52640
    },
    {
      "epoch": 2.7477396375592,
      "grad_norm": 4.5918169021606445,
      "learning_rate": 4.2203781987718115e-06,
      "loss": 0.9458,
      "step": 52650
    },
    {
      "epoch": 2.748261510561405,
      "grad_norm": 3.702176809310913,
      "learning_rate": 4.211679974949116e-06,
      "loss": 0.8289,
      "step": 52660
    },
    {
      "epoch": 2.7487833835636097,
      "grad_norm": 5.303500175476074,
      "learning_rate": 4.20298175112642e-06,
      "loss": 0.8301,
      "step": 52670
    },
    {
      "epoch": 2.7493052565658145,
      "grad_norm": 4.806183338165283,
      "learning_rate": 4.194283527303725e-06,
      "loss": 0.8075,
      "step": 52680
    },
    {
      "epoch": 2.7498271295680197,
      "grad_norm": 4.866878509521484,
      "learning_rate": 4.185585303481029e-06,
      "loss": 0.8071,
      "step": 52690
    },
    {
      "epoch": 2.7503490025702244,
      "grad_norm": 4.3777875900268555,
      "learning_rate": 4.176887079658334e-06,
      "loss": 0.8116,
      "step": 52700
    },
    {
      "epoch": 2.7508708755724296,
      "grad_norm": 4.8127970695495605,
      "learning_rate": 4.168188855835639e-06,
      "loss": 0.8618,
      "step": 52710
    },
    {
      "epoch": 2.7513927485746343,
      "grad_norm": 4.4751763343811035,
      "learning_rate": 4.159490632012943e-06,
      "loss": 0.8859,
      "step": 52720
    },
    {
      "epoch": 2.751914621576839,
      "grad_norm": 5.039577960968018,
      "learning_rate": 4.1507924081902475e-06,
      "loss": 0.861,
      "step": 52730
    },
    {
      "epoch": 2.7524364945790443,
      "grad_norm": 3.5089266300201416,
      "learning_rate": 4.142094184367552e-06,
      "loss": 0.7884,
      "step": 52740
    },
    {
      "epoch": 2.752958367581249,
      "grad_norm": 4.076320171356201,
      "learning_rate": 4.133395960544857e-06,
      "loss": 0.7985,
      "step": 52750
    },
    {
      "epoch": 2.753480240583454,
      "grad_norm": 4.821890830993652,
      "learning_rate": 4.1246977367221616e-06,
      "loss": 0.7997,
      "step": 52760
    },
    {
      "epoch": 2.754002113585659,
      "grad_norm": 4.2773051261901855,
      "learning_rate": 4.115999512899466e-06,
      "loss": 0.889,
      "step": 52770
    },
    {
      "epoch": 2.7545239865878637,
      "grad_norm": 4.792583465576172,
      "learning_rate": 4.107301289076771e-06,
      "loss": 0.7809,
      "step": 52780
    },
    {
      "epoch": 2.755045859590069,
      "grad_norm": 5.432275295257568,
      "learning_rate": 4.098603065254076e-06,
      "loss": 0.7676,
      "step": 52790
    },
    {
      "epoch": 2.7555677325922736,
      "grad_norm": 5.834824562072754,
      "learning_rate": 4.08990484143138e-06,
      "loss": 0.8124,
      "step": 52800
    },
    {
      "epoch": 2.756089605594479,
      "grad_norm": 4.955850601196289,
      "learning_rate": 4.081206617608684e-06,
      "loss": 0.8984,
      "step": 52810
    },
    {
      "epoch": 2.7566114785966835,
      "grad_norm": 4.918042182922363,
      "learning_rate": 4.072508393785989e-06,
      "loss": 0.8806,
      "step": 52820
    },
    {
      "epoch": 2.7571333515988883,
      "grad_norm": 4.970832347869873,
      "learning_rate": 4.063810169963294e-06,
      "loss": 0.8489,
      "step": 52830
    },
    {
      "epoch": 2.7576552246010935,
      "grad_norm": 4.789533615112305,
      "learning_rate": 4.0551119461405984e-06,
      "loss": 0.8113,
      "step": 52840
    },
    {
      "epoch": 2.758177097603298,
      "grad_norm": 4.575680255889893,
      "learning_rate": 4.046413722317903e-06,
      "loss": 0.8614,
      "step": 52850
    },
    {
      "epoch": 2.7586989706055034,
      "grad_norm": 4.974977970123291,
      "learning_rate": 4.037715498495207e-06,
      "loss": 0.7639,
      "step": 52860
    },
    {
      "epoch": 2.759220843607708,
      "grad_norm": 5.242154598236084,
      "learning_rate": 4.029017274672512e-06,
      "loss": 0.8621,
      "step": 52870
    },
    {
      "epoch": 2.759742716609913,
      "grad_norm": 5.450020790100098,
      "learning_rate": 4.020319050849817e-06,
      "loss": 0.9374,
      "step": 52880
    },
    {
      "epoch": 2.7602645896121176,
      "grad_norm": 4.206097602844238,
      "learning_rate": 4.011620827027121e-06,
      "loss": 0.8164,
      "step": 52890
    },
    {
      "epoch": 2.760786462614323,
      "grad_norm": 4.257573127746582,
      "learning_rate": 4.002922603204426e-06,
      "loss": 0.8509,
      "step": 52900
    },
    {
      "epoch": 2.7613083356165276,
      "grad_norm": 4.654383659362793,
      "learning_rate": 3.994224379381731e-06,
      "loss": 0.8197,
      "step": 52910
    },
    {
      "epoch": 2.7618302086187327,
      "grad_norm": 5.431704998016357,
      "learning_rate": 3.985526155559035e-06,
      "loss": 0.8546,
      "step": 52920
    },
    {
      "epoch": 2.7623520816209375,
      "grad_norm": 5.767782211303711,
      "learning_rate": 3.97682793173634e-06,
      "loss": 0.922,
      "step": 52930
    },
    {
      "epoch": 2.7628739546231422,
      "grad_norm": 4.629824638366699,
      "learning_rate": 3.968129707913644e-06,
      "loss": 0.825,
      "step": 52940
    },
    {
      "epoch": 2.7633958276253474,
      "grad_norm": 4.48004150390625,
      "learning_rate": 3.9594314840909485e-06,
      "loss": 0.7768,
      "step": 52950
    },
    {
      "epoch": 2.763917700627552,
      "grad_norm": 4.853242874145508,
      "learning_rate": 3.950733260268254e-06,
      "loss": 0.8322,
      "step": 52960
    },
    {
      "epoch": 2.7644395736297573,
      "grad_norm": 4.743142604827881,
      "learning_rate": 3.942035036445558e-06,
      "loss": 0.8639,
      "step": 52970
    },
    {
      "epoch": 2.764961446631962,
      "grad_norm": 4.676274299621582,
      "learning_rate": 3.9333368126228625e-06,
      "loss": 0.8295,
      "step": 52980
    },
    {
      "epoch": 2.765483319634167,
      "grad_norm": 4.403550624847412,
      "learning_rate": 3.924638588800167e-06,
      "loss": 0.8255,
      "step": 52990
    },
    {
      "epoch": 2.766005192636372,
      "grad_norm": 4.7364373207092285,
      "learning_rate": 3.915940364977472e-06,
      "loss": 0.8338,
      "step": 53000
    },
    {
      "epoch": 2.7665270656385768,
      "grad_norm": 4.80698823928833,
      "learning_rate": 3.9072421411547766e-06,
      "loss": 0.8483,
      "step": 53010
    },
    {
      "epoch": 2.767048938640782,
      "grad_norm": 5.01146125793457,
      "learning_rate": 3.898543917332081e-06,
      "loss": 0.7706,
      "step": 53020
    },
    {
      "epoch": 2.7675708116429867,
      "grad_norm": 5.409875392913818,
      "learning_rate": 3.889845693509385e-06,
      "loss": 0.9109,
      "step": 53030
    },
    {
      "epoch": 2.7680926846451914,
      "grad_norm": 4.888004302978516,
      "learning_rate": 3.881147469686691e-06,
      "loss": 0.8768,
      "step": 53040
    },
    {
      "epoch": 2.7686145576473966,
      "grad_norm": 4.960576057434082,
      "learning_rate": 3.872449245863995e-06,
      "loss": 0.8972,
      "step": 53050
    },
    {
      "epoch": 2.7691364306496014,
      "grad_norm": 4.362586498260498,
      "learning_rate": 3.863751022041299e-06,
      "loss": 0.836,
      "step": 53060
    },
    {
      "epoch": 2.7696583036518065,
      "grad_norm": 4.7644219398498535,
      "learning_rate": 3.855052798218604e-06,
      "loss": 0.8225,
      "step": 53070
    },
    {
      "epoch": 2.7701801766540113,
      "grad_norm": 4.668942451477051,
      "learning_rate": 3.846354574395908e-06,
      "loss": 0.8631,
      "step": 53080
    },
    {
      "epoch": 2.770702049656216,
      "grad_norm": 4.752531051635742,
      "learning_rate": 3.837656350573213e-06,
      "loss": 0.7786,
      "step": 53090
    },
    {
      "epoch": 2.771223922658421,
      "grad_norm": 3.984245777130127,
      "learning_rate": 3.828958126750518e-06,
      "loss": 0.8577,
      "step": 53100
    },
    {
      "epoch": 2.771745795660626,
      "grad_norm": 5.08901309967041,
      "learning_rate": 3.820259902927822e-06,
      "loss": 0.9071,
      "step": 53110
    },
    {
      "epoch": 2.772267668662831,
      "grad_norm": 4.688510417938232,
      "learning_rate": 3.8115616791051266e-06,
      "loss": 0.7587,
      "step": 53120
    },
    {
      "epoch": 2.772789541665036,
      "grad_norm": 4.360070705413818,
      "learning_rate": 3.8028634552824314e-06,
      "loss": 0.8817,
      "step": 53130
    },
    {
      "epoch": 2.7733114146672406,
      "grad_norm": 4.185540199279785,
      "learning_rate": 3.7941652314597362e-06,
      "loss": 0.8536,
      "step": 53140
    },
    {
      "epoch": 2.7738332876694454,
      "grad_norm": 5.110836505889893,
      "learning_rate": 3.7854670076370406e-06,
      "loss": 0.9579,
      "step": 53150
    },
    {
      "epoch": 2.7743551606716506,
      "grad_norm": 4.836660861968994,
      "learning_rate": 3.7767687838143455e-06,
      "loss": 0.847,
      "step": 53160
    },
    {
      "epoch": 2.7748770336738557,
      "grad_norm": 4.658722877502441,
      "learning_rate": 3.7680705599916503e-06,
      "loss": 0.898,
      "step": 53170
    },
    {
      "epoch": 2.7753989066760605,
      "grad_norm": 4.564839839935303,
      "learning_rate": 3.7593723361689543e-06,
      "loss": 0.8061,
      "step": 53180
    },
    {
      "epoch": 2.7759207796782652,
      "grad_norm": 5.360745429992676,
      "learning_rate": 3.750674112346259e-06,
      "loss": 0.8282,
      "step": 53190
    },
    {
      "epoch": 2.77644265268047,
      "grad_norm": 4.806331634521484,
      "learning_rate": 3.7419758885235635e-06,
      "loss": 0.9025,
      "step": 53200
    },
    {
      "epoch": 2.776964525682675,
      "grad_norm": 4.727933406829834,
      "learning_rate": 3.7332776647008683e-06,
      "loss": 0.8841,
      "step": 53210
    },
    {
      "epoch": 2.77748639868488,
      "grad_norm": 4.784524440765381,
      "learning_rate": 3.724579440878173e-06,
      "loss": 0.8413,
      "step": 53220
    },
    {
      "epoch": 2.778008271687085,
      "grad_norm": 3.830685615539551,
      "learning_rate": 3.715881217055477e-06,
      "loss": 0.8932,
      "step": 53230
    },
    {
      "epoch": 2.77853014468929,
      "grad_norm": 4.770347595214844,
      "learning_rate": 3.7071829932327823e-06,
      "loss": 0.8826,
      "step": 53240
    },
    {
      "epoch": 2.7790520176914946,
      "grad_norm": 5.679720401763916,
      "learning_rate": 3.6984847694100863e-06,
      "loss": 0.8627,
      "step": 53250
    },
    {
      "epoch": 2.7795738906936998,
      "grad_norm": 4.659633636474609,
      "learning_rate": 3.689786545587391e-06,
      "loss": 0.971,
      "step": 53260
    },
    {
      "epoch": 2.7800957636959045,
      "grad_norm": 4.767498016357422,
      "learning_rate": 3.681088321764696e-06,
      "loss": 0.8892,
      "step": 53270
    },
    {
      "epoch": 2.7806176366981097,
      "grad_norm": 4.839452266693115,
      "learning_rate": 3.6723900979420003e-06,
      "loss": 0.9541,
      "step": 53280
    },
    {
      "epoch": 2.7811395097003144,
      "grad_norm": 4.594598770141602,
      "learning_rate": 3.663691874119305e-06,
      "loss": 0.8754,
      "step": 53290
    },
    {
      "epoch": 2.781661382702519,
      "grad_norm": 5.373115062713623,
      "learning_rate": 3.65499365029661e-06,
      "loss": 0.9405,
      "step": 53300
    },
    {
      "epoch": 2.7821832557047244,
      "grad_norm": 5.104553699493408,
      "learning_rate": 3.646295426473914e-06,
      "loss": 0.878,
      "step": 53310
    },
    {
      "epoch": 2.782705128706929,
      "grad_norm": 4.972804546356201,
      "learning_rate": 3.6375972026512188e-06,
      "loss": 0.8075,
      "step": 53320
    },
    {
      "epoch": 2.7832270017091343,
      "grad_norm": 4.594744682312012,
      "learning_rate": 3.628898978828523e-06,
      "loss": 0.8505,
      "step": 53330
    },
    {
      "epoch": 2.783748874711339,
      "grad_norm": 4.821499824523926,
      "learning_rate": 3.620200755005828e-06,
      "loss": 0.8527,
      "step": 53340
    },
    {
      "epoch": 2.7842707477135438,
      "grad_norm": 5.241959571838379,
      "learning_rate": 3.611502531183133e-06,
      "loss": 0.8722,
      "step": 53350
    },
    {
      "epoch": 2.784792620715749,
      "grad_norm": 5.413724422454834,
      "learning_rate": 3.602804307360437e-06,
      "loss": 0.8627,
      "step": 53360
    },
    {
      "epoch": 2.7853144937179537,
      "grad_norm": 5.350245952606201,
      "learning_rate": 3.594106083537742e-06,
      "loss": 0.8072,
      "step": 53370
    },
    {
      "epoch": 2.785836366720159,
      "grad_norm": 5.112687110900879,
      "learning_rate": 3.585407859715046e-06,
      "loss": 0.8332,
      "step": 53380
    },
    {
      "epoch": 2.7863582397223636,
      "grad_norm": 4.600361347198486,
      "learning_rate": 3.576709635892351e-06,
      "loss": 0.7452,
      "step": 53390
    },
    {
      "epoch": 2.7868801127245684,
      "grad_norm": 5.0388970375061035,
      "learning_rate": 3.5680114120696556e-06,
      "loss": 0.846,
      "step": 53400
    },
    {
      "epoch": 2.787401985726773,
      "grad_norm": 4.75618314743042,
      "learning_rate": 3.55931318824696e-06,
      "loss": 0.8761,
      "step": 53410
    },
    {
      "epoch": 2.7879238587289783,
      "grad_norm": 4.076675891876221,
      "learning_rate": 3.550614964424265e-06,
      "loss": 0.8239,
      "step": 53420
    },
    {
      "epoch": 2.7884457317311835,
      "grad_norm": 4.828895092010498,
      "learning_rate": 3.5419167406015697e-06,
      "loss": 0.7584,
      "step": 53430
    },
    {
      "epoch": 2.7889676047333882,
      "grad_norm": 4.926983833312988,
      "learning_rate": 3.5332185167788736e-06,
      "loss": 0.7947,
      "step": 53440
    },
    {
      "epoch": 2.789489477735593,
      "grad_norm": 4.075766086578369,
      "learning_rate": 3.524520292956179e-06,
      "loss": 0.769,
      "step": 53450
    },
    {
      "epoch": 2.7900113507377977,
      "grad_norm": 5.092861175537109,
      "learning_rate": 3.515822069133483e-06,
      "loss": 0.8492,
      "step": 53460
    },
    {
      "epoch": 2.790533223740003,
      "grad_norm": 4.5864338874816895,
      "learning_rate": 3.5071238453107877e-06,
      "loss": 0.769,
      "step": 53470
    },
    {
      "epoch": 2.7910550967422076,
      "grad_norm": 4.091537952423096,
      "learning_rate": 3.4984256214880925e-06,
      "loss": 0.9051,
      "step": 53480
    },
    {
      "epoch": 2.791576969744413,
      "grad_norm": 4.747897624969482,
      "learning_rate": 3.489727397665397e-06,
      "loss": 0.835,
      "step": 53490
    },
    {
      "epoch": 2.7920988427466176,
      "grad_norm": 5.448486328125,
      "learning_rate": 3.4810291738427017e-06,
      "loss": 0.7781,
      "step": 53500
    },
    {
      "epoch": 2.7926207157488223,
      "grad_norm": 4.362408638000488,
      "learning_rate": 3.4723309500200057e-06,
      "loss": 0.8581,
      "step": 53510
    },
    {
      "epoch": 2.7931425887510275,
      "grad_norm": 5.064918041229248,
      "learning_rate": 3.4636327261973105e-06,
      "loss": 0.8238,
      "step": 53520
    },
    {
      "epoch": 2.7936644617532322,
      "grad_norm": 4.546888828277588,
      "learning_rate": 3.4549345023746153e-06,
      "loss": 0.952,
      "step": 53530
    },
    {
      "epoch": 2.7941863347554374,
      "grad_norm": 5.5368499755859375,
      "learning_rate": 3.4462362785519197e-06,
      "loss": 0.7787,
      "step": 53540
    },
    {
      "epoch": 2.794708207757642,
      "grad_norm": 5.7111406326293945,
      "learning_rate": 3.4375380547292245e-06,
      "loss": 0.8503,
      "step": 53550
    },
    {
      "epoch": 2.795230080759847,
      "grad_norm": 5.212932109832764,
      "learning_rate": 3.4288398309065293e-06,
      "loss": 0.7992,
      "step": 53560
    },
    {
      "epoch": 2.795751953762052,
      "grad_norm": 4.7331132888793945,
      "learning_rate": 3.4201416070838333e-06,
      "loss": 0.7889,
      "step": 53570
    },
    {
      "epoch": 2.796273826764257,
      "grad_norm": 4.801006317138672,
      "learning_rate": 3.4114433832611386e-06,
      "loss": 0.819,
      "step": 53580
    },
    {
      "epoch": 2.796795699766462,
      "grad_norm": 5.0145792961120605,
      "learning_rate": 3.4027451594384425e-06,
      "loss": 0.9131,
      "step": 53590
    },
    {
      "epoch": 2.7973175727686668,
      "grad_norm": 4.864352703094482,
      "learning_rate": 3.3940469356157474e-06,
      "loss": 0.8306,
      "step": 53600
    },
    {
      "epoch": 2.7978394457708715,
      "grad_norm": 5.227617263793945,
      "learning_rate": 3.385348711793052e-06,
      "loss": 0.888,
      "step": 53610
    },
    {
      "epoch": 2.7983613187730767,
      "grad_norm": 4.442567348480225,
      "learning_rate": 3.3766504879703566e-06,
      "loss": 0.9,
      "step": 53620
    },
    {
      "epoch": 2.7988831917752814,
      "grad_norm": 5.589305877685547,
      "learning_rate": 3.3679522641476614e-06,
      "loss": 0.858,
      "step": 53630
    },
    {
      "epoch": 2.7994050647774866,
      "grad_norm": 5.315630912780762,
      "learning_rate": 3.3592540403249654e-06,
      "loss": 0.8589,
      "step": 53640
    },
    {
      "epoch": 2.7999269377796914,
      "grad_norm": 4.04055118560791,
      "learning_rate": 3.35055581650227e-06,
      "loss": 0.717,
      "step": 53650
    },
    {
      "epoch": 2.800448810781896,
      "grad_norm": 4.406097412109375,
      "learning_rate": 3.341857592679575e-06,
      "loss": 0.819,
      "step": 53660
    },
    {
      "epoch": 2.8009706837841013,
      "grad_norm": 4.8203840255737305,
      "learning_rate": 3.3331593688568794e-06,
      "loss": 0.8908,
      "step": 53670
    },
    {
      "epoch": 2.801492556786306,
      "grad_norm": 4.772217750549316,
      "learning_rate": 3.3244611450341842e-06,
      "loss": 0.9123,
      "step": 53680
    },
    {
      "epoch": 2.8020144297885112,
      "grad_norm": 4.740432262420654,
      "learning_rate": 3.315762921211489e-06,
      "loss": 0.9007,
      "step": 53690
    },
    {
      "epoch": 2.802536302790716,
      "grad_norm": 4.745832920074463,
      "learning_rate": 3.3070646973887934e-06,
      "loss": 0.8328,
      "step": 53700
    },
    {
      "epoch": 2.8030581757929207,
      "grad_norm": 4.599094390869141,
      "learning_rate": 3.2983664735660982e-06,
      "loss": 0.8589,
      "step": 53710
    },
    {
      "epoch": 2.8035800487951255,
      "grad_norm": 4.72005033493042,
      "learning_rate": 3.2896682497434022e-06,
      "loss": 0.9143,
      "step": 53720
    },
    {
      "epoch": 2.8041019217973306,
      "grad_norm": 4.519838809967041,
      "learning_rate": 3.280970025920707e-06,
      "loss": 0.7785,
      "step": 53730
    },
    {
      "epoch": 2.8046237947995354,
      "grad_norm": 4.2518486976623535,
      "learning_rate": 3.272271802098012e-06,
      "loss": 0.8654,
      "step": 53740
    },
    {
      "epoch": 2.8051456678017406,
      "grad_norm": 4.523396968841553,
      "learning_rate": 3.2635735782753163e-06,
      "loss": 0.7595,
      "step": 53750
    },
    {
      "epoch": 2.8056675408039453,
      "grad_norm": 4.467085838317871,
      "learning_rate": 3.254875354452621e-06,
      "loss": 0.7972,
      "step": 53760
    },
    {
      "epoch": 2.80618941380615,
      "grad_norm": 4.851487636566162,
      "learning_rate": 3.246177130629925e-06,
      "loss": 0.7419,
      "step": 53770
    },
    {
      "epoch": 2.8067112868083552,
      "grad_norm": 5.492932319641113,
      "learning_rate": 3.23747890680723e-06,
      "loss": 0.83,
      "step": 53780
    },
    {
      "epoch": 2.80723315981056,
      "grad_norm": 4.833293437957764,
      "learning_rate": 3.2287806829845347e-06,
      "loss": 0.7686,
      "step": 53790
    },
    {
      "epoch": 2.807755032812765,
      "grad_norm": 4.534058094024658,
      "learning_rate": 3.220082459161839e-06,
      "loss": 0.8552,
      "step": 53800
    },
    {
      "epoch": 2.80827690581497,
      "grad_norm": 4.969235420227051,
      "learning_rate": 3.211384235339144e-06,
      "loss": 0.9091,
      "step": 53810
    },
    {
      "epoch": 2.8087987788171747,
      "grad_norm": 5.126391887664795,
      "learning_rate": 3.2026860115164487e-06,
      "loss": 0.7971,
      "step": 53820
    },
    {
      "epoch": 2.80932065181938,
      "grad_norm": 5.237850666046143,
      "learning_rate": 3.193987787693753e-06,
      "loss": 0.8729,
      "step": 53830
    },
    {
      "epoch": 2.8098425248215846,
      "grad_norm": 4.645598411560059,
      "learning_rate": 3.185289563871058e-06,
      "loss": 0.8017,
      "step": 53840
    },
    {
      "epoch": 2.8103643978237898,
      "grad_norm": 4.648401260375977,
      "learning_rate": 3.176591340048362e-06,
      "loss": 0.8895,
      "step": 53850
    },
    {
      "epoch": 2.8108862708259945,
      "grad_norm": 4.602783203125,
      "learning_rate": 3.1678931162256667e-06,
      "loss": 0.7151,
      "step": 53860
    },
    {
      "epoch": 2.8114081438281993,
      "grad_norm": 4.961795330047607,
      "learning_rate": 3.1591948924029715e-06,
      "loss": 0.806,
      "step": 53870
    },
    {
      "epoch": 2.8119300168304044,
      "grad_norm": 4.9539055824279785,
      "learning_rate": 3.150496668580276e-06,
      "loss": 0.7975,
      "step": 53880
    },
    {
      "epoch": 2.812451889832609,
      "grad_norm": 4.503081321716309,
      "learning_rate": 3.1417984447575808e-06,
      "loss": 0.82,
      "step": 53890
    },
    {
      "epoch": 2.8129737628348144,
      "grad_norm": 5.595965385437012,
      "learning_rate": 3.1331002209348856e-06,
      "loss": 0.8518,
      "step": 53900
    },
    {
      "epoch": 2.813495635837019,
      "grad_norm": 5.669341087341309,
      "learning_rate": 3.1244019971121896e-06,
      "loss": 0.9569,
      "step": 53910
    },
    {
      "epoch": 2.814017508839224,
      "grad_norm": 4.968335151672363,
      "learning_rate": 3.1157037732894944e-06,
      "loss": 0.8414,
      "step": 53920
    },
    {
      "epoch": 2.814539381841429,
      "grad_norm": 4.866791725158691,
      "learning_rate": 3.107005549466799e-06,
      "loss": 0.8849,
      "step": 53930
    },
    {
      "epoch": 2.815061254843634,
      "grad_norm": 4.974632740020752,
      "learning_rate": 3.0983073256441036e-06,
      "loss": 0.8298,
      "step": 53940
    },
    {
      "epoch": 2.815583127845839,
      "grad_norm": 5.186878204345703,
      "learning_rate": 3.089609101821408e-06,
      "loss": 0.7595,
      "step": 53950
    },
    {
      "epoch": 2.8161050008480437,
      "grad_norm": 4.417918682098389,
      "learning_rate": 3.080910877998713e-06,
      "loss": 0.8073,
      "step": 53960
    },
    {
      "epoch": 2.8166268738502485,
      "grad_norm": 4.625093460083008,
      "learning_rate": 3.0722126541760176e-06,
      "loss": 0.8544,
      "step": 53970
    },
    {
      "epoch": 2.817148746852453,
      "grad_norm": 4.81516695022583,
      "learning_rate": 3.063514430353322e-06,
      "loss": 0.8049,
      "step": 53980
    },
    {
      "epoch": 2.8176706198546584,
      "grad_norm": 4.0535125732421875,
      "learning_rate": 3.0548162065306264e-06,
      "loss": 0.7194,
      "step": 53990
    },
    {
      "epoch": 2.818192492856863,
      "grad_norm": 4.269594669342041,
      "learning_rate": 3.0461179827079312e-06,
      "loss": 0.8411,
      "step": 54000
    },
    {
      "epoch": 2.8187143658590683,
      "grad_norm": 5.172957420349121,
      "learning_rate": 3.037419758885236e-06,
      "loss": 0.8317,
      "step": 54010
    },
    {
      "epoch": 2.819236238861273,
      "grad_norm": 4.998737335205078,
      "learning_rate": 3.0287215350625405e-06,
      "loss": 0.8258,
      "step": 54020
    },
    {
      "epoch": 2.819758111863478,
      "grad_norm": 4.5267558097839355,
      "learning_rate": 3.020023311239845e-06,
      "loss": 0.8385,
      "step": 54030
    },
    {
      "epoch": 2.820279984865683,
      "grad_norm": 5.061676502227783,
      "learning_rate": 3.0113250874171497e-06,
      "loss": 0.8492,
      "step": 54040
    },
    {
      "epoch": 2.8208018578678877,
      "grad_norm": 4.248621940612793,
      "learning_rate": 3.002626863594454e-06,
      "loss": 0.8636,
      "step": 54050
    },
    {
      "epoch": 2.821323730870093,
      "grad_norm": 4.8062920570373535,
      "learning_rate": 2.993928639771759e-06,
      "loss": 0.7548,
      "step": 54060
    },
    {
      "epoch": 2.8218456038722977,
      "grad_norm": 4.961895942687988,
      "learning_rate": 2.9852304159490633e-06,
      "loss": 0.8472,
      "step": 54070
    },
    {
      "epoch": 2.8223674768745024,
      "grad_norm": 5.442155838012695,
      "learning_rate": 2.9765321921263677e-06,
      "loss": 0.9443,
      "step": 54080
    },
    {
      "epoch": 2.8228893498767076,
      "grad_norm": 4.495909690856934,
      "learning_rate": 2.9678339683036725e-06,
      "loss": 0.8923,
      "step": 54090
    },
    {
      "epoch": 2.8234112228789123,
      "grad_norm": 4.804051876068115,
      "learning_rate": 2.9591357444809773e-06,
      "loss": 0.8133,
      "step": 54100
    },
    {
      "epoch": 2.8239330958811175,
      "grad_norm": 4.076406955718994,
      "learning_rate": 2.9504375206582817e-06,
      "loss": 0.8557,
      "step": 54110
    },
    {
      "epoch": 2.8244549688833223,
      "grad_norm": 5.088968753814697,
      "learning_rate": 2.941739296835586e-06,
      "loss": 0.9129,
      "step": 54120
    },
    {
      "epoch": 2.824976841885527,
      "grad_norm": 4.634774684906006,
      "learning_rate": 2.933041073012891e-06,
      "loss": 0.7484,
      "step": 54130
    },
    {
      "epoch": 2.825498714887732,
      "grad_norm": 4.236855983734131,
      "learning_rate": 2.9243428491901957e-06,
      "loss": 0.805,
      "step": 54140
    },
    {
      "epoch": 2.826020587889937,
      "grad_norm": 4.158115863800049,
      "learning_rate": 2.9156446253675e-06,
      "loss": 0.7583,
      "step": 54150
    },
    {
      "epoch": 2.826542460892142,
      "grad_norm": 4.605752468109131,
      "learning_rate": 2.9069464015448045e-06,
      "loss": 0.8056,
      "step": 54160
    },
    {
      "epoch": 2.827064333894347,
      "grad_norm": 5.172940731048584,
      "learning_rate": 2.8982481777221094e-06,
      "loss": 0.8819,
      "step": 54170
    },
    {
      "epoch": 2.8275862068965516,
      "grad_norm": 4.394381046295166,
      "learning_rate": 2.889549953899414e-06,
      "loss": 0.8137,
      "step": 54180
    },
    {
      "epoch": 2.828108079898757,
      "grad_norm": 4.572744369506836,
      "learning_rate": 2.8808517300767186e-06,
      "loss": 0.8711,
      "step": 54190
    },
    {
      "epoch": 2.8286299529009615,
      "grad_norm": 5.994036674499512,
      "learning_rate": 2.872153506254023e-06,
      "loss": 0.9741,
      "step": 54200
    },
    {
      "epoch": 2.8291518259031667,
      "grad_norm": 4.419554233551025,
      "learning_rate": 2.863455282431328e-06,
      "loss": 0.7789,
      "step": 54210
    },
    {
      "epoch": 2.8296736989053715,
      "grad_norm": 4.624835014343262,
      "learning_rate": 2.854757058608632e-06,
      "loss": 0.9237,
      "step": 54220
    },
    {
      "epoch": 2.830195571907576,
      "grad_norm": 5.141058921813965,
      "learning_rate": 2.846058834785937e-06,
      "loss": 0.9055,
      "step": 54230
    },
    {
      "epoch": 2.830717444909781,
      "grad_norm": 6.002162456512451,
      "learning_rate": 2.8373606109632414e-06,
      "loss": 0.836,
      "step": 54240
    },
    {
      "epoch": 2.831239317911986,
      "grad_norm": 4.5323076248168945,
      "learning_rate": 2.828662387140546e-06,
      "loss": 0.8583,
      "step": 54250
    },
    {
      "epoch": 2.831761190914191,
      "grad_norm": 4.86954402923584,
      "learning_rate": 2.8199641633178506e-06,
      "loss": 0.8757,
      "step": 54260
    },
    {
      "epoch": 2.832283063916396,
      "grad_norm": 5.109849452972412,
      "learning_rate": 2.8112659394951554e-06,
      "loss": 0.8804,
      "step": 54270
    },
    {
      "epoch": 2.832804936918601,
      "grad_norm": 4.655041217803955,
      "learning_rate": 2.80256771567246e-06,
      "loss": 0.8848,
      "step": 54280
    },
    {
      "epoch": 2.8333268099208055,
      "grad_norm": 5.237457275390625,
      "learning_rate": 2.7938694918497642e-06,
      "loss": 0.845,
      "step": 54290
    },
    {
      "epoch": 2.8338486829230107,
      "grad_norm": 4.745026111602783,
      "learning_rate": 2.785171268027069e-06,
      "loss": 0.8481,
      "step": 54300
    },
    {
      "epoch": 2.8343705559252155,
      "grad_norm": 4.155112266540527,
      "learning_rate": 2.776473044204374e-06,
      "loss": 0.8368,
      "step": 54310
    },
    {
      "epoch": 2.8348924289274207,
      "grad_norm": 4.620169639587402,
      "learning_rate": 2.7677748203816783e-06,
      "loss": 0.8066,
      "step": 54320
    },
    {
      "epoch": 2.8354143019296254,
      "grad_norm": 4.880812644958496,
      "learning_rate": 2.7590765965589827e-06,
      "loss": 0.8771,
      "step": 54330
    },
    {
      "epoch": 2.83593617493183,
      "grad_norm": 5.29133415222168,
      "learning_rate": 2.7503783727362875e-06,
      "loss": 0.8482,
      "step": 54340
    },
    {
      "epoch": 2.8364580479340353,
      "grad_norm": 5.312952518463135,
      "learning_rate": 2.741680148913592e-06,
      "loss": 0.8078,
      "step": 54350
    },
    {
      "epoch": 2.83697992093624,
      "grad_norm": 4.8305487632751465,
      "learning_rate": 2.7329819250908967e-06,
      "loss": 0.8299,
      "step": 54360
    },
    {
      "epoch": 2.8375017939384453,
      "grad_norm": 4.465566635131836,
      "learning_rate": 2.724283701268201e-06,
      "loss": 0.7136,
      "step": 54370
    },
    {
      "epoch": 2.83802366694065,
      "grad_norm": 4.367676258087158,
      "learning_rate": 2.715585477445506e-06,
      "loss": 0.7925,
      "step": 54380
    },
    {
      "epoch": 2.8385455399428547,
      "grad_norm": 5.179498672485352,
      "learning_rate": 2.7068872536228103e-06,
      "loss": 0.8624,
      "step": 54390
    },
    {
      "epoch": 2.83906741294506,
      "grad_norm": 4.193542957305908,
      "learning_rate": 2.698189029800115e-06,
      "loss": 0.7896,
      "step": 54400
    },
    {
      "epoch": 2.8395892859472647,
      "grad_norm": 5.000940799713135,
      "learning_rate": 2.6894908059774195e-06,
      "loss": 0.8728,
      "step": 54410
    },
    {
      "epoch": 2.84011115894947,
      "grad_norm": 4.854762554168701,
      "learning_rate": 2.680792582154724e-06,
      "loss": 0.8265,
      "step": 54420
    },
    {
      "epoch": 2.8406330319516746,
      "grad_norm": 3.835188627243042,
      "learning_rate": 2.6720943583320287e-06,
      "loss": 0.8361,
      "step": 54430
    },
    {
      "epoch": 2.8411549049538793,
      "grad_norm": 5.051098346710205,
      "learning_rate": 2.6633961345093336e-06,
      "loss": 0.8821,
      "step": 54440
    },
    {
      "epoch": 2.8416767779560845,
      "grad_norm": 5.237095355987549,
      "learning_rate": 2.654697910686638e-06,
      "loss": 0.7847,
      "step": 54450
    },
    {
      "epoch": 2.8421986509582893,
      "grad_norm": 4.554717540740967,
      "learning_rate": 2.6459996868639423e-06,
      "loss": 0.8038,
      "step": 54460
    },
    {
      "epoch": 2.8427205239604945,
      "grad_norm": 4.641448974609375,
      "learning_rate": 2.637301463041247e-06,
      "loss": 0.9226,
      "step": 54470
    },
    {
      "epoch": 2.843242396962699,
      "grad_norm": 5.004825115203857,
      "learning_rate": 2.6286032392185516e-06,
      "loss": 0.8938,
      "step": 54480
    },
    {
      "epoch": 2.843764269964904,
      "grad_norm": 4.71535587310791,
      "learning_rate": 2.6199050153958564e-06,
      "loss": 0.8333,
      "step": 54490
    },
    {
      "epoch": 2.8442861429671087,
      "grad_norm": 5.295953273773193,
      "learning_rate": 2.6112067915731608e-06,
      "loss": 0.8906,
      "step": 54500
    },
    {
      "epoch": 2.844808015969314,
      "grad_norm": 5.150154113769531,
      "learning_rate": 2.6025085677504656e-06,
      "loss": 0.8737,
      "step": 54510
    },
    {
      "epoch": 2.8453298889715186,
      "grad_norm": 4.498551368713379,
      "learning_rate": 2.59381034392777e-06,
      "loss": 0.8213,
      "step": 54520
    },
    {
      "epoch": 2.845851761973724,
      "grad_norm": 4.635546684265137,
      "learning_rate": 2.585112120105075e-06,
      "loss": 0.8717,
      "step": 54530
    },
    {
      "epoch": 2.8463736349759285,
      "grad_norm": 4.543516159057617,
      "learning_rate": 2.576413896282379e-06,
      "loss": 0.7626,
      "step": 54540
    },
    {
      "epoch": 2.8468955079781333,
      "grad_norm": 4.730410575866699,
      "learning_rate": 2.567715672459684e-06,
      "loss": 0.8682,
      "step": 54550
    },
    {
      "epoch": 2.8474173809803385,
      "grad_norm": 5.231871604919434,
      "learning_rate": 2.5590174486369884e-06,
      "loss": 0.763,
      "step": 54560
    },
    {
      "epoch": 2.847939253982543,
      "grad_norm": 4.734189987182617,
      "learning_rate": 2.5503192248142932e-06,
      "loss": 0.877,
      "step": 54570
    },
    {
      "epoch": 2.8484611269847484,
      "grad_norm": 3.487588882446289,
      "learning_rate": 2.5416210009915976e-06,
      "loss": 0.8354,
      "step": 54580
    },
    {
      "epoch": 2.848982999986953,
      "grad_norm": 4.851349830627441,
      "learning_rate": 2.532922777168902e-06,
      "loss": 0.9287,
      "step": 54590
    },
    {
      "epoch": 2.849504872989158,
      "grad_norm": 4.446521282196045,
      "learning_rate": 2.524224553346207e-06,
      "loss": 0.8695,
      "step": 54600
    },
    {
      "epoch": 2.850026745991363,
      "grad_norm": 5.524801254272461,
      "learning_rate": 2.5155263295235113e-06,
      "loss": 0.8582,
      "step": 54610
    },
    {
      "epoch": 2.850548618993568,
      "grad_norm": 4.730689525604248,
      "learning_rate": 2.506828105700816e-06,
      "loss": 0.8016,
      "step": 54620
    },
    {
      "epoch": 2.851070491995773,
      "grad_norm": 4.776847839355469,
      "learning_rate": 2.4981298818781205e-06,
      "loss": 0.9071,
      "step": 54630
    },
    {
      "epoch": 2.8515923649979777,
      "grad_norm": 5.015197277069092,
      "learning_rate": 2.4894316580554253e-06,
      "loss": 0.7994,
      "step": 54640
    },
    {
      "epoch": 2.8521142380001825,
      "grad_norm": 4.791595458984375,
      "learning_rate": 2.4807334342327297e-06,
      "loss": 0.8413,
      "step": 54650
    },
    {
      "epoch": 2.8526361110023877,
      "grad_norm": 4.57969856262207,
      "learning_rate": 2.4720352104100345e-06,
      "loss": 0.883,
      "step": 54660
    },
    {
      "epoch": 2.8531579840045924,
      "grad_norm": 4.796972274780273,
      "learning_rate": 2.463336986587339e-06,
      "loss": 0.7947,
      "step": 54670
    },
    {
      "epoch": 2.8536798570067976,
      "grad_norm": 4.873917102813721,
      "learning_rate": 2.4546387627646437e-06,
      "loss": 0.7823,
      "step": 54680
    },
    {
      "epoch": 2.8542017300090023,
      "grad_norm": 5.005276203155518,
      "learning_rate": 2.445940538941948e-06,
      "loss": 0.8997,
      "step": 54690
    },
    {
      "epoch": 2.854723603011207,
      "grad_norm": 4.850111961364746,
      "learning_rate": 2.437242315119253e-06,
      "loss": 0.8264,
      "step": 54700
    },
    {
      "epoch": 2.8552454760134123,
      "grad_norm": 4.866440296173096,
      "learning_rate": 2.4285440912965573e-06,
      "loss": 0.8835,
      "step": 54710
    },
    {
      "epoch": 2.855767349015617,
      "grad_norm": 5.517363548278809,
      "learning_rate": 2.419845867473862e-06,
      "loss": 0.8173,
      "step": 54720
    },
    {
      "epoch": 2.856289222017822,
      "grad_norm": 5.299288749694824,
      "learning_rate": 2.4111476436511665e-06,
      "loss": 0.8963,
      "step": 54730
    },
    {
      "epoch": 2.856811095020027,
      "grad_norm": 5.381692886352539,
      "learning_rate": 2.4024494198284714e-06,
      "loss": 0.8591,
      "step": 54740
    },
    {
      "epoch": 2.8573329680222317,
      "grad_norm": 4.914559841156006,
      "learning_rate": 2.3937511960057758e-06,
      "loss": 0.8788,
      "step": 54750
    },
    {
      "epoch": 2.8578548410244364,
      "grad_norm": 4.249580383300781,
      "learning_rate": 2.38505297218308e-06,
      "loss": 0.7748,
      "step": 54760
    },
    {
      "epoch": 2.8583767140266416,
      "grad_norm": 5.270496368408203,
      "learning_rate": 2.376354748360385e-06,
      "loss": 0.7982,
      "step": 54770
    },
    {
      "epoch": 2.858898587028847,
      "grad_norm": 4.663375377655029,
      "learning_rate": 2.3676565245376894e-06,
      "loss": 0.8452,
      "step": 54780
    },
    {
      "epoch": 2.8594204600310515,
      "grad_norm": 5.281522750854492,
      "learning_rate": 2.358958300714994e-06,
      "loss": 0.838,
      "step": 54790
    },
    {
      "epoch": 2.8599423330332563,
      "grad_norm": 3.943495273590088,
      "learning_rate": 2.3502600768922986e-06,
      "loss": 0.8212,
      "step": 54800
    },
    {
      "epoch": 2.860464206035461,
      "grad_norm": 4.463057041168213,
      "learning_rate": 2.3415618530696034e-06,
      "loss": 0.7401,
      "step": 54810
    },
    {
      "epoch": 2.860986079037666,
      "grad_norm": 4.116408348083496,
      "learning_rate": 2.332863629246908e-06,
      "loss": 0.8817,
      "step": 54820
    },
    {
      "epoch": 2.861507952039871,
      "grad_norm": 4.435390949249268,
      "learning_rate": 2.3241654054242126e-06,
      "loss": 0.8077,
      "step": 54830
    },
    {
      "epoch": 2.862029825042076,
      "grad_norm": 4.973489284515381,
      "learning_rate": 2.315467181601517e-06,
      "loss": 0.8788,
      "step": 54840
    },
    {
      "epoch": 2.862551698044281,
      "grad_norm": 4.334677696228027,
      "learning_rate": 2.306768957778822e-06,
      "loss": 0.8071,
      "step": 54850
    },
    {
      "epoch": 2.8630735710464856,
      "grad_norm": 4.826790809631348,
      "learning_rate": 2.2980707339561262e-06,
      "loss": 0.7422,
      "step": 54860
    },
    {
      "epoch": 2.863595444048691,
      "grad_norm": 4.275681018829346,
      "learning_rate": 2.289372510133431e-06,
      "loss": 0.8271,
      "step": 54870
    },
    {
      "epoch": 2.8641173170508956,
      "grad_norm": 5.522550582885742,
      "learning_rate": 2.2806742863107354e-06,
      "loss": 0.7981,
      "step": 54880
    },
    {
      "epoch": 2.8646391900531007,
      "grad_norm": 5.222818374633789,
      "learning_rate": 2.27197606248804e-06,
      "loss": 0.8273,
      "step": 54890
    },
    {
      "epoch": 2.8651610630553055,
      "grad_norm": Infinity,
      "learning_rate": 2.264147661047614e-06,
      "loss": 0.7079,
      "step": 54900
    },
    {
      "epoch": 2.8656829360575102,
      "grad_norm": 5.768791198730469,
      "learning_rate": 2.2554494372249187e-06,
      "loss": 0.7469,
      "step": 54910
    },
    {
      "epoch": 2.8662048090597154,
      "grad_norm": 5.309103012084961,
      "learning_rate": 2.2467512134022235e-06,
      "loss": 0.8343,
      "step": 54920
    },
    {
      "epoch": 2.86672668206192,
      "grad_norm": 5.6531877517700195,
      "learning_rate": 2.238052989579528e-06,
      "loss": 0.8051,
      "step": 54930
    },
    {
      "epoch": 2.8672485550641253,
      "grad_norm": 5.664869785308838,
      "learning_rate": 2.2293547657568323e-06,
      "loss": 0.9416,
      "step": 54940
    },
    {
      "epoch": 2.86777042806633,
      "grad_norm": 5.968109130859375,
      "learning_rate": 2.220656541934137e-06,
      "loss": 0.798,
      "step": 54950
    },
    {
      "epoch": 2.868292301068535,
      "grad_norm": 4.663913249969482,
      "learning_rate": 2.211958318111442e-06,
      "loss": 0.9082,
      "step": 54960
    },
    {
      "epoch": 2.86881417407074,
      "grad_norm": 4.9769744873046875,
      "learning_rate": 2.2032600942887464e-06,
      "loss": 0.8703,
      "step": 54970
    },
    {
      "epoch": 2.8693360470729448,
      "grad_norm": 4.958072185516357,
      "learning_rate": 2.1945618704660507e-06,
      "loss": 0.7691,
      "step": 54980
    },
    {
      "epoch": 2.86985792007515,
      "grad_norm": 5.074587821960449,
      "learning_rate": 2.1858636466433556e-06,
      "loss": 0.8901,
      "step": 54990
    },
    {
      "epoch": 2.8703797930773547,
      "grad_norm": 5.144672870635986,
      "learning_rate": 2.1771654228206604e-06,
      "loss": 0.9835,
      "step": 55000
    },
    {
      "epoch": 2.8709016660795594,
      "grad_norm": 4.936463832855225,
      "learning_rate": 2.1684671989979648e-06,
      "loss": 0.8077,
      "step": 55010
    },
    {
      "epoch": 2.871423539081764,
      "grad_norm": 4.840882778167725,
      "learning_rate": 2.159768975175269e-06,
      "loss": 0.8882,
      "step": 55020
    },
    {
      "epoch": 2.8719454120839694,
      "grad_norm": 3.920609474182129,
      "learning_rate": 2.151070751352574e-06,
      "loss": 0.7666,
      "step": 55030
    },
    {
      "epoch": 2.8724672850861745,
      "grad_norm": 4.8864288330078125,
      "learning_rate": 2.1423725275298784e-06,
      "loss": 0.8037,
      "step": 55040
    },
    {
      "epoch": 2.8729891580883793,
      "grad_norm": 5.327945232391357,
      "learning_rate": 2.1336743037071832e-06,
      "loss": 0.8606,
      "step": 55050
    },
    {
      "epoch": 2.873511031090584,
      "grad_norm": 4.3695526123046875,
      "learning_rate": 2.1249760798844876e-06,
      "loss": 0.863,
      "step": 55060
    },
    {
      "epoch": 2.8740329040927888,
      "grad_norm": 5.114997386932373,
      "learning_rate": 2.116277856061792e-06,
      "loss": 0.7736,
      "step": 55070
    },
    {
      "epoch": 2.874554777094994,
      "grad_norm": 4.938705921173096,
      "learning_rate": 2.107579632239097e-06,
      "loss": 0.7921,
      "step": 55080
    },
    {
      "epoch": 2.8750766500971987,
      "grad_norm": 4.792423725128174,
      "learning_rate": 2.0988814084164016e-06,
      "loss": 0.778,
      "step": 55090
    },
    {
      "epoch": 2.875598523099404,
      "grad_norm": 5.230308532714844,
      "learning_rate": 2.090183184593706e-06,
      "loss": 0.7881,
      "step": 55100
    },
    {
      "epoch": 2.8761203961016086,
      "grad_norm": 4.086584568023682,
      "learning_rate": 2.0814849607710104e-06,
      "loss": 0.7979,
      "step": 55110
    },
    {
      "epoch": 2.8766422691038134,
      "grad_norm": 4.954080104827881,
      "learning_rate": 2.0727867369483153e-06,
      "loss": 0.8111,
      "step": 55120
    },
    {
      "epoch": 2.8771641421060186,
      "grad_norm": 5.456474304199219,
      "learning_rate": 2.06408851312562e-06,
      "loss": 0.828,
      "step": 55130
    },
    {
      "epoch": 2.8776860151082233,
      "grad_norm": 4.5422563552856445,
      "learning_rate": 2.0553902893029245e-06,
      "loss": 0.9455,
      "step": 55140
    },
    {
      "epoch": 2.8782078881104285,
      "grad_norm": 4.812232494354248,
      "learning_rate": 2.046692065480229e-06,
      "loss": 0.8237,
      "step": 55150
    },
    {
      "epoch": 2.8787297611126332,
      "grad_norm": 4.189203262329102,
      "learning_rate": 2.0379938416575337e-06,
      "loss": 0.944,
      "step": 55160
    },
    {
      "epoch": 2.879251634114838,
      "grad_norm": 4.811344146728516,
      "learning_rate": 2.0292956178348385e-06,
      "loss": 0.8655,
      "step": 55170
    },
    {
      "epoch": 2.879773507117043,
      "grad_norm": 4.664456844329834,
      "learning_rate": 2.020597394012143e-06,
      "loss": 0.8352,
      "step": 55180
    },
    {
      "epoch": 2.880295380119248,
      "grad_norm": 4.894687652587891,
      "learning_rate": 2.0118991701894473e-06,
      "loss": 0.9103,
      "step": 55190
    },
    {
      "epoch": 2.880817253121453,
      "grad_norm": 4.398310661315918,
      "learning_rate": 2.0032009463667517e-06,
      "loss": 0.8612,
      "step": 55200
    },
    {
      "epoch": 2.881339126123658,
      "grad_norm": 4.984282493591309,
      "learning_rate": 1.9945027225440565e-06,
      "loss": 0.8453,
      "step": 55210
    },
    {
      "epoch": 2.8818609991258626,
      "grad_norm": 4.287540435791016,
      "learning_rate": 1.9858044987213613e-06,
      "loss": 0.9051,
      "step": 55220
    },
    {
      "epoch": 2.8823828721280678,
      "grad_norm": 5.06110954284668,
      "learning_rate": 1.9771062748986657e-06,
      "loss": 0.8576,
      "step": 55230
    },
    {
      "epoch": 2.8829047451302725,
      "grad_norm": 4.96169900894165,
      "learning_rate": 1.96840805107597e-06,
      "loss": 0.8164,
      "step": 55240
    },
    {
      "epoch": 2.8834266181324777,
      "grad_norm": 4.228497505187988,
      "learning_rate": 1.959709827253275e-06,
      "loss": 0.8645,
      "step": 55250
    },
    {
      "epoch": 2.8839484911346824,
      "grad_norm": 5.164516925811768,
      "learning_rate": 1.9510116034305798e-06,
      "loss": 0.8763,
      "step": 55260
    },
    {
      "epoch": 2.884470364136887,
      "grad_norm": 5.36165189743042,
      "learning_rate": 1.942313379607884e-06,
      "loss": 0.8377,
      "step": 55270
    },
    {
      "epoch": 2.8849922371390924,
      "grad_norm": 4.895534515380859,
      "learning_rate": 1.9336151557851886e-06,
      "loss": 0.7893,
      "step": 55280
    },
    {
      "epoch": 2.885514110141297,
      "grad_norm": 4.7388529777526855,
      "learning_rate": 1.9249169319624934e-06,
      "loss": 0.9038,
      "step": 55290
    },
    {
      "epoch": 2.8860359831435023,
      "grad_norm": 4.493605136871338,
      "learning_rate": 1.916218708139798e-06,
      "loss": 0.9169,
      "step": 55300
    },
    {
      "epoch": 2.886557856145707,
      "grad_norm": 4.863831996917725,
      "learning_rate": 1.9075204843171026e-06,
      "loss": 0.8507,
      "step": 55310
    },
    {
      "epoch": 2.8870797291479118,
      "grad_norm": 4.417086124420166,
      "learning_rate": 1.898822260494407e-06,
      "loss": 0.8418,
      "step": 55320
    },
    {
      "epoch": 2.8876016021501165,
      "grad_norm": 5.151023864746094,
      "learning_rate": 1.8901240366717116e-06,
      "loss": 0.9235,
      "step": 55330
    },
    {
      "epoch": 2.8881234751523217,
      "grad_norm": 4.9526591300964355,
      "learning_rate": 1.8814258128490164e-06,
      "loss": 0.82,
      "step": 55340
    },
    {
      "epoch": 2.8886453481545264,
      "grad_norm": 4.45172119140625,
      "learning_rate": 1.872727589026321e-06,
      "loss": 0.8584,
      "step": 55350
    },
    {
      "epoch": 2.8891672211567316,
      "grad_norm": 4.892184734344482,
      "learning_rate": 1.8640293652036254e-06,
      "loss": 0.8947,
      "step": 55360
    },
    {
      "epoch": 2.8896890941589364,
      "grad_norm": 5.430438995361328,
      "learning_rate": 1.85533114138093e-06,
      "loss": 0.8574,
      "step": 55370
    },
    {
      "epoch": 2.890210967161141,
      "grad_norm": 4.438523292541504,
      "learning_rate": 1.8466329175582348e-06,
      "loss": 0.8038,
      "step": 55380
    },
    {
      "epoch": 2.8907328401633463,
      "grad_norm": 3.9426827430725098,
      "learning_rate": 1.8379346937355392e-06,
      "loss": 0.752,
      "step": 55390
    },
    {
      "epoch": 2.891254713165551,
      "grad_norm": 4.439480304718018,
      "learning_rate": 1.8292364699128439e-06,
      "loss": 0.7732,
      "step": 55400
    },
    {
      "epoch": 2.8917765861677562,
      "grad_norm": 4.337193012237549,
      "learning_rate": 1.8205382460901485e-06,
      "loss": 0.8222,
      "step": 55410
    },
    {
      "epoch": 2.892298459169961,
      "grad_norm": 4.629064559936523,
      "learning_rate": 1.8118400222674533e-06,
      "loss": 0.8265,
      "step": 55420
    },
    {
      "epoch": 2.8928203321721657,
      "grad_norm": 4.897387504577637,
      "learning_rate": 1.8031417984447577e-06,
      "loss": 0.8458,
      "step": 55430
    },
    {
      "epoch": 2.893342205174371,
      "grad_norm": 4.267148017883301,
      "learning_rate": 1.7944435746220623e-06,
      "loss": 0.8582,
      "step": 55440
    },
    {
      "epoch": 2.8938640781765756,
      "grad_norm": 4.429988384246826,
      "learning_rate": 1.7857453507993667e-06,
      "loss": 0.8235,
      "step": 55450
    },
    {
      "epoch": 2.894385951178781,
      "grad_norm": 4.700254917144775,
      "learning_rate": 1.7770471269766713e-06,
      "loss": 0.7956,
      "step": 55460
    },
    {
      "epoch": 2.8949078241809856,
      "grad_norm": 3.814852476119995,
      "learning_rate": 1.768348903153976e-06,
      "loss": 0.7782,
      "step": 55470
    },
    {
      "epoch": 2.8954296971831903,
      "grad_norm": 5.154529571533203,
      "learning_rate": 1.7596506793312807e-06,
      "loss": 0.8435,
      "step": 55480
    },
    {
      "epoch": 2.8959515701853955,
      "grad_norm": 4.441112518310547,
      "learning_rate": 1.7509524555085851e-06,
      "loss": 0.8556,
      "step": 55490
    },
    {
      "epoch": 2.8964734431876002,
      "grad_norm": 3.949958324432373,
      "learning_rate": 1.7422542316858897e-06,
      "loss": 0.7931,
      "step": 55500
    },
    {
      "epoch": 2.8969953161898054,
      "grad_norm": 4.765547752380371,
      "learning_rate": 1.7335560078631945e-06,
      "loss": 0.8363,
      "step": 55510
    },
    {
      "epoch": 2.89751718919201,
      "grad_norm": 4.849027156829834,
      "learning_rate": 1.7248577840404991e-06,
      "loss": 0.8255,
      "step": 55520
    },
    {
      "epoch": 2.898039062194215,
      "grad_norm": 4.130672931671143,
      "learning_rate": 1.7161595602178035e-06,
      "loss": 0.811,
      "step": 55530
    },
    {
      "epoch": 2.89856093519642,
      "grad_norm": 4.647409915924072,
      "learning_rate": 1.7074613363951081e-06,
      "loss": 0.8597,
      "step": 55540
    },
    {
      "epoch": 2.899082808198625,
      "grad_norm": 5.263655185699463,
      "learning_rate": 1.698763112572413e-06,
      "loss": 0.8565,
      "step": 55550
    },
    {
      "epoch": 2.89960468120083,
      "grad_norm": 4.036778450012207,
      "learning_rate": 1.6900648887497174e-06,
      "loss": 0.8615,
      "step": 55560
    },
    {
      "epoch": 2.900126554203035,
      "grad_norm": 5.347250461578369,
      "learning_rate": 1.681366664927022e-06,
      "loss": 0.959,
      "step": 55570
    },
    {
      "epoch": 2.9006484272052395,
      "grad_norm": 5.311855792999268,
      "learning_rate": 1.6726684411043266e-06,
      "loss": 0.8089,
      "step": 55580
    },
    {
      "epoch": 2.9011703002074443,
      "grad_norm": 5.002476215362549,
      "learning_rate": 1.6639702172816314e-06,
      "loss": 0.8614,
      "step": 55590
    },
    {
      "epoch": 2.9016921732096495,
      "grad_norm": 5.511478900909424,
      "learning_rate": 1.6552719934589358e-06,
      "loss": 0.8038,
      "step": 55600
    },
    {
      "epoch": 2.902214046211854,
      "grad_norm": 4.2806396484375,
      "learning_rate": 1.6465737696362404e-06,
      "loss": 0.8049,
      "step": 55610
    },
    {
      "epoch": 2.9027359192140594,
      "grad_norm": 4.56095552444458,
      "learning_rate": 1.6378755458135448e-06,
      "loss": 0.8708,
      "step": 55620
    },
    {
      "epoch": 2.903257792216264,
      "grad_norm": 5.535151958465576,
      "learning_rate": 1.6291773219908494e-06,
      "loss": 0.8878,
      "step": 55630
    },
    {
      "epoch": 2.903779665218469,
      "grad_norm": 5.480021953582764,
      "learning_rate": 1.6204790981681542e-06,
      "loss": 0.8026,
      "step": 55640
    },
    {
      "epoch": 2.904301538220674,
      "grad_norm": 4.346857070922852,
      "learning_rate": 1.6117808743454588e-06,
      "loss": 0.8311,
      "step": 55650
    },
    {
      "epoch": 2.904823411222879,
      "grad_norm": 5.1321845054626465,
      "learning_rate": 1.6030826505227632e-06,
      "loss": 0.8356,
      "step": 55660
    },
    {
      "epoch": 2.905345284225084,
      "grad_norm": 4.518435955047607,
      "learning_rate": 1.5943844267000678e-06,
      "loss": 0.8184,
      "step": 55670
    },
    {
      "epoch": 2.9058671572272887,
      "grad_norm": 3.7929675579071045,
      "learning_rate": 1.5856862028773727e-06,
      "loss": 0.8671,
      "step": 55680
    },
    {
      "epoch": 2.9063890302294935,
      "grad_norm": 4.629269123077393,
      "learning_rate": 1.5769879790546773e-06,
      "loss": 0.8144,
      "step": 55690
    },
    {
      "epoch": 2.9069109032316987,
      "grad_norm": 4.361545085906982,
      "learning_rate": 1.5682897552319817e-06,
      "loss": 0.7539,
      "step": 55700
    },
    {
      "epoch": 2.9074327762339034,
      "grad_norm": 4.9067888259887695,
      "learning_rate": 1.5595915314092863e-06,
      "loss": 0.7786,
      "step": 55710
    },
    {
      "epoch": 2.9079546492361086,
      "grad_norm": 4.848426818847656,
      "learning_rate": 1.5508933075865909e-06,
      "loss": 0.8179,
      "step": 55720
    },
    {
      "epoch": 2.9084765222383133,
      "grad_norm": 4.624410629272461,
      "learning_rate": 1.5421950837638955e-06,
      "loss": 0.8436,
      "step": 55730
    },
    {
      "epoch": 2.908998395240518,
      "grad_norm": 5.098776817321777,
      "learning_rate": 1.5334968599412e-06,
      "loss": 0.8693,
      "step": 55740
    },
    {
      "epoch": 2.9095202682427233,
      "grad_norm": 4.902401924133301,
      "learning_rate": 1.5247986361185047e-06,
      "loss": 0.8974,
      "step": 55750
    },
    {
      "epoch": 2.910042141244928,
      "grad_norm": 4.355867862701416,
      "learning_rate": 1.5161004122958093e-06,
      "loss": 0.7495,
      "step": 55760
    },
    {
      "epoch": 2.910564014247133,
      "grad_norm": 4.649875640869141,
      "learning_rate": 1.5074021884731137e-06,
      "loss": 0.8652,
      "step": 55770
    },
    {
      "epoch": 2.911085887249338,
      "grad_norm": 4.593348503112793,
      "learning_rate": 1.4987039646504185e-06,
      "loss": 0.8925,
      "step": 55780
    },
    {
      "epoch": 2.9116077602515427,
      "grad_norm": 5.2109832763671875,
      "learning_rate": 1.490005740827723e-06,
      "loss": 0.8838,
      "step": 55790
    },
    {
      "epoch": 2.912129633253748,
      "grad_norm": 4.8571953773498535,
      "learning_rate": 1.4813075170050277e-06,
      "loss": 0.8512,
      "step": 55800
    },
    {
      "epoch": 2.9126515062559526,
      "grad_norm": 3.590566635131836,
      "learning_rate": 1.4726092931823321e-06,
      "loss": 0.7708,
      "step": 55810
    },
    {
      "epoch": 2.913173379258158,
      "grad_norm": 4.183398246765137,
      "learning_rate": 1.463911069359637e-06,
      "loss": 0.8215,
      "step": 55820
    },
    {
      "epoch": 2.9136952522603625,
      "grad_norm": 4.532772064208984,
      "learning_rate": 1.4552128455369413e-06,
      "loss": 0.8915,
      "step": 55830
    },
    {
      "epoch": 2.9142171252625673,
      "grad_norm": 5.3204193115234375,
      "learning_rate": 1.4465146217142462e-06,
      "loss": 0.8737,
      "step": 55840
    },
    {
      "epoch": 2.914738998264772,
      "grad_norm": 4.816673278808594,
      "learning_rate": 1.4378163978915506e-06,
      "loss": 0.8702,
      "step": 55850
    },
    {
      "epoch": 2.915260871266977,
      "grad_norm": 5.593655586242676,
      "learning_rate": 1.4291181740688554e-06,
      "loss": 0.8877,
      "step": 55860
    },
    {
      "epoch": 2.915782744269182,
      "grad_norm": 5.116939544677734,
      "learning_rate": 1.4204199502461598e-06,
      "loss": 0.8309,
      "step": 55870
    },
    {
      "epoch": 2.916304617271387,
      "grad_norm": 5.320344924926758,
      "learning_rate": 1.4117217264234644e-06,
      "loss": 0.8842,
      "step": 55880
    },
    {
      "epoch": 2.916826490273592,
      "grad_norm": 4.374407768249512,
      "learning_rate": 1.403023502600769e-06,
      "loss": 0.775,
      "step": 55890
    },
    {
      "epoch": 2.9173483632757966,
      "grad_norm": 4.046535968780518,
      "learning_rate": 1.3943252787780736e-06,
      "loss": 0.7518,
      "step": 55900
    },
    {
      "epoch": 2.917870236278002,
      "grad_norm": 4.54264497756958,
      "learning_rate": 1.3856270549553782e-06,
      "loss": 0.7512,
      "step": 55910
    },
    {
      "epoch": 2.9183921092802065,
      "grad_norm": 5.038548469543457,
      "learning_rate": 1.3769288311326828e-06,
      "loss": 0.9041,
      "step": 55920
    },
    {
      "epoch": 2.9189139822824117,
      "grad_norm": 4.910364151000977,
      "learning_rate": 1.3682306073099874e-06,
      "loss": 0.8144,
      "step": 55930
    },
    {
      "epoch": 2.9194358552846165,
      "grad_norm": 4.898083209991455,
      "learning_rate": 1.3595323834872918e-06,
      "loss": 0.8384,
      "step": 55940
    },
    {
      "epoch": 2.919957728286821,
      "grad_norm": 5.383793354034424,
      "learning_rate": 1.3508341596645966e-06,
      "loss": 0.8346,
      "step": 55950
    },
    {
      "epoch": 2.9204796012890264,
      "grad_norm": 3.8654892444610596,
      "learning_rate": 1.342135935841901e-06,
      "loss": 0.7995,
      "step": 55960
    },
    {
      "epoch": 2.921001474291231,
      "grad_norm": 4.358118534088135,
      "learning_rate": 1.3334377120192059e-06,
      "loss": 0.7866,
      "step": 55970
    },
    {
      "epoch": 2.9215233472934363,
      "grad_norm": 4.5004425048828125,
      "learning_rate": 1.3247394881965103e-06,
      "loss": 0.7419,
      "step": 55980
    },
    {
      "epoch": 2.922045220295641,
      "grad_norm": 4.986687183380127,
      "learning_rate": 1.316041264373815e-06,
      "loss": 0.8879,
      "step": 55990
    },
    {
      "epoch": 2.922567093297846,
      "grad_norm": 4.054242134094238,
      "learning_rate": 1.3073430405511195e-06,
      "loss": 0.7723,
      "step": 56000
    }
  ],
  "logging_steps": 10,
  "max_steps": 57483,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3199321754215383e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
