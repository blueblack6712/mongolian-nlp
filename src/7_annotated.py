import ujson
import gc
import os
import psutil
import signal

# âš™ï¸ é…ç½®å‚æ•°
DATASET_PATH = "/dev/shm/pos_tagged_dataset_1.jsonl"
BERT_VOCAB = "/dev/shm/bert-base-multiligual-cased/vocab.txt"
OUTPUT_BERT = "bert_annotations.jsonl"
OUTPUT_LLAMA = "llama_annotations.jsonl"
PAGE_SIZE = 200  # åˆå§‹åˆ†é¡µå¤§å°
MEMORY_THRESHOLD = 80  # å†…å­˜è­¦æˆ’ç™¾åˆ†æ¯”

# ğŸ”¥ å†…å­˜å®ˆæŠ¤ç³»ç»Ÿ
class MemoryGuardian:
    @staticmethod
    def get_memory_usage():
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”"""
        return psutil.virtual_memory().percent
    
    @staticmethod
    def memory_safety_check():
        """å†…å­˜å®‰å…¨æ£€æŸ¥"""
        if MemoryGuardian.get_memory_usage() > MEMORY_THRESHOLD:
            print(f"ğŸš¨ å†…å­˜å‘Šè­¦ ({MemoryGuardian.get_memory_usage()}%)ï¼Œå¯åŠ¨ç´§æ€¥æ¸…ç†...")
            gc.collect()
            return True
        return False

# ğŸ”¥ å´©æºƒé˜²æŠ¤å‹BERTå¤„ç†å™¨
class CrashSafeBertProcessor:
    def __init__(self, vocab_path):
        """å®‰å…¨åˆå§‹åŒ–"""
        self.root = {}
        with open(vocab_path, "r", encoding="utf-8") as f:
            for line in f:
                if (word := line.strip()):
                    node = self.root
                    for char in word:
                        node = node.setdefault(char, {})
                    node["__end__"] = None
    
    def _adaptive_page_size(self):
        """åŠ¨æ€è°ƒæ•´åˆ†é¡µå¤§å°"""
        global PAGE_SIZE
        mem_usage = MemoryGuardian.get_memory_usage()
        if mem_usage > 75:
            PAGE_SIZE = max(50, PAGE_SIZE // 2)
        elif mem_usage < 40:
            PAGE_SIZE = min(1000, PAGE_SIZE * 2)
        return PAGE_SIZE

    def tokenize(self, word):
        """å®‰å…¨åˆ†è¯æµ"""
        start = 0
        while start < len(word):
            max_len = self._find_longest(word, start)
            if max_len == 0:
                yield "[UNK]"
                start += 1
            else:
                token = word[start:start+max_len]
                if start > 0:
                    token = "##" + token
                yield token
                start += max_len

    def process(self, input_path, output_path):
        """å´©æºƒé˜²æŠ¤å¤„ç†æµç¨‹"""
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "w", encoding="utf-8") as fout:
            
            page = []
            for line_num, line in enumerate(fin, 1):
                try:
                    # å†…å­˜ç´§æ€¥æ£€æŸ¥
                    if MemoryGuardian.memory_safety_check():
                        # ç«‹å³å†™å…¥å½“å‰é¡µ
                        if page:
                            fout.write("\n".join(page) + "\n")
                            page = []
                            gc.collect()
                    
                    # åŠ¨æ€åˆ†é¡µè°ƒæ•´
                    current_page_size = self._adaptive_page_size()
                    
                    # æ•°æ®å¤„ç†
                    data = ujson.loads(line)
                    tokens = data["tokens"]
                    tags = data["pos_tags"]
                    
                    # æµå¼ç”Ÿæˆè¾“å‡º
                    bert_tokens = []
                    aligned_tags = []
                    for w, t in zip(tokens, tags):
                        subs = list(self.tokenize(w))
                        bert_tokens.extend(subs)
                        aligned_tags.append(t)
                        aligned_tags.extend(["X"]*(len(subs)-1))
                    
                    page.append(ujson.dumps({
                        "tokens": bert_tokens,
                        "pos_tags": aligned_tags
                    }, ensure_ascii=False))
                    
                    # å¼ºåˆ¶å†…å­˜æ¸…ç†
                    del data, tokens, tags, bert_tokens, aligned_tags
                    if len(page) >= current_page_size:
                        fout.write("\n".join(page) + "\n")
                        page = []
                        print(f"BERT: {line_num} lines | Page: {current_page_size} | Mem: {MemoryGuardian.get_memory_usage()}%")
                        gc.collect()
                
                except Exception as e:
                    print(f"ğŸš‘ è¡Œ {line_num} é”™è¯¯è·³è¿‡: {str(e)}")
                    del line  # ç¡®ä¿é”™è¯¯è¡Œå†…å­˜é‡Šæ”¾
                    gc.collect()
                    continue
            
            if page:
                fout.write("\n".join(page) + "\n")

# âš¡ å®‰å…¨å‹LLaMAå¤„ç†å™¨
class CrashSafeLlamaProcessor:
    def process(self, input_path, output_path):
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "w", encoding="utf-8") as fout:
            
            page = []
            for line_num, line in enumerate(fin, 1):
                # å†…å­˜è­¦æˆ’æ£€æŸ¥
                if MemoryGuardian.memory_safety_check() and page:
                    fout.write("\n".join(page) + "\n")
                    page = []
                    gc.collect()
                
                data = ujson.loads(line)
                annotated = " ".join(f"{w}[{t}]" for w,t in zip(data["tokens"], data["pos_tags"]))
                
                page.append(ujson.dumps({
                    "prompt": "Annotate:",
                    "completion": f" {annotated}"
                }, ensure_ascii=False))
                
                if len(page) >= self._adaptive_page_size():
                    fout.write("\n".join(page) + "\n")
                    page = []
                    print(f"LLaMA: {line_num} lines | Mem: {MemoryGuardian.get_memory_usage()}%")
                    gc.collect()
                
                del data, annotated
                gc.collect()
            
            if page:
                fout.write("\n".join(page) + "\n")

def system_cleanup():
    """æœ€åä¸€é“å†…å­˜é˜²çº¿"""
    print("ğŸš’ å¯åŠ¨ç³»ç»Ÿçº§æ¸…ç†...")
    os.system('sync && echo 3 > /proc/sys/vm/drop_caches')  # Linuxç³»ç»Ÿç¼“å­˜æ¸…ç†

if __name__ == "__main__":
    # æ³¨å†Œå´©æºƒä¿æŠ¤
    signal.signal(signal.SIGINT, lambda *_: system_cleanup())
    
    try:
        print("ğŸ›¡ï¸ BERTå¤„ç†å¯åŠ¨ (å´©æºƒé˜²æŠ¤æ¨¡å¼)")
        bert_processor = CrashSafeBertProcessor(BERT_VOCAB)
        bert_processor.process(DATASET_PATH, OUTPUT_BERT)
        del bert_processor
        gc.collect()
        
        print("\nğŸ›¡ï¸ LLaMAå¤„ç†å¯åŠ¨ (å´©æºƒé˜²æŠ¤æ¨¡å¼)")
        llama_processor = CrashSafeLlamaProcessor()
        llama_processor.process(DATASET_PATH, OUTPUT_LLAMA)
        
    finally:
        system_cleanup()
        print("âœ… å…¨æµç¨‹å®Œæˆï¼")